[
  {
    "title": "",
    "url": "https://fly.io/blog/vscode-ssh-wtf/",
    "content": "We‚Äôre interested in getting integrated into the flow VSCode uses to do remote editing over SSH, because everybody is using VSCode now, and, in particular, they‚Äôre using forks of VSCode that generate code with LLMs.\n\n‚Äùhallucination‚Äù is what we call it when LLMs get code wrong; ‚Äúengineering‚Äù is what we call it when people do.\n\nLLM-generated code is useful in the general case if you know what you‚Äôre doing. But it‚Äôs ultra-useful if you can close the loop between the LLM and the execution environment (with an ‚ÄúAgent‚Äù setup). There‚Äôs lots to say about this, but for the moment: it‚Äôs a semi-effective antidote to hallucination: the LLM generates the code, the agent scaffolding runs the code, the code generates errors, the agent feeds it back to the LLM, the process iterates.\n\nSo, obviously, the issue here is you don‚Äôt want this iterative development process happening on your development laptop, because LLMs have boundary issues, and they‚Äôll iterate on your system configuration just as happily on the Git project you happen to be working in. A thing you‚Äôd really like to be able to do: run a closed-loop agent-y (‚Äúagentic‚Äù? is that what we say now) configuration for an LLM, on a clean-slate Linux instance that spins up instantly and that can‚Äôt screw you over in any way. You get where we‚Äôre going with this.\n\nAnyways! I would like to register a concern.\n\nEmacs hosts the spiritual forebearer of remote editing systems, a blob of hyper-useful Elisp called ‚ÄúTramp‚Äù. If you can hook Tramp up to any kind of interactive environment ‚Äî usually, an SSH session ‚Äî where it can run Bourne shell commands, it can extend Emacs to that environment.\n\nSo, VSCode has a feature like Tramp. Which, neat, right? You‚Äôd think, take Tramp, maybe simplify it a bit, switch out Elisp for Typescript.\n\nYou‚Äôd think wrong!\n\nUnlike Tramp, which lives off the land on the remote connection, VSCode mounts a full-scale invasion: it runs a Bash snippet stager that downloads an agent, including a binary installation of Node.\n\nI think this is the source code?\n\nThe agent runs over port-forwarded SSH. It establishes a WebSockets connection back to your running VSCode front-end. The underlying protocol on that connection can:\n\nIn security-world, there‚Äôs a name for tools that work this way. I won‚Äôt say it out loud, because that‚Äôs not fair to VSCode, but let‚Äôs just say the name is murid in nature.\n\nI would be a little nervous about letting people VSCode-remote-edit stuff on dev servers, and apoplectic if that happened during an incident on something in production.\n\nIt turns out we don‚Äôt have to care about any of this to get a custom connection to a Fly Machine working in VSCode, so none of this matters in any kind of deep way, but: we‚Äôve decided to just be a blog again, so: we had to learn this, and now you do too."
  },
  {
    "title": "Launch a GPU app in seconds",
    "url": "https://fly.io/blog/ai-gpu-clusters-from-your-laptop-livebook/",
    "content": "Livebook, FLAME, and the Nx stack: three Elixir components that are easy to describe, more powerful than they look, and intricately threaded into the Elixir ecosystem. A few weeks ago, Chris McCord (üëã) and Chris Grainger showed them off at ElixirConf 2024. We thought the talk was worth a recap.\n\nLet‚Äôs begin by introducing our cast of characters.\n\nLivebook is usually described as Elixir‚Äôs answer to Jupyter Notebooks. And that‚Äôs a good way to think about it. But Livebook takes full advantage of the Elixir platform, which makes it sneakily powerful. By linking up directly with Elixir app clusters, Livebook can switch easily between driving compute locally or on remote servers, and makes it easy to bring in any kind of data into reproducible workflows.\n\nFLAME is the Elixir‚Äôs answer to serverless computing. By having the library manage a pool of executors for you, FLAME lets you treat your entire application as if it was elastic and scale-to-zero. You configure FLAME with some basic information about where to run code and how many instances it‚Äôs allowed to run with, and then mark off any arbitrary section of code with Flame.call. The framework takes care of the rest. It‚Äôs the upside of serverless without committing yourself to blowing your app apart into tiny, intricately connected pieces.\n\nThe Nx stack is how you do Elixir-native AI and ML. Nx gives you an Elixir-native notion of tensor computations with GPU backends. Axon builds a common interface for ML models on top of it. Bumblebee makes those models available to any Elixir app that wants to download them, from just a couple lines of code.\n\nHere is quick video showing how to transfer a local tensor to a remote GPU, using Livebook, FLAME, and Nx:\n\nLet‚Äôs dive into the keynote.\n\n\n## Poking a hole in your infrastructure\n\nAny Livebook, including the one running on your laptop, can start a runtime running on a Fly Machine, in Fly.io‚Äôs public cloud. That Elixir machine will (by default) live in your default Fly.io organization, giving it networked access to all the other apps that might live there.\n\nThis is an access control situation that mostly just does what you want it to do without asking. Unless you ask it to, Fly.io isn‚Äôt exposing anything to the Internet, or to other users of Fly.io. For instance: say we have a database we‚Äôre going to use to generate reports. It can hang out on our Fly organization, inside of a private network with no connectivity to the world. We can spin up a Livebook instance that can talk to it, without doing any network or infrastructure engineering to make that happen.\n\nBut wait, there‚Äôs more. Because this is all Elixir, Livebook also allows you to connect to any running Erlang/Elixir application in your infrastructure to debug, introspect, and monitor them.\n\nCheck out this clip of Chris McCord connecting to an existing application during the keynote:\n\nRunning a snippet of code from a laptop on a remote server is a neat trick, but Livebook is doing something deeper than that. It‚Äôs taking advantage of Erlang/Elixir‚Äôs native facility with cluster computation and making it available to the notebook. As a result, when we do things like auto-completing, Livebook delivers results from modules defined on the remote note itself. ü§Ø\n\n\n## Elastic scale with FLAME\n\nWhen we first introduced FLAME, the example we used was video encoding.\n\nVideo encoding is complicated and slow enough that you‚Äôd normally make arrangements to run it remotely or in a background job queue, or as a triggerable Lambda function. The point of FLAME is to get rid of all those steps, and give them over to the framework instead. So: we wrote our ffpmeg calls inline like normal code, as if they were going to complete in microseconds, and wrapped them in Flame.call blocks. That was it, that was the demo.\n\nHere, we‚Äôre going to put a little AI spin on it.\n\nThe first thing we‚Äôre doing here is driving FLAME pools from Livebook. Livebook will automatically synchronize your notebook dependencies as well as any module or code defined in your notebook across nodes. That means any code we write in our notebook can be dispatched transparently out to arbitrarily many compute nodes, without ceremony.\n\nNow let‚Äôs add some AI flair. We take an object store bucket full of video files. We use ffmpeg to extract stills from the video at different moments. Then: we send them to Llama, running on GPU Fly Machines (still locked to our organization), to get descriptions of the stills.\n\nAll those stills and descriptions get streamed back to our notebook, in real time:\n\nAt the end, the descriptions are sent to Mistral, which builds a summary.\n\nThanks to FLAME, we get explicit control over the minimum and the maximum amount of nodes you want running at once, as well their concurrency settings. As nodes finish processing each video, new ones are automatically sent to them, until the whole bucket has been traversed. Each node will automatically shut down after an idle timeout and the whole cluster terminates if you disconnect the Livebook runtime.\n\nJust like your app code, FLAME lets you take your notebook code designed to run locally, change almost nothing, and elastically execute it across ephemeral infrastructure.\n\n\n## 64-GPUs hyperparameter tuning on a laptop\n\nNext, Chris Grainger, CTO of Amplified, takes the stage.\n\nFor work at Amplified, Chris wants to analyze a gigantic archive of patents, on behalf of a client doing edible cannibinoid work. To do that, he uses a BERT model (BERT, from Google, is one of the OG ‚Äútransformer‚Äù models, optimized for text comprehension).\n\nTo make the BERT model effective for this task, he‚Äôs going to do a hyperparameter training run.\n\nThis is a much more complicated AI task than the Llama work we just showed up. Chris is going to generate a cluster of 64 GPU Fly Machines, each running an L40s GPU. On each of these nodes, he needs to:\n\nHere‚Äôs the clip. You‚Äôll see the results stream in, in real time, directly back to his Livebook. We‚Äôll wait, because it won‚Äôt take long to watch:\n\n\n## This is just the beginning\n\nThe suggestion of mixing Livebook and FLAME to elastically scale notebook execution was originally proposed by Chris Grainger during ElixirConf EU. During the next four months, Jonatan K≈Çosko, Chris McCord, and Jos√© Valim worked part-time on making it a reality in time for ElixirConf US. Our ability to deliver such a rich combination of features in such a short period of time is a testament to the capabilities of the Erlang Virtual Machine, which Elixir and Livebook runs on. Other features, such as remote dataframes and distributed GC, were implemented in a weekend. Bringing the same functionality to other ecosystems would take several additional months, sometimes accompanied by millions in funding, and often times as part of a closed-source product.\n\nFurthermore, since we announced this feature, Michael Ruoss stepped in and brought the same functionality to Kubernetes. From Livebook v0.14.1, you can start Livebook runtimes inside a Kubernetes cluster and also use FLAME to elastically scale them. Expect more features and news in this space!\n\nFinally, Fly‚Äôs infrastructure played a key role in making it possible to start a cluster of GPUs in seconds rather than minutes, and all it requires is a Docker image. We‚Äôre looking forward to see how other technologies and notebook platforms can leverage Fly to also elevate their developer experiences.\n\nRun your own LLMs or use Livebook for elastic GPU workflows¬†‚ú®"
  },
  {
    "title": "Support For Developers, By Developers",
    "url": "https://fly.io/blog/accident-forgiveness/",
    "content": "We‚Äôre Fly.io, a new public cloud with simple, developer-friendly ergonomics. Try it out; you‚Äôll be deployed in just minutes, and, as you‚Äôre about to read, with less financial risk.\n\nPublic cloud billing is terrifying.\n\nThe premise of a public cloud ‚Äî what sets it apart from a hosting provider ‚Äî is 8,760 hours/year of on-tap deployable compute, storage, and networking. Cloud resources are ‚Äúelastic‚Äù: they‚Äôre acquired and released as needed; in the ‚Äúcloud-iest‚Äù apps, without human intervention. Public cloud resources behave like utilities, and that‚Äôs how they‚Äôre priced.\n\nYou probably can‚Äôt tell me how much electricity your home is using right now, and may only come within tens of dollars of accurately predicting your water bill. But neither of those bills are all that scary, because you assume there‚Äôs a limit to how much you could run them up in a single billing interval.\n\nThat‚Äôs not true of public clouds. There are only so many ways to ‚Äúspend‚Äù water at your home, but there are indeterminably many ways to land on a code path that grabs another VM, or to miskey a configuration, or to leak long-running CI/CD environments every time a PR gets merged. Pick a practitioner at random, and I bet they‚Äôve read a story within the last couple months about someone running up a galactic-scale bill at some provider or other.\n\n\n## Implied Accident Forgiveness\n\nFor people who don‚Äôt do a lot of cloud work, what all this means is that every PR push sets off a little alarm in the back of their heads: ‚Äúyou may have just incurred $200,000 of costs!‚Äù. The alarm is quickly silenced, though it‚Äôs still subtly extracting a cortisol penalty. But by deadening the nerves that sense the danger of unexpected charges, those people are nudged closer to themselves being the next story on Twitter about an accidental $200,000 bill.\n\nThe saving grace here, which you‚Äôll learn if you ever become that $200,000 story, is that nobody pays those bills.\n\nSee, what cloud-savvy people know already is that providers have billing support teams, which spend a big chunk of their time conceding disputed bills. If you do something luridly stupid and rack up costs, AWS and GCP will probably cut you a break. We will too. Everyone does.\n\nIf you didn‚Äôt already know this, you‚Äôre welcome; I‚Äôve made your life a little better, even if you don‚Äôt run things on Fly.io.\n\nBut as soothing as it is to know you can get a break from cloud providers, the billing situation here is still a long ways away from ‚Äúgood‚Äù. If you accidentally add a zero to a scale count and don‚Äôt notice for several weeks, AWS or GCP will probably cut you a break. But they won‚Äôt definitely do it, and even though your odds are good, you‚Äôre still finding out at email- and phone-tag scale speeds. That‚Äôs not fun!\n\n\n## Explicit Accident Forgiveness\n\nCharging you for stuff you didn‚Äôt want is bad business.\n\nGood business, we think, means making you so comfortable with your cloud you try new stuff. You, and everyone else on your team. Without a chaperone from the finance department.\n\nSo we‚Äôre going to do the work to make this official. If you‚Äôre a customer of ours, we‚Äôre going to charge you in exacting detail for every resource you intentionally use of ours, but if something blows up and you get an unexpected bill, we‚Äôre going to let you off the hook.\n\n\n## Not So Fast\n\nThis is a Project, with a capital P. While we‚Äôre kind of kicking ourselves for not starting it earlier, there are reasons we couldn‚Äôt do it back in 2020.\n\nThe Fully Automated Accident-Forgiving Billing System of the Future (which we are in fact building and may even one day ship) will give you a line-item veto on your invoice. We are a long ways away. The biggest reason is fraud.\n\nSit back, close your eyes, and try to think about everything public clouds do to make your life harder. Chances are, most of those things are responses to fraud. Cloud platforms attract fraudsters like ants to an upturned ice cream cone. Thanks to the modern science of cryptography, fraudsters have had a 15 year head start on turning metered compute into picodollar-granular near-money assets.\n\nSince there‚Äôs no bouncer at the door checking IDs here, an open-ended and automated commitment to accident forgiveness is, with iron certainty, going to be used overwhelmingly in order to trick us into ‚Äúforgiving‚Äù cryptocurrency miners. We‚Äôre cloud platform engineers. They‚Äôre our primary pathogen.\n\nSo, we‚Äôre going to roll this out incrementally.\n\nWhy not billing alerts? We‚Äôll get there, but here too there are reasons we haven‚Äôt yet: (1) meaningful billing alerts were incredibly difficult to do with our previous billing system, and building the new system and migrating our customers to it has been a huge lift, a nightmare from which we are only now waking (the billing system‚Äôs official name); and (2) we‚Äôre wary about alerts being a product design cop-out; if we can alert on something, why aren‚Äôt we fixing it?\n\n\n## Accident Forgiveness v0.84beta\n\nAll the same subtextual, implied reassurances that every cloud provider offers remain in place at Fly.io. You are strictly better off after this announcement, we promise.\n\nI added the ‚Äúalmost‚Äù right before publishing, because I‚Äôm chicken.\n\nNow: for customers that have a support contract with us, at any level, there‚Äôs something new: I‚Äôm saying the quiet part loud. The next time you see a bill with an unexpected charge on it, we‚Äôll refund that charge, (almost) no questions asked.\n\nThat policy is so simple it feels anticlimactic to write. So, some additional color commentary:\n\nWe‚Äôre not advertising a limit to the number of times you can do this. If you‚Äôre a serious customer of ours, I promise that you cannot remotely fathom the fullness of our fellow-feeling. You‚Äôre not annoying us by getting us to refund unexpected charges. If you are growing a project on Fly.io, we will bend over backwards to keep you growing.\n\nHow far can we take this? How simple can we keep this policy? We‚Äôre going to find out together.\n\nTo begin with, and in the spirit of ‚Äúdoing things that won‚Äôt scale‚Äù, when we forgive a bill, what‚Äôs going to happen next is this: I‚Äôm going to set an irritating personal reminder for Kurt to look into what happened, now and then the day before your next bill, so we can see what‚Äôs going wrong. He‚Äôs going to hate that, which is the point: our best feature work is driven by Kurt-hate.\n\nObviously, if you‚Äôre rubbing your hands together excitedly over the opportunity this policy presents, then, well, not so much with the fellow-feeling. We reserve the right to cut you off.\n\nExplicit Accident Forgiveness is just one thing we like about Support at Fly.io.\n\n\n## What‚Äôs Next: Accident Protection\n\nWe think this is a pretty good first step. But that‚Äôs all it is.\n\nWe can do better than offering you easy refunds for mistaken deployments and botched CI/CD jobs. What‚Äôs better than getting a refund is never incurring the charge to begin with, and that‚Äôs the next step we‚Äôre working on.\n\nMore to come on that billing system.\n\nWe built a new billing system so that we can do things like that. For instance: we‚Äôre in a position to catch sudden spikes in your month-over-month bills, flag them, and catch weird-looking deployments before we bill for them.\n\nAnother thing we rebuilt billing for is reserved pricing. Already today you can get a steep discount from us reserving blocks of compute in advance. The trick to taking advantage of reserved pricing is confidently predicting a floor to your usage. For a lot of people, that means fighting feelings of loss aversion (nobody wants to get gym priced!). So another thing we can do in this same vein: catch opportunities to move customers to reserved blocks, and offer backdated reservations. We‚Äôll figure this out too.\n\nSomeday, when we‚Äôre in a monopoly position, our founders have all been replaced by ruthless MBAs, and Kurt has retired to farm coffee beans in lower Montana, we may stop doing this stuff. But until that day this is the right choice for our business.\n\nMeanwhile: like every public cloud, we provision our own hardware, and we have excess capacity. Your messed-up CI/CD jobs didn‚Äôt really cost us anything, so if you didn‚Äôt really want them, they shouldn‚Äôt cost you anything either. Take us up on this! We love talking to you."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/cutting-prices-for-l40s-gpus-in-half/",
    "content": "We‚Äôre Fly.io, a new public cloud with simple, developer-friendly ergonomics. And as of today, cheaper GPUs. Try it out; you‚Äôll be deployed in just minutes.\n\nWe just lowered the prices on NVIDIA L40s GPUs to $1.25 per hour. Why? Because our feet are cold and we burn processor cycles for heat. But also other reasons.\n\nLet‚Äôs back up.\n\nWe offer 4 different NVIDIA GPU models; in increasing order of performance, they‚Äôre the A10, the L40S, the 40G PCI A100, and the 80G SXM A100. Guess which one is most popular.\n\nWe guessed wrong, and spent a lot of time working out how to maximize the amount of GPU power we could deliver to a single Fly Machine. Users surprised us. By a wide margin, the most popular GPU in our inventory is the A10.\n\nThe A10 is an older generation of NVIDIA GPU with fewer, slower cores and less memory. It‚Äôs the least capable GPU we offer. But that doesn‚Äôt matter, because it‚Äôs capable enough. It‚Äôs solid for random inference tasks, and handles mid-sized generative AI stuff like Mistral Nemo or Stable Diffusion. For those workloads, there‚Äôs not that much benefit in getting a beefier GPU.\n\nAs a result, we can‚Äôt get new A10s in fast enough for our users.\n\nIf there‚Äôs one thing we‚Äôve learned by talking to our customers over the last 4 years, it‚Äôs that y'all love a peek behind the curtain. So we‚Äôre going to let you in on a little secret about how a hardware provider like Fly.io formulates GPU strategy: none of us know what the hell we‚Äôre doing.\n\nIf you had asked us in 2023 what the biggest GPU problem we could solve was, we‚Äôd have said ‚Äúselling fractional A100 slices‚Äù. We burned a whole quarter trying to get MIG, or at least vGPUs, working through IOMMU PCI passthrough on Fly Machines, in a project so cursed that Thomas has forsworn ever programming again. Then we went to market selling whole A100s, and for several more months it looked like the biggest problem we needed to solve was finding a secure way to expose NVLink-ganged A100 clusters to VMs so users could run training. Then H100s; can we find H100s anywhere? Maybe in a black market in Shenzhen?\n\nAnd here we are, a year later, looking at the data, and the least sexy, least interesting GPU part in the catalog is where all the action is.\n\nWith actual customer data to back up the hypothesis, here‚Äôs what we think is happening today:\n\nThis is a thing we didn‚Äôt see coming, but should have: training workloads tend to look more like batch jobs, and inference tends to look more like transactions. Batch training jobs aren‚Äôt that sensitive to networking or even reliability. Live inference jobs responding to end-user HTTP requests are. So, given our pricing, of course the A10s are a sweet spot.\n\nThe next step up in our lineup after the A10 is the L40S. The L40S is a nice piece of kit. We‚Äôre going to take a beat here and sell you on the L40S, because it‚Äôs kind of awesome.\n\nThe L40S is an AI-optimized version of the L40, which is the data center version of the GeForce RTX 4090, resembling two 4090s stapled together.\n\nIf you‚Äôre not a GPU hardware person, the RTX 4090 is a gaming GPU, the kind you‚Äôd play ray-traced Witcher 3 on. NVIDIA‚Äôs high-end gaming GPUs are actually reasonably good at AI workloads! But they suck in a data center rack: they chug power, they‚Äôre hard to cool, and they‚Äôre less dense. Also, NVIDIA can‚Äôt charge as much for them.\n\nHence the L40: (much) more memory, less energy consumption, designed for a rack, not a tower case. Marked up for ‚Äúenterprise‚Äù.\n\nNVIDIA positioned the L40 as a kind of ‚Äúgraphics‚Äù AI GPU. Unlike the super high-end cards like the A100/H100, the L40 keeps all the rendering hardware, so it‚Äôs good for 3D graphics and video processing. Which is sort of what you‚Äôd expect from a ‚Äúprofessionalized‚Äù GeForce card.\n\nA funny thing happened in the middle of 2023, though: the market for ultra-high-end NVIDIA cards went absolutely batshit. The huge cards you‚Äôd gang up for training jobs got impossible to find, and NVIDIA became one of the most valuable companies in the world. Serious shops started working out plans to acquire groups of L40-type cards to work around the problem, whether or not they had graphics workloads.\n\nThe only company in this space that does know what they‚Äôre doing is NVIDIA. Nobody has written a highly-ranked Reddit post about GPU workloads without NVIDIA noticing and creating a new SKU. So they launched the L40S, which is an L40 with AI workload compute performance comparable to that of the A100 (without us getting into the details of F32 vs. F16 models).\n\nLong story short, the L40S is an A100-performer that we can price for A10 customers; the Volkswagen GTI of our lineup. We‚Äôre going to see if we can make that happen.\n\nWe think the combination of just-right-sized inference GPUs and Tigris object storage is pretty killer:\n\nYou should use L40S cards without thinking hard about it. So we‚Äôre making it official. You won‚Äôt pay us a dime extra to use one instead of an A10. Have at it! Revolutionize the industry. For $1.25 an hour.\n\nHere are things you can do with an L40S on Fly.io today:\n\nIt‚Äôs going to get chilly in Chicago in a month or so. Go light some cycles on fire!"
  },
  {
    "title": "Speedrun your app onto Fly.io.",
    "url": "https://fly.io/blog/machine-migrations/",
    "content": "We‚Äôre Fly.io, a global public cloud with simple, developer-friendly ergonomics. If you‚Äôve got a working Docker image, we‚Äôll transmogrify it into a Fly Machine: a VM running on our hardware anywhere in the world. Try it out; you‚Äôll be deployed in just minutes.\n\nAt the heart of our platform is a systems design tradeoff about durable storage for applications. When we added storage three years ago, to support stateful apps, we built it on attached NVMe drives. A benefit: a Fly App accessing a file on a Fly Volume is never more than a bus hop away from the data. A cost: a Fly App with an attached Volume is anchored to a particular worker physical.\n\nbird: a BGP4 route server.\n\nBefore offering attached storage, our on-call runbook was almost as simple as ‚Äúde-bird that edge server‚Äù, ‚Äútell Nomad to drain that worker‚Äù, and ‚Äúgo back to sleep‚Äù. NVMe cost us that drain operation, which terribly complicated the lives of our infra team. We‚Äôve spent the last year getting ‚Äúdrain‚Äù back. It‚Äôs one of the biggest engineering lifts we‚Äôve made, and if you didn‚Äôt notice, we lifted it cleanly.\n\n\n## The Goalposts\n\nWith stateless apps, draining a worker is easy. For each app instance running on the victim server, start a new instance elsewhere. Confirm it‚Äôs healthy, then kill the old one. Rinse, repeat. At our 2020 scale, we could drain a fully loaded worker in just a handful of minutes.\n\nYou can see why this process won‚Äôt work for apps with attached volumes. Sure, create a new volume elsewhere on the fleet, and boot up a new Fly Machine attached to it. But the new volume is empty. The data‚Äôs still stuck on the original worker. We asked, and customers were not OK with this kind of migration.\n\nOf course, we back Volumes snapshots up (at an interval) to off-network storage. But for ‚Äúdrain‚Äù, restoring backups isn‚Äôt nearly good enough. No matter the backup interval, a ‚Äúrestore from backup migration\" will lose data, and a ‚Äúbackup and restore‚Äù migration incurs untenable downtime.\n\nThe next thought you have is, ‚ÄúOK, copy the volume over‚Äù. And, yes, of course you have to do that. But you can‚Äôt just copy, boot, and then kill the old Fly Machine. Because the original Fly Machine is still alive and writing, you have to killfirst, then copy, then boot.\n\nFly Volumes can get pretty big. Even to a rack buddy physical server, you‚Äôll hit a point where draining incurs minutes of interruption, especially if you‚Äôre moving lots of volumes simultaneously. Kill, copy, boot is too slow.\n\nThere‚Äôs a world where even 15 minutes of interruption is tolerable. It‚Äôs the world where you run more than one instance of your application to begin with, so prolonged interruption of a single Fly Machine isn‚Äôt visible to your users. Do this! But we have to live in the same world as our customers, many of whom don‚Äôt run in high-availability configurations.\n\n\n## Behold The Clone-O-Mat\n\nCopy, boot, kill loses data. Kill, copy, boot takes too long. What we needed is a new operation: clone.\n\nClone is a lazier, asynchronous copy. It creates a new volume elsewhere on our fleet, just like copy would. But instead of blocking, waiting to transfer every byte from the original volume, clone returns immediately, with a transfer running in the background.\n\nA new Fly Machine can be booted with that cloned volume attached. Its blocks are mostly empty. But that‚Äôs OK: when the new Fly Machine tries to read from it, the block storage system works out whether the block has been transferred. If it hasn‚Äôt, it‚Äôs fetched over the network from the original volume; this is called ‚Äúhydration‚Äù. Writes are even easier, and don‚Äôt hit the network at all.\n\nKill, copy, boot is slow. But kill, clone, boot is fast; it can be made asymptotically as fast as stateless migration.\n\nThere are three big moving pieces to this design.\n\n\n## Block-Level Clone\n\nThe Linux feature we need to make this work already exists; it‚Äôs called dm-clone. Given an existing, readable storage device, dm-clone gives us a new device, of identical size, where reads of uninitialized blocks will pull from the original. It sounds terribly complicated, but it‚Äôs actually one of the simpler kernel lego bricks. Let‚Äôs demystify it.\n\nAs far as Unix is concerned, random-access storage devices, be they spinning rust or NVMe drives, are all instances of the common class ‚Äúblock device‚Äù. A block device is addressed in fixed-size (say, 4KiB) chunks, and handles (roughly) these operations:\n\nYou can imagine designing a simple network protocol that supported all these options. It might have messages that looked something like:\n\nGood news! The Linux block system is organized as if your computer was a network running a protocol that basically looks just like that. Here‚Äôs the message structure:\n\nI‚Äôve stripped a bunch of stuff out of here but you don‚Äôt need any of it to understand what‚Äôs coming next.\n\nNo nerd has ever looked at a fixed-format message like this without thinking about writing a proxy for it, and struct bio is no exception. The proxy system in the Linux kernel for struct bio is called device mapper, or DM.\n\nDM target devices can plug into other DM devices. For that matter, they can do whatever the hell else they want, as long as they honor the interface. It boils down to a map(bio) function, which can dispatch a struct bio, or drop it, or muck with it and ask the kernel to resubmit it.\n\nYou can do a whole lot of stuff with this interface: carve a big device into a bunch of smaller ones (dm-linear), make one big striped device out of a bunch of smaller ones (dm-stripe), do software RAID mirroring (dm-raid1), create snapshots of arbitrary existing devices (dm-snap), cryptographically verify boot devices (dm-verity), and a bunch more. Device Mapper is the kernel backend for the userland LVM2 system, which is how we do thin pools and snapshot backups.\n\nWhich brings us to dm-clone : it‚Äôs a map function that boils down to:\n\na kcopyd thread runs in the background, rehydrating the device in addition to (and independent of) read accesses.\n\ndm-clone takes, in addition to the source device to clone from, a ‚Äúmetadata‚Äù device on which is stored a bitmap of the status of all the blocks: either ‚Äúrehydrated‚Äù from the source, or not. That‚Äôs how it knows whether to fetch a block from the original device or the clone.\n\n\n## Network Clone\n\nflyd in a nutshell: worker physical run a service, flyd, which manages a couple of databases that are the source of truth for all the Fly Machines running there. Concepturally, flyd is a server for on-demand instances of durable finite state machines, each representing some operation on a Fly Machine (creation, start, stop, \u0026c), with the transition steps recorded carefully in a BoltDB database. An FSM step might be something like ‚Äúassign a local IPv6 address to this interface‚Äù, or ‚Äúcheck out a block device with the contents of this container‚Äù, and it‚Äôs straightforward to add and manage new ones.\n\nSay we‚Äôve got flyd managing a Fly Machine with a volume on worker-xx-cdg1-1. We want it running on worker-xx-cdg1-2. Our whole fleet is meshed with WireGuard; everything can talk directly to everything else. So, conceptually:\n\nFor step (3) to work, the ‚Äúoriginal volume‚Äù on cdg1-1 has to be visible on cdg1-2, which means we need to mount it over the network.\n\nnbd is so simple that it‚Äôs used as a sort of dm-user userland block device; to prototype a new block device, don‚Äôt bother writing a kernel module, just write an nbd server.\n\nTake your pick of protocols. iSCSI is the obvious one, but it‚Äôs relatively complicated, and Linux has native support for a much simpler one: nbd, the ‚Äúnetwork block device‚Äù. You could implement an nbd server in an afternoon, on top of a file or a SQLite database or S3, and the Linux kernel could mount it as a drive.\n\nWe started out using nbd. But we kept getting stuck nbd kernel threads when there was any kind of network disruption. We‚Äôre a global public cloud; network disruption happens. Honestly, we could have debugged our way through this. But it was simpler just to spike out an iSCSI implementation, observe that didn‚Äôt get jammed up when the network hiccuped, and move on.\n\n\n## Putting The Pieces Together\n\nTo drain a worker with minimal downtime and no lost data, we turn workers into a temporary SANs, serving the volumes we need to drain to fresh-booted replica Fly Machines on a bunch of ‚Äútarget‚Äù physicals. Those SANs ‚Äî combinations of dm-clone, iSCSI, and our flyd orchestrator ‚Äî track the blocks copied from the origin, copying each one exactly once and cleaning up when the original volume has been fully copied.\n\nProblem solved!\n\n\n## No, There Were More Problems\n\nWhen your problem domain is hard, anything you build whose design you can‚Äôt fit completely in your head is going to be a fiasco. Shorter form: ‚Äúif you see Raft consensus in a design, we‚Äôve done something wrong‚Äù.\n\nA virtue of this migration system is that, for as many moving pieces as it has, it fits in your head. What complexity it has is mostly shouldered by strategic bets we‚Äôve already built teams around, most notably the flyd orchestrator. So we‚Äôve been running this system for the better part of a year without much drama. Not no drama, though. Some drama.\n\nExample: we encrypt volumes. Our key management is fussy. We do per-volume encryption keys that provision alongside the volumes themselves, so no one worker has a volume skeleton key.\n\nIf you think ‚Äúmigrating those volume keys from worker to worker‚Äù is the problem I‚Äôm building up to, well, that too, but the bigger problem is trim.\n\nMost people use just a small fraction of the volumes they allocate. A 100GiB volume with just 5MiB used wouldn‚Äôt be at all weird. You don‚Äôt want to spend minutes copying a volume that could have been fully hydrated in seconds.\n\nAnd indeed, dm-clone doesn‚Äôt want to do that either. Given a source block device (for us, an iSCSI mount) and the clone device, a DISCARD issued on the clone device will get picked up by dm-clone, which will simply short-circuit the read of the relevant blocks by marking them as hydrated in the metadata volume. Simple enough.\n\nTo make that work, we need the target worker to see the plaintext of the source volume (so that it can do an fstrim ‚Äî don‚Äôt get us started on how annoying it is to sandbox this ‚Äî to read the filesystem, identify the unused block, and issue the DISCARDs where dm-clone can see them) Easy enough.\n\nthese curses have a lot to do with how hard it was to drain workers!\n\nExcept: two different workers, for cursed reasons, might be running different versions of cryptsetup, the userland bridge between LUKS2 and the kernel dm-crypt driver. There are (or were) two different versions of cryptsetup on our network, and they default to different LUKS2 header sizes ‚Äî 4MiB and 16MiB. Implying two different plaintext volume sizes.\n\nSo now part of the migration FSM is an RPC call that carries metadata about the designed LUKS2 configuration for the target VM. Not something we expected to have to build, but, whatever.\n\nCorrosion deserves its own post.\n\nGnarlier example: workers are the source of truth for information about the Fly Machines running on them. Migration knocks the legs out from under that constraint, which we were relying on in Corrosion, the SWIM-gossip SQLite database we use to connect Fly Machines to our request routing. Race conditions. Debugging. Design changes. Next!\n\nGnarliest example: our private networks. Recall: we automatically place every Fly Machine into a private network; by default, it‚Äôs the one all the other apps in your organization run in. This is super handy for setting up background services, databases, and clustered applications. 20 lines of eBPF code in our worker kernels keeps anybody from ‚Äúcrossing the streams‚Äù, sending packets from one private network to another.\n\nwe‚Äôre members of an elite cadre of idiots who have managed to find designs that made us wish IPv6 addresses were even bigger.\n\nWe call this scheme 6PN (for ‚ÄúIPv6 Private Network‚Äù). It functions by embedding routing information directly into IPv6 addresses. This is, perhaps, gross. But it allows us to route diverse private networks with constantly changing membership across a global fleet of servers without running a distributed routing protocol. As the beardy wizards who kept the Internet backbone up and running on Cisco AGS+‚Äôs once said: the best routing protocol is ‚Äústatic‚Äù.\n\nProblem: the embedded routing information in a 6PN address refers in part to specific worker servers.\n\nThat‚Äôs fine, right? They‚Äôre IPv6 addresses. Nobody uses literal IPv6 addresses. Nobody uses IP addresses at all; they use the DNS. When you migrate a host, just give it a new 6PN address, and update the DNS.\n\nFriends, somebody did use literal IPv6 addresses. It was us. In the configurations for Fly Postgres clusters.\n\nIt‚Äôs also not operationally easy for us to shell into random Fly Machines, for good reason.\n\nThe obvious fix for this is not complicated; given flyctl ssh access to a Fly Postgres cluster, it‚Äôs like a 30 second ninja edit. But we run a lot of Fly Postgres clusters, and the change has to be coordinated carefully to avoid getting the cluster into a confused state.¬†We went as far as adding feature to our init to do network address mappings to keep old 6PN addresses reachable before biting the bullet and burning several weeks doing the direct configuration fix fleet-wide.\n\n3‚Ä¶2‚Ä¶1‚Ä¶\n\n\n## The Learning, It Burns!\n\nWe get asked a lot why we don‚Äôt do storage the ‚Äúobvious‚Äù way, with an EBS-type SAN fabric, abstracting it away from our compute. Locally-attached NVMe storage is an idiosyncratic choice, one we‚Äôve had to write disclaimers for (single-node clusters can lose data!) since we first launched it.\n\nOne answer is: we‚Äôre a startup. Building SAN infrastructure in every region we operate in would be tremendously expensive. Look at any feature in AWS that normal people know the name of, like EC2, EBS, RDS, or S3 ‚Äî there‚Äôs a whole company in there. We launched storage when we were just 10 people, and even at our current size we probably have nothing resembling the resources EBS gets. AWS is pretty great!\n\nBut another thing to keep in mind is: we‚Äôre learning as we go. And so even if we had the means to do an EBS-style SAN, we might not build it today.\n\nInstead, we‚Äôre a lot more interested in log-structured virtual disks (LSVD). LSVD uses NVMe as a local cache, but durably persists writes in object storage. You get most of the performance benefit of bus-hop disk writes, along with unbounded storage and S3-grade reliability.\n\nWe launched LSVD experimentally last year; in the intervening year, something happened to make LSVD even more interesting to us: Tigris Data launched S3-compatible object storage in every one our regions, so instead of backhauling updates to Northern Virginia, we can keep them local. We have more to say about LSVD, and a lot more to say about Tigris.\n\nOur first several months of migrations were done gingerly. By summer of 2024, we got to where our infra team can pull ‚Äúdrain this host‚Äù out of their toolbelt without much ceremony.\n\nWe‚Äôre still not to the point where we‚Äôre migrating casually. Your Fly Machines are probably not getting migrated! There‚Äôd need to be a reason! But the dream is fully-automated luxury space migration, in which you might get migrated semiregularly, as our systems work not just to drain problematic hosts but to rebalance workloads regularly. No time soon. But we‚Äôll get there.\n\nThis is the biggest thing our team has done since we replaced Nomad with flyd. Only the new billing system comes close. We did this thing not because it was easy, but because we thought it would be easy. It was not. But: worth it!"
  },
  {
    "title": "Speedrun your app onto Fly.io.",
    "url": "https://fly.io/blog/oidc-cloud-roles/",
    "content": "It‚Äôs dangerous to go alone. Fly.io runs full-stack apps by transmuting Docker containers into Fly Machines: ultra-lightweight hardware-backed VMs. You can run all your dependencies on Fly.io, but sometimes, you‚Äôll need to work with other clouds, and we‚Äôve made that pretty simple. Try Fly.io out for yourself; your Rails or Node app can be up and running in just minutes.\n\nLet‚Äôs hypopulate you an app serving generative AI cat images based on the weather forecast, running on a g4dn.xlarge ECS task in AWS us-east-1. It‚Äôs going great; people didn‚Äôt realize how dependent their cat pic prefs are on barometric pressure, and you‚Äôre all anyone can talk about.\n\nWord reaches Australia and Europe, but you‚Äôre not catching on, because the‚Ä¶ latency is too high? Just roll with us here. Anyways: fixing this is going to require replicating ECS tasks and ECR images into ap-southeast-2 and eu-central-1 while also setting up load balancing. Nah.\n\nThis is the O.G. Fly.io deployment story; one deployed app, one versioned container, one command to get it running anywhere in the world.\n\nBut you have a problem: your app relies on training data, it‚Äôs huge, your giant employer manages it, and it‚Äôs in S3. Getting this to work will require AWS credentials.\n\nYou could ask your security team to create a user, give it permissions, and hand over the AWS keypair. Then you could wash your neck and wait for the blade. Passing around AWS keypairs is the beginning of every horror story told about cloud security, and security team ain‚Äôt having it.\n\nThere‚Äôs a better way. It‚Äôs drastically more secure, so your security people will at least hear you out. It‚Äôs also so much easier on Fly.io that you might never bother creating a IAM service account again.\n\n\n## Let‚Äôs Get It out of the Way\n\nWe‚Äôre going to use OIDC to set up strictly limited trust between AWS and Fly.io.\n\nOur machines will now magically have access to the S3 bucket.\n\n\n## What the What\n\nA reasonable question to ask here is, ‚Äúwhere‚Äôs the credential‚Äù? Ordinarily, to give a Fly Machine access to an AWS resource, you‚Äôd use fly secrets set to add an AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to the environment in the Machine. Here, we‚Äôre not setting any secrets at all; we‚Äôre just adding an ARN ‚Äî which is not a credential ‚Äî to the Machine.\n\nHere‚Äôs what‚Äôs happening.\n\nFly.io operates an OIDC IdP at oidc.fly.io. It issues OIDC tokens, exclusively to Fly Machines. AWS can be configured to trust these tokens, on a role-by-role basis. That‚Äôs the ‚Äúsecret credential‚Äù: the pre-configured trust relationship in IAM, and the public keypairs it manages. You, the user, never need to deal with these keys directly; it all happens behind the scenes, between AWS and Fly.io.\n\nThe key actor in this picture is STS, the AWS Security Token Service. STS‚Äòs main job is to vend short-lived AWS credentials, usually through some variant of an API called AssumeRole. Specifically, in our case: AssumeRoleWithWebIdentity tells STS to cough up an AWS keypair given an OIDC token (that matches a pre-configured trust relationship).\n\nThat still leaves the question: how does your code, which is reaching out to the AWS APIs to get cat weights, drive any of this?\n\n\n## The Init Thickens\n\nEvery Fly Machine boots up into an init we wrote in Rust. It has slowly been gathering features.\n\nOne of those features, which has been around for awhile, is a server for a Unix socket at /.fly/api, which exports a subset of the Fly Machines API to privileged processes in the Machine. Think of it as our answer to the EC2 Instant Metadata Service. How it works is, every time we boot a Fly Machine, we pass it a Macaroon token locked to that particular Machine; init‚Äôs server for /.fly/api is a proxy that attaches that token to requests.\n\nIn addition to the API proxy being tricky to SSRF to.\n\nWhat‚Äôs neat about this is that the credential that drives /.fly/api is doubly protected:\n\nYou could rig up a local privilege escalation vulnerability and work out how to steal the Macaroon, but you can‚Äôt exfiltrate it productively.\n\nSo now you have half the puzzle worked out: OIDC is just part of the Fly Machines API (specifically: /v1/tokens/oidc). A Fly Machine can hit a Unix socket and get an OIDC token tailored to that machine:\n\nLook upon this holy blob, sealed with a published key managed by Fly.io‚Äôs OIDC vault, and see that there lies within it enough information for AWS STS to decide to issue a session credential.\n\nWe have still not completed the puzzle, because while you can probably now see how you‚Äôd drive this process with a bunch of new code that you‚Äôd tediously write, you are acutely aware that you have not yet endured that tedium ‚Äî e pur si muove!\n\nOne init feature remains to be disclosed, and it‚Äôs cute.\n\nIf, when init starts in a Fly Machine, it sees an AWS_ROLE_ARN environment variable set, it initiates a little dance; it:\n\nThe AWS SDK, linked to your application, does all the rest.\n\nLet‚Äôs review: you add an AWS_ROLE_ARN variable to your Fly App, launch a Machine, and have it go fetch a file from S3. What happens next is:\n\n\n## How Much Better Is This?\n\nIt is a lot better.\n\nThey asymptotically approach the security properties of Macaroon tokens.\n\nMost importantly: AWS STS credentials are short-lived. Because they‚Äôre generated dynamically, rather than stored in a configuration file or environment variable, they‚Äôre already a little bit annoying for an attacker to recover. But they‚Äôre also dead in minutes. They have a sharply limited blast radius. They rotate themselves, and fail closed.\n\nThey‚Äôre also easier to manage. This is a rare instance where you can reasonably drive the entire AWS side of the process from within the web console. Your cloud team adds Roles all the time; this is just a Role with an extra snippet of JSON. The resulting ARN isn‚Äôt even a secret; your cloud team could just email or Slack message it back to you.\n\nFinally, they offer finer-grained control.\n\nTo understand the last part, let‚Äôs look at that extra snippet of JSON (the ‚ÄúTrust Policy‚Äù) your cloud team is sticking on the new cat-bucket Role:\n\nThe aud check guarantees STS will only honor tokens that Fly.io deliberately vended for STS.\n\nRecall the OIDC token we dumped earlier; much of what‚Äôs in it, we can match in the Trust Policy. Every OIDC token Fly.io generates is going to have a sub field formatted org:app:machine, so we can lock IAM Roles down to organizations, or to specific Fly Apps, or even specific Fly Machine instances.\n\n3‚Ä¶2‚Ä¶1‚Ä¶\n\n\n## And So\n\nIn case it‚Äôs not obvious: this pattern works for any AWS API, not just S3.\n\nOur OIDC support on the platform and in Fly Machines will set arbitrary OIDC audience strings, so you can use it to authenticate to any OIDC-compliant cloud provider. It won‚Äôt be as slick on Azure or GCP, because we haven‚Äôt done the init features to light their APIs up with a single environment variable ‚Äî but those features are easy, and we‚Äôre just waiting for people to tell us what they need.\n\nFor us, the gold standard for least-privilege, conditional access tokens remains Macaroons, and it‚Äôs unlikely that we‚Äôre going to do a bunch of internal stuff using OIDC. We even snuck Macaroons into this feature. But the security you‚Äôre getting from this OIDC dance closes a lot of the gap between hardcoded user credentials and Macaroons, and it‚Äôs easy to use ‚Äî easier, in some ways, than it is to manage role-based access inside of a legacy EC2 deployment!"
  },
  {
    "title": "This can run on Fly.io.",
    "url": "https://fly.io/blog/llm-image-description/",
    "content": "I‚Äôm Nolan, and I work on Fly Machines here at Fly.io. We‚Äôre building a new public cloud‚Äîone where you can spin up CPU and GPU workloads, around the world, in a jiffy. Try us out; you can be up and running in minutes. This is a post about LLMs being really helpful, and an extensible project you can build with open source on a weekend.\n\nPicture this, if you will.\n\nYou‚Äôre blind. You‚Äôre in an unfamiliar hotel room on a trip to Chicago.\n\nIf you live in Chicago IRL, imagine the hotel in Winnipeg, the Chicago of the North.\n\nYou‚Äôve absent-mindedly set your coffee down, and can‚Äôt remember where. You‚Äôre looking for the thermostat so you don‚Äôt wake up frozen. Or, just maybe, you‚Äôre playing a fun-filled round of ‚Äúfind the damn light switch so your sighted partner can get some sleep already!‚Äù\n\nIf, like me, you‚Äôve been blind for a while, you have plenty of practice finding things without the luxury of a quick glance around. It may be more tedious than you‚Äôd like, but you‚Äôll get it done.\n\nBut the speed of innovation in machine learning and large language models has been dizzying, and in 2024 you can snap a photo with your phone and have an app like Be My AI or Seeing AI tell you where in that picture it found your missing coffee mug, or where it thinks the light switch is.\n\nCreative switch locations seem to be a point of pride for hotels, so the light game may be good for a few rounds of quality entertainment, regardless of how good your AI is.\n\nThis is big. It‚Äôs hard for me to state just how exciting and empowering AI image descriptions have been for me without sounding like a shill. In the past year, I‚Äôve:\n\nI‚Äôve been consistently blown away at how impressive and helpful AI-created image descriptions have been.\n\nAlso‚Ä¶\n\n\n## Which thousand words is this picture worth?\n\nAs a blind internet user for the last three decades, I have extensive empirical evidence to corroborate what you already know in your heart: humans are pretty flaky about writing useful alt text for all the images they publish. This does tend to make large swaths of the internet inaccessible to me!\n\nIn just a few years, the state of image description on the internet has gone from complete reliance on the aforementioned lovable, but ultimately tragically flawed, humans, to automated strings of words like Image may contain person, glasses, confusion, banality, disillusionment, to LLM-generated text that reads a lot like it was written by a person, perhaps sipping from a steaming cup of Earl Grey as they reflect on their previous experiences of a background that features a tree with snow on its branches, suggesting that this scene takes place during winter.\n\nIf an image is missing alt text, or if you want a second opinion, there are screen-reader addons, like this one for NVDA, that you can use with an API key to get image descriptions from GPT-4 or Google Gemini as you read. This is awesome!\n\nAnd this brings me to the nerd snipe. How hard would it be to build an image description service we can host ourselves, using open source technologies? It turns out to be spookily easy.\n\nHere‚Äôs what I came up with:\n\nThe idea is to keep it modular and hackable, so if sentiment analysis or joke creation is your thing, you can swap out image description for that and have something going in, like, a weekend.\n\nIf you‚Äôre like me, and you go skipping through recipe blogs to find the ‚Äúgo directly to recipe‚Äù link, find the code itself here.\n\n\n## The LLM is the easiest part\n\nAn API to accept images and prompts, run the model, and spit out answers sounds like a lot! But it‚Äôs the simplest part of this whole thing, because: that‚Äôs Ollama.\n\nYou can just run the Ollama Docker image, get it to grab the model you want to use, and that‚Äôs it. There‚Äôs your AI server. (We have a blog post all about deploying Ollama on Fly.io; Fly GPUs are rad, try'em out, etc.).\n\nFor this project, we need a model that can make sense‚Äîor at least words‚Äîout of a picture. LLaVA is a trained, Apache-licensed ‚Äúlarge multimodal model‚Äù that fits the bill. Get the model with the Ollama CLI:\n\nIf you have hardware that can handle it, you could run this on your computer at home. If you run AI models on a cloud provider, be aware that GPU compute is expensive! It‚Äôs important to take steps to ensure you‚Äôre not paying for a massive GPU 24/7.\n\nOn Fly.io, at the time of writing, you‚Äôd achieve this with the autostart and autostop functions of the Fly Proxy, restricting Ollama access to internal requests over Flycast from the PocketBase app. That way, if there haven‚Äôt been any requests for a few minutes, the Fly Proxy stops the Ollama Machine, which releases the CPU, GPU, and RAM allocated to it. Here‚Äôs a post that goes into more detail.\n\n\n## A multi-tool on the backend\n\nI want user auth to make sure just anyone can‚Äôt grab my ‚Äúimage description service‚Äù and keep it busy generating short stories about their cat. If I build this out into a service for others to use, I might also want business logic around plans or credits, or mobile-friendly APIs for use in the field. PocketBase provides a scaffolding for all of it. It‚Äôs a Swiss army knife: a Firebase-like API on top of SQLite, complete with authentication, authorization, an admin UI, extensibility in JavaScript and Go, and various client-side APIs.\n\nYes, of course I‚Äôve used an LLM to generate feline fanfic. Theme songs too. Hasn‚Äôt everyone?\n\nI ‚Äúfaked‚Äù a task-specific API that supports followup questions by extending PocketBase in Go, modeling requests and responses as collections (i.e. SQLite tables) with event hooks to trigger pre-set interactions with the Ollama app (via LangChainGo) and the client (via the PocketBase API).\n\nIf you‚Äôre following along, here‚Äôs the module that handles all that, along with initializing the LLM connection.\n\nIn a nutshell, this is the dance:\n\nThis is a super simple hack to handle followup questions, and it‚Äôll let you keep adding followups until something breaks. You‚Äôll see the quality of responses get poorer‚Äîpossibly incoherent‚Äîas the context exceeds the context window.\n\nI also set up API rules in PocketBase, ensuring that users can‚Äôt read to and write from others‚Äô chats with the AI.\n\nIf image descriptions aren‚Äôt your thing, this business logic is easily swappable for joke generation, extracting details from text, any other simple task you might want to throw at an LLM. Just slot the best model into Ollama (LLaVA is pretty OK as a general starting point too), and match the PocketBase schema and pre-set prompts to your application.\n\n\n## A seedling of a client\n\nWith the image description service in place, the user can talk to it with any client that speaks the PocketBase API. PocketBase already has SDK clients in JavaScript and Dart, but because my screen reader is written in Python, I went with a community-created Python library. That way I can build this out into an NVDA add-on if I want to.\n\nIf you‚Äôre a fancy Python developer, you probably have your preferred tooling for handling virtualenvs and friends. I‚Äôm not, and since my screen reader doesn‚Äôt use those anyway, I just pip installed the library so my client can import it:\n\nMy client is a very simple script. It expects a couple of things: a file called image.jpg, located in the current directory, and environment variables to provide the service URL and user credentials to log into it with.\n\nWhen you run the client script, it uploads the image to the user‚Äôs images collection on the backend app, starting the back-and-forth between user and model we saw in the previous section. The client prints the model‚Äôs output to the CLI and prompts the user to input a followup question, which it passes up to the followups collection, and so on.\n\nRun your LLM on a datacenter-grade GPU.\n\n\n## All together now\n\nI grabbed this image and saved it to a file called image.jpg.\n\nWhile I knew I was downloading an image of a winter scene, all I see on Unsplash is:\n\nbrown trees beside river under blue sky during daytime Bright winter landscape with lake, snow, forest, beautiful blue sky and white clouds. An example of charming wildlife in Russia.\n\nLet‚Äôs see what our very own AI describer thinks of this picture:\n\nIs it a stellar description? Maybe not, but it certainly gives me a better sense of connection with the scene.\n\nLet‚Äôs see how our describer copes with a followup question.\n\nBoo, the general-purpose LLaVA model couldn‚Äôt identify the leafless trees. At least it knows why it can‚Äôt. Maybe there‚Äôs a better model out there for that. Or we could train one, if we really needed tree identification! We could make every component of this service more sophisticated!\n\nBut that I, personally, can make a proof of concept like this with a few days of effort continues to boggle my mind. Thanks to a handful of amazing open source projects, it‚Äôs really, spookily, easy. And from here, I (or you) can build out a screen-reader addon, or a mobile app, or a different kind of AI service, with modular changes.\n\n\n## Deployment notes\n\nOn Fly.io, stopping GPU Machines saves you a bunch of money and some carbon footprint, in return for cold-start latency when you make a request for the first time in more than a few minutes. In testing this project, on the a100-40gb Fly Machine preset, the 34b-parameter LLaVA model took several seconds to generate each response. If the Machine was stopped when the request came in, starting it up took another handful of seconds, followed by several tens of seconds to load the model into GPU RAM. The total time from cold start to completed description was about 45 seconds. Just something to keep in mind.\n\nIf you‚Äôre running Ollama in the cloud, you likely want to put the model onto storage that‚Äôs persistent, so you don‚Äôt have to download it repeatedly. You could also build the model into a Docker image ahead of deployment.\n\nThe PocketBase Golang app compiles to a single executable that you can run wherever. I run it on Fly.io, unsurprisingly, and the repo comes with a Dockerfile and a fly.toml config file, which you can edit to point at your own Ollama instance. It uses a small persistent storage volume for the SQLite database. Under testing, it runs fine on a shared-cpu-1x Machine."
  },
  {
    "title": "Launch an app in minutes",
    "url": "https://fly.io/blog/jit-wireguard-peers/",
    "content": "We‚Äôre Fly.io and we transmute containers into VMs, running them on our hardware around the world with the power of Firecracker alchemy. We do a lot of stuff with WireGuard, which has become a part of our customer API. This is a quick story about some tricks we played to make WireGuard faster and more scalable for the hundreds of thousands of people who now use it here.\n\nOne of many odd decisions we‚Äôve made at Fly.io is how we use WireGuard. It‚Äôs not just that we use it in many places where other shops would use HTTPS and REST APIs. We‚Äôve gone a step beyond that: every time you run flyctl, our lovable, sprawling CLI, it conjures a TCP/IP stack out of thin air, with its own IPv6 address, and speaks directly to Fly Machines running on our networks.\n\nThere are plusses and minuses to this approach, which we talked about in a blog post a couple years back. Some things, like remote-operated Docker builders, get easier to express (a Fly Machine, as far as flyctl is concerned, might as well be on the same LAN). But everything generally gets trickier to keep running reliably.\n\nIt was a decision. We own it.\n\nAnyways, we‚Äôve made some improvements recently, and I‚Äôd like to talk about them.\n\n\n## Where we left off\n\nUntil a few weeks ago, our gateways ran on a pretty simple system.\n\nI copy-pasted those last two bullet points from that two-year-old post, because when it works, it does just work reasonably well. (We ultimately did end up defaulting everybody to WireGuard-over-WebSockets, though.)\n\nBut if it always worked, we wouldn‚Äôt be here, would we?\n\nWe ran into two annoying problems:\n\nOne: NATS is fast, but doesn‚Äôt guarantee delivery. Back in 2022, Fly.io was pretty big on NATS internally. We‚Äôve moved away from it. For instance, our internal flyd API used to be driven by NATS; today, it‚Äôs HTTP. Our NATS cluster was losing too many messages to host a reliable API on it. Scaling back our use of NATS made WireGuard gateways better, but still not great.\n\nTwo: When flyctl exits, the WireGuard peer it created sticks around on the gateway. Nothing cleans up old peers. After all, you‚Äôre likely going to come back tomorrow and deploy a new version of your app, or fly ssh console into it to debug something. Why remove a peer just to re-add it the next day?\n\nUnfortunately, the vast majority of peers are created by flyctl in CI jobs, which don‚Äôt have persistent storage and can‚Äôt reconnect to the same peer the next run; they generate new peers every time, no matter what.\n\nSo, we ended up with a not-reliable-enough provisioning system, and gateways with hundreds of thousands of peers that will never be used again. The high stale peer count made kernel WireGuard operations very slow - especially loading all the peers back into the kernel after a gateway server reboot - as well as some kernel panics.\n\nThere had to be\n\n\n## A better way.\n\nStoring bajillions of WireGuard peers is no big challenge for any serious n-tier RDBMS. This isn‚Äôt ‚Äúbig data‚Äù. The problem we have at Fly.io is that our gateways don‚Äôt have serious n-tier RDBMSs. They‚Äôre small. Scrappy. They live off the land.\n\nSeriously, though: you could store every WireGuard peer everybody has ever used at Fly.io in a single SQLite database, easily. What you can‚Äôt do is store them all in the Linux kernel.\n\nSo, at some point, as you push more and more peer configurations to a gateway, you have to start making decisions about which peers you‚Äôll enable in the kernel, and which you won‚Äôt.\n\nWouldn‚Äôt it be nice if we just didn‚Äôt have this problem? What if, instead of pushing configs to gateways, we had the gateways pull them from our API on demand?\n\nIf you did that, peers would only have to be added to the kernel when the client wanted to connect. You could yeet them out of the kernel any time you wanted; the next time the client connected, they‚Äôd just get pulled again, and everything would work fine.\n\nThe problem you quickly run into to build this design is that Linux kernel WireGuard doesn‚Äôt have a feature for installing peers on demand. However:\n\n\n## It is possible to JIT WireGuard peers\n\nThe Linux kernel‚Äôs interface for configuring WireGuard is Netlink (which is basically a way to create a userland socket to talk to a kernel service). Here‚Äôs a summary of it as a C API. Note that there‚Äôs no API call to subscribe for ‚Äúincoming connection attempt‚Äù events.\n\nThat‚Äôs OK! We can just make our own events. WireGuard connection requests are packets, and they‚Äôre easily identifiable, so we can efficiently snatch them with a BPF filter and a packet socket.\n\nMost of the time, it‚Äôs even easier for us to get the raw WireGuard packets, because our users now default to WebSockets WireGuard (which is just an unauthenticated WebSockets connect that shuttles framed UDP packets to and from an interface on the gateway), so that people who have trouble talking end-to-end in UDP can bring connections up.\n\nWe own the daemon code for that, and can just hook the packet receive function to snarf WireGuard packets.\n\nIt‚Äôs not obvious, but WireGuard doesn‚Äôt have notions of ‚Äúclient‚Äù or ‚Äúserver‚Äù. It‚Äôs a pure point-to-point protocol; peers connect to each other when they have traffic to send. The first peer to connect is called the initiator, and the peer it connects to is the responder.\n\nThe WireGuard paper is a good read.\n\nFor Fly.io, flyctl is typically our initiator, sending a single UDP packet to the gateway, which is the responder. According to the WireGuard paper, this first packet is a handshake initiation. It gets better: the packet type is recorded in a single plaintext byte. So this simple BPF filter catches all the incoming connections: udp and dst port 51820 and udp[8] = 1.\n\nIn most other protocols, we‚Äôd be done at this point; we‚Äôd just scrape the username or whatnot out of the packet, go fetch the matching configuration, and install it in the kernel. With WireGuard, not so fast. WireGuard is based on Trevor Perrin‚Äôs Noise Protocol Framework, and Noise goes way out of its way to hide identities during handshakes. To identify incoming requests, we‚Äôll need to run enough Noise cryptography to decrypt the identity.\n\nThe code to do this is fussy, but it‚Äôs relatively short (about 200 lines). Helpfully, the kernel Netlink interface will give a privileged process the private key for an interface, so the secrets we need to unwrap WireGuard are easy to get. Then it‚Äôs just a matter of running the first bit of the Noise handshake. If you‚Äôre that kind of nerdy, here‚Äôs the code.\n\nAt this point, we have the event feed we wanted: the public keys of every user trying to make a WireGuard connection to our gateways. We keep a rate-limited cache in SQLite, and when we see new peers, we‚Äôll make an internal HTTP API request to fetch the matching peer information and install it. This fits nicely into the little daemon that already runs on our gateways to manage WireGuard, and allows us to ruthlessly and recklessly remove stale peers with a cron job.\n\nBut wait! There‚Äôs more! We bounced this plan off Jason Donenfeld, and he tipped us off on a sneaky feature of the Linux WireGuard Netlink interface.\n\nJason is the hardest working person in show business.\n\nOur API fetch for new peers is generally not going to be fast enough to respond to the first handshake initiation message a new client sends us. That‚Äôs OK; WireGuard is pretty fast about retrying. But we can do better.\n\nWhen we get an incoming initiation message, we have the 4-tuple address of the desired connection, including the ephemeral source port flyctl is using. We can install the peer as if we‚Äôre the initiator, and flyctl is the responder. The Linux kernel will initiate a WireGuard connection back to flyctl. This works; the protocol doesn‚Äôt care a whole lot who‚Äôs the server and who‚Äôs the client. We get new connections established about as fast as they can possibly be installed.\n\nSpeedrun an app onto Fly.io and get your own JIT WireGuard peer¬†‚ú®\n\n\n## Look at this graph\n\nWe‚Äôve been running this in production for a few weeks and we‚Äôre feeling pretty happy about it. We went from thousands, or hundreds of thousands, of stale WireGuard peers on a gateway to what rounds to none. Gateways now hold a lot less state, are faster at setting up peers, and can be rebooted without having to wait for many unused peers to be loaded back into the kernel.\n\nI‚Äôll leave you with this happy Grafana chart from the day of the switchover.\n\nEditor‚Äôs note: Despite our tearful protests, Lillian has decided to move on from Fly.io to explore new pursuits. We wish her much success and happiness!¬†‚ú®"
  },
  {
    "title": "Get in on the FKS beta",
    "url": "https://fly.io/blog/fks-beta-live/",
    "content": "Eons ago, we announced we were working on Fly Kubernetes. It drummed up enough excitement to prove we were heading in the right direction. So, we got hard to work to get from barebones ‚Äúearly access‚Äù to a beta release. We‚Äôll be onboarding customers to the closed beta over the next few weeks. Email us at sales@fly.io and we‚Äôll hook you up.\n\nFly Kubernetes is the ‚Äúblessed path\"‚Ñ¢Ô∏è to using Kubernetes backed by Fly.io infrastructure. Or, in simpler terms, it is our managed Kubernetes service. We take care of the complexity of operating the Kubernetes control plane, leaving you with the unfettered joy of deploying your Kubernetes workloads. If you love Fly.io and K8s, this product is for you.\n\n\n## What even is a Kubernete?\n\nSo how did this all come to be‚Äîand what even is a Kubernete?\n\nYou can see more fun details in Introducing Fly Kubernetes.\n\nIf you wade through all the YAML and CNCF projects, what‚Äôs left is an API for declaring workloads and how it should be accessed.\n\nBut that‚Äôs not what people usually talk / groan about. It‚Äôs everything else that comes along with adopting Kubernetes: a container runtime (CRI), networking between workloads (CNI) which leads to DNS (CoreDNS). Then you layer on Prometheus for metrics and whatever the logging daemon du jour is at the time. Now you get to debate which Ingress‚Äîstrike that‚ÄîGateway API to deploy and if the next thing is anything to do with a Service Mess, then as they like to say where I live, \"bless your heart‚Äù.\n\nFinally, there‚Äôs capacity planning. You‚Äôve got to pick and choose where, how and what the Nodes will look like in order to configure and run the workloads.\n\nWhen we began thinking about what a Fly Kubernetes Service could look like, we started from first principles, as we do with most everything here. The best way we can describe it is the scene from Iron Man 2 when Tony Stark discovers a new element. As he‚Äôs looking at the knowledge left behind by those that came before, he starts to imagine something entirely different and more capable than could have been accomplished previously. That‚Äôs what happened to JP, but with K3s and Virtual Kubelet.\n\n\n## OK then, WTF (what‚Äôs the FKS)?\n\nWe looked at what people need to get started‚Äîthe API‚Äîand then started peeling away all the noise, filling in the gaps to connect things together to provide the power. Here‚Äôs how this looks currently:\n\nNow‚Ä¶not everything is a one-to-one comparison, and we explicitly did not set out to support any and every configuration. We aren‚Äôt dealing with resources like Network Policy and init containers, though we‚Äôre also not completely ignoring them. By mapping many of the core primitives of Kubernetes to a Fly.io resource, we‚Äôre able to focus on continuing to build the primitives that make our cloud better for workloads of all shapes and sizes.\n\nA key thing to notice above is that there‚Äôs no ‚ÄúNode‚Äù.\n\nVirtual Kubelet plays a central role in FKS. It‚Äôs magic, really. A Virtual Kubelet acts as if it‚Äôs a standard Kubelet running on a Node, eager to run your workloads. However, there‚Äôs no Node backing it. It instead behaves like an API, receiving requests from Kubernetes and transforming them into requests to deploy on a cloud compute service. In our case, that‚Äôs Fly Machines.\n\nSo what we have is Kubernetes calling out to our Virtual Kubelet provider, a small Golang program we run alongside K3s, to create and run your pod. It creates your pod as a Fly Machine, via the Fly Machines API, deploying it to any underlying host within that region. This shifts the burden of managing hardware capacity from you to us. We think that‚Äôs a cool trick‚Äîthanks, Virtual Kubelet magic!\n\n\n## Speedrun\n\nYou can deploy your workloads (including GPUs) across any of our available regions using the Kubernetes API.\n\nYou create a cluster with flyctl:\n\nWhen a cluster is created, it has the standard default namespace. You can inspect it:\n\nThe fly.io/app label shows the name of the Fly App that corresponds to your cluster.\n\nIt would seem appropriate to deploy the Kubernetes Up And Running demo here, but since your pods are connected over an IPv6 WireGuard mesh, we‚Äôre going to use a fork with support for IPv6 DNS.\n\nAnd you can see its Machine representation via:\n\nThis is important! Your pod is a Fly Machine! While we don‚Äôt yet support all kubectl features, Fly.io tooling will ‚Äújust work‚Äù for cases where we don‚Äôt yet support the kubectl way. So, for example, we don‚Äôt have kubectl port-forward and kubectl exec, but you can use flyctl to forward ports and get a shell into a pod.\n\nExpose it to your internal network using the standard ClusterIP Service:\n\nClusterIP Services work natively, and Fly.io internal DNS supports them. Within the cluster, CoreDNS works too.\n\nAccess this Service locally via flycast: Get connected to your org‚Äôs 6PN private WireGuard network. Get kubectl to describe the kuard Service:\n\nYou can pull out the Service‚Äôs IP address from the above output, and get at the KUARD UI using that: in this case, http://[fdaa:0:48c8:0:1::1a]:8080.\n\nUsing internal DNS: http://\u003cservice_name\u003e.svc.\u003capp_name\u003e.flycast:8080. Or, in our example: http://kuard.svc.fks-default-7zyjm3ovpdxmd0ep.flycast:8080.\n\nAnd finally CoreDNS: \u003cservice_name\u003e.\u003cnamespace\u003e.svc.cluster.local resolves to the fdaa IP and is routable within the cluster.\n\nEmail us at sales@fly.io\n\n\n## Pricing\n\nThe Fly Kubernetes Service is free during the beta. Fly Machines and Fly Volumes you create with it will cost the same as for your other Fly.io projects. It‚Äôll be $75/mo per cluster after that, plus the cost of the other resources you create.\n\n\n## Today and the future\n\nToday, Fly Kubernetes supports only a portion of the Kubernetes API. You can deploy pods using Deployments/ReplicaSets. Pods are able to communicate via Services using the standard K8s DNS format. Ephemeral and persistent volumes are supported.\n\nThe most notable absences are: multi-container pods, StatefulSets, network policies, horizontal pod autoscaling and emptyDir volumes. We‚Äôre working at supporting autoscaling and emptyDir volumes in the coming weeks and multi-container pods in the coming months.\n\nIf you‚Äôve made it this far and are eagerly awaiting your chance to tell us and the rest of the internet ‚Äúthis isn‚Äôt Kubernetes!‚Äù, well, we agree! It‚Äôs not something we take lightly. We‚Äôre still building, and conformance tests may be in the future for FKS. We‚Äôve made a deliberate decision to only care about fast launching VMs as the one and only way to run workloads on our cloud. And we also know enough of our customers would like to use the Kubernetes API to create a fast launching VM in the form of a Pod, and that‚Äôs where this story begins."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/tigris-public-beta/",
    "content": "We‚Äôre Fly.io and we transmute containers into VMs, running them on our hardware around the world with the power of Firecracker alchemy. That‚Äôs pretty cool, but we want to talk about something someone else built, that you can use today to build applications.\n\nThere are three hard things in computer science:\n\nOf all the annoying software problems that have no business being annoying, handling a file upload in a full-stack application stands apart, a universal if tractable malady, the plantar fasciitis of programming.\n\nNow, the actual act of clients placing files on servers is straightforward. Your framework has a feature that does it. What‚Äôs hard is making sure that uploads stick around to be downloaded later.\n\n(yes, yes, we know, sharding /dev/null is faster)\n\nEnter object storage, a pattern you may know by its colloquial name ‚ÄúS3‚Äù. Object storage occupies a funny place in software architecture, somewhere between a database and a filesystem. It‚Äôs like malloc(), but for cloud storage instead of program memory.\n\nS3‚Äîerr, object storage ‚Äî is so important that it was the second AWS service ever introduced (EC2 was not the first!). Everybody wants it. We know, because they keep asking us for it.\n\nSo why didn‚Äôt we build it?\n\nBecause we couldn‚Äôt figure out a way to improve on S3. And we still haven‚Äôt! But someone else did, at least for the kinds of applications we see on Fly.io.\n\n\n## But First, Some Back Story\n\nS3 checks all the boxes. It‚Äôs trivial to use. It‚Äôs efficient and cost-effective. It has redundancies that would make a DoD contractor blush. It integrates with archival services like Glacier. And every framework supports it. At some point, the IETF should just take a deep sigh and write an S3 API RFC, XML signatures and all.\n\nThere‚Äôs at least one catch, though.\n\nBack in, like, ‚Äò07 people ran all their apps from a single city. S3 was designed to work for those kinds of apps. The data, the bytes on the disks (or whatever weird hyperputer AWS stores S3 bytes on), live in one place. A specific place. In a specific data center. As powerful and inspiring as The Architects are, they are mortals, and must obey the laws of physics.\n\nThis observation feels banal, until you realize how apps have changed in the last decade. Apps and their users don‚Äôt live in one specific place. They live all over the world. When users are close to the S3 data center, things are amazing! But things get less amazing the further away you get from the data center, and even less amazing the smaller and more frequent your reads and writes are.\n\n(Thought experiment: you have to pick one place in the world to route all your file storage. Where is it? Is it Loudoun County, Virginia?)\n\nSo, for many modern apps, you end up having to write things into different regions, so that people close to the data get it from a region-specific bucket. Doing that pulls in CDN-caching things that complicated your application and put barriers between you and your data. Before you know it, you‚Äôre wearing custom orthotics on your, uh, developer feet. (I am done with this metaphor now, I promise.)\n\n(well, okay, Backblaze B2 because somehow my bucket fits into their free tier, but you get the idea)\n\nPersonally, I know this happens. Because I had to build one! I run a CDN backend that‚Äôs a caching proxy for S3 in six continents across the world. All so that I can deliver images and video efficiently for the readers of my blog.\n\n(shut up, it‚Äôs a sandwich)\n\nWhat if data was really global? For some applications, it might not matter much. But for others, it matters a lot. When a sandwich lover in Australia snaps a picture of a hamdog, the people most likely to want to see that photo are also in Australia. Routing those uploads and downloads through one building in Ashburn is no way to build a sandwich reviewing empire.\n\nLocalizing all the data sounds like a hard problem. What if you didn‚Äôt need to change anything on your end to accomplish it?\n\n\n## Show Me A Hero\n\nBuilding a miniature CDN infrastructure just to handle file uploads seems like the kind of thing that could take a week or so of tinkering. The Fly.io unified theory of cloud development is that solutions are completely viable for full-stack developers only when they take less than 2 hours to get working.\n\nAWS agrees, which is why they have a SKU for it, called Cloudfront, which will, at some variably metered expense, optimize the read side of a single-write-region bucket: they‚Äôll set up a simple caching CDN for you. You can probably get S3 and Cloudfront working within 2 hours, especially if you‚Äôve set it up before.\n\nOur friends at Tigris have this problem down to single-digit minutes, and what they came up with is a lot cooler than a cache CDN.\n\nHere‚Äôs how it works. Tigris runs redundant FoundationDB clusters in our regions to track objects. They use Fly.io‚Äôs NVMe volumes as a first level of cached raw byte store, and a queuing system modelled on Apple‚Äôs QuiCK paper to distribute object data to multiple replicas, to regions where the data is in demand, and to 3rd party object stores‚Ä¶ like S3.\n\nIf your objects are less than about 128 kilobytes, Tigris makes them instantly global. By default! Things are just snappy, all over the world, automatically, because they‚Äôve done all the work.\n\nBut it gets better, because Tigris is also much more flexible than a cache simple CDN. It‚Äôs globally distributed from the jump, with inter-region routing baked into its distribution layer. Tigris isn‚Äôt a CDN, but rather a toolset that you can use to build arbitrary CDNs, with consistency guarantees, instant purge and relay regions.\n\nThere‚Äôs a lot going on in this architecture, and it‚Äôd be fun to dig into it more. But for now, you don‚Äôt have to understand any of it. Because Tigris ties all this stuff together with an S3-compatible object storage API. If your framework can talk to S3, it can use Tigris.\n\n\n## fly storage\n\nTo get started with this, run the fly storage create command:\n\nAll you have to do is fill in a bucket name. Hit enter. All of the configuration for the AWS S3 library will be injected into your application for you. And you don‚Äôt even need to change the libraries that you‚Äôre using. The Tigris examples all use the AWS libraries to put and delete objects into Tigris using the same calls that you use for S3.\n\nI know how this looks for a lot of you. It looks like we‚Äôre partnering with Tigris because we‚Äôre chicken, and we didn‚Äôt want to build something like this. Well, guess what: you‚Äôre right!\n\nCompute and networking: those are things we love and understand. Object storage? We already gave away the game on how we‚Äôd design a CDN for our own content, and it wasn‚Äôt nearly as slick as Tigris.\n\nObject storage is important. It needs to be good. We did not want to half-ass it. So we partnered with Tigris, so that they can put their full resources into making object storage as ‚ú®magical‚ú® as Fly.io is.\n\nThis also mirrors a lot of the Unix philosophy of Days Gone Past, you have individual parts that do one thing very well that are then chained together to create a composite result. I mean, come on, would you seriously want to buy your servers the same place you buy your shoes?\n\n\n## One bill to rule them all\n\nWell, okay, the main reason why you would want to do that is because having everything under one bill makes it really easy for your accounting people. So, to make one bill for your computer, your block storage, your databases, your networking, and your object storage, we‚Äôve wrapped everything under one bill. You don‚Äôt have to create separate accounts with Supabase or Upstash or PlanetScale or Tigris. Everything gets charged to your Fly.io bill and you pay one bill per month.\n\nThis was actually going to be posted on Valentine‚Äôs Day, but we had to wait for the chocolate to go on sale.\n\nThis is our Valentine‚Äôs Day gift to you all. Object storage that just works. Stay tuned because we have a couple exciting features that build on top of the integration of Fly.io and Tigris that allow really unique things, such as truly global static website hosting and turning your bucket into a CDN in 5 minutes at most.\n\nHere‚Äôs to many more happy developer days to come."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/gpu-ga/",
    "content": "Fly.io makes it easy to spin up compute around the world, now including powerful GPUs. Unlock the power of large language models, text transcription, and image generation with our datacenter-grade muscle!\n\nGPUs are now available to everyone!\n\nWe know you‚Äôve been excited about wanting to use GPUs on Fly.io and we‚Äôre happy to announce that they‚Äôre available for everyone. If you want, you can spin up GPU instances with any of the following cards:\n\nTo use a GPU instance today, change the vm.size for one of your apps or processes to any of the above GPU kinds. Here‚Äôs how you can spin up an Ollama server in seconds:\n\nDeploy this and bam, large language model inferencing from anywhere. If you want a private setup, see the article Scaling Large Language Models to zero with Ollama for more information. You never know when you have a sandwich emergency and don‚Äôt know what you can make with what you have on hand.\n\nWe are working on getting some lower-cost A10 GPUs in the next few weeks. We‚Äôll update you when they‚Äôre ready.\n\nIf you want to explore the possibilities of GPUs on Fly.io, here‚Äôs a few articles that may give you ideas:\n\nDepending on factors such as your organization‚Äôs age and payment history, you may need to go through additional verification steps.\n\nIf you‚Äôve been experimenting with Fly.io GPUs and have made something cool, let us know on the Community Forums or by mentioning us on Mastodon! We‚Äôll boost the cool ones."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/event-driven-machines/",
    "content": "We‚Äôre Fly.io and we transmute containers into VMs, running them on our hardware around the world. We have fast booting VM‚Äôs, so why not take advantage of them?\n\nServerless is great because is has good ergonomics - when an event is received, a ‚Äúnot-server‚Äù boots quickly, code is run, and then everything is torn down. We‚Äôre billed only on usage.\n\nIt turns out that Fly.io shares many of the same ergonomics as serverless. Can we do a serverless on Fly.io? ü¶Ü Well, if it‚Äôs quacking like a duck, let‚Äôs call it a mallard.\n\nHere‚Äôs a useful pattern for triggering our own not-servers with Fly Machines.\n\n\n## Triggering Machines\n\nI want to make Machines do some work based on my own events. Fly.io can already stop Machines when idle based on HTTP, so let‚Äôs concentrate on non-HTTP events.\n\nThe process of running evented Machines involves:\n\nTo do this, I made a project and named it Lambdo because reasons. You can consider this project ‚Äúreference architecture‚Äù in the same way you call a toddler‚Äôs scribbling ‚Äúart‚Äù.\n\nThe goal is to run some of our code on a fresh not-server when an event is received. We want this done efficiently - a Machine should only exist long enough to process an event or 3.\n\nLambdo does just that - it receives some events, and spins up Fly Machines with those events placed inside the VMs. Once the code finishes, the Machine is destroyed.\n\n\n## Listening for Events\n\nFor our purposes, an event is just a JSON object. {\"any\": \"object\", \"will\": \"do\"}.\n\nWe want to turn events into compute, so we need some sort of event system. I decided to use a queue.\n\n\n## The Queue\n\nThe first thing I needed was a place to send events! I chose to use SQS, which let me continue to pretend servers don‚Äôt exist.\n\nIt‚Äôs no surprise then that the first part of this project is code that polls SQS.\n\nWhen the polling returns some non-zero number of events, it collects the SQS messages‚Äô JSON strings (and some meta data), resulting in an array of objects (a list of events).\n\nThen we send these events to some Machines.\n\n\n## Spinning Up Machines\n\nFly Machines are fast-booting Micro-VM‚Äôs, controlled by an API.\n\nA feature of that API is the ability to create files on a new Machine. This is how we‚Äôll get our events into the Machine.\n\nWhen Lambdo creates a Machine, it places a file at /tmp/events.json. Our code just needs to read that file and parse the JSON.\n\n\n## Running Our Code\n\nPart of the ergonomics of Serverless is (usually) being limited to running just a function. Fly.io doesn‚Äôt really care what you run, which is to our advantage. We can choose to write discreet functions per event, or we can bring our whole Majestic Monolith to bear.\n\nHow do we package up our code? The real answer is ‚Äúhowever you want!‚Äù, but here‚Äôs 2 ideas.\n\nUse Your Existing Code Base\n\nYou can just use your existing code base. This is especially easy if you‚Äôre already deploying apps to Fly.io.\n\nAll we‚Äôd need to do is add some additional code - a command perhaps (rake, artisan, whatever) - that sucks in that JSON, iterates over the events, and does some stuff.\n\nWhen we create an event, we‚Äôll tell Lambdo how to run your code - more on that later.\n\nUse Lambdo‚Äôs Base Images\n\nThis project also provides some ‚Äúruntimes‚Äù (base images). This is a bit more ‚Äútraditional serverless‚Äù, were you provide a function to run.\n\nLambdo contains two runtimes right now - Node and PHP. There could be more, of course, but you know‚Ä¶lazy.\n\nThe Node runtime contains some code¬†that will read the JSON payload file (again, just an array of JSON events), and call a user-supplied JS function once per event.\n\nAn¬†example is here¬†- our code just needs to export a function that does stuff to the given event:\n\nThe¬†PHP runtime¬†is the same idea, a user-supplied handler looks like this:\n\nExplore the runtime directory of the project to see how that‚Äôs put together.\n\n\n## Sending an Event\n\nSince our events are sent via SQS queue, it would be helpful to see an example SQS message. Remember how I mentioned the SQS message has some meta data?\n\nHere‚Äôs an example, with said meta data:\n\nThe Body field of the SQS message is assumed to be a JSON string (it‚Äôs the event itself, and its contents are arbitrary - whatever makes sense for you).\n\nThe message Attributes contains the meta data - up to 3 important details:\n\n‚Ä†You can get valid values for the¬†size¬†option by running¬†fly platform vm-sizes.\n\n‚Ä†‚Ä†It‚Äôs an array form, e.g. [\"php\", \"artisan\", \"foo\"], you may need to do some escaping of double quotes if you‚Äôre sending messages to SQS via terminal.\n\n\n## We did a Lambda?\n\nFly.io isn‚Äôt serverless, but it has all these primitives that add up to serverless. You have events, Fly.io has fast-booting VM‚Äôs. They just make sense together!\n\nWhat we did here is use Lambdo to respond to events by spinning up a Machine. Our code can process those events any way we want.\n\nWhat I like about this approach is how flexible it can be. We can choose the base image to use and the server type (even using GPU-enabled Machines) per event. Since we have full control over the Machine VM‚Äôs responding to the events, we can do whatever we want inside of them. Pretty neat!"
  },
  {
    "title": "Get more done on Fly.io",
    "url": "https://fly.io/blog/delegate-tasks-to-fly-machines/",
    "content": "We‚Äôre Fly.io. We run apps for our users on hardware we host around the world. Leveraging Fly.io Machines and Fly.io‚Äôs private network can make delegating expensive tasks a breeze. It‚Äôs easy to get started!\n\nThere are many ways to delegate work in web applications, from using background workers to serverless architecture. In this article, we explore a new machine pattern that takes advantage of Fly Machines and distinct process groups to make quick work of resource-intensive tasks.\n\n\n## The Problem\n\nLet‚Äôs say you‚Äôre building a web application that has a few tasks that demand a hefty amount of memory or CPU juice. Resizing images, for example, can require a shocking amount of memory, but you might not need that much memory all of the time, for handling most of your web requests. Why pay for all that horsepower when you don‚Äôt need it most of the time?\n\nWhat if there‚Äôs a different way to delegate these resource-intensive tasks?\n\n\n## The Solution\n\nWhat if you could simply delegate these types of tasks to a more powerful machine only when necessary? Let‚Äôs build an example of this method in a sample app. We‚Äôll be using Next.js today, but this pattern is framework (and language) agnostic.\n\nHere‚Äôs how it will work:\n\nTo demonstrate this task-delegation pattern, we‚Äôre going to start with a single-page application that looks like this:\n\nOur ‚ÄúOpen Pickle Jar‚Äù app is quite simple: you provide the width and height and it goes off and resizes some high-resolution photos to those dimensions (exciting!).\n\nIf you‚Äôd like to follow along, you can clone the start-here branch of this repository: https://github.com/fly-apps/open-pickle-jar . The final changes are visible on the main branch. This app uses S3 for image storage, so you‚Äôll need to create a bucket called open-pickle-jar and provide AWS_REGION, AWS_ACCESS_KEY_ID, and AWS_SECRET_ACCESS_KEY as environment variables.\n\nThis task is really just a stand-in for any HTTP request that kicks off a resource-intensive task. Get the request from the user, delegate it to a more powerful machine, and then return the result to the user. It‚Äôs what happens when you can‚Äôt open a pickle jar, and you ask for someone to help.\n\nBefore we start, let‚Äôs define some terms and what they mean on Fly.io:\n\nSetup Overview\n\nHere‚Äôs what we‚Äôll need for our application:\n\nIn short, this is what our architecture will look like, a standard web and worker duo.\n\n\n## Creating our route\n\nNext.js has two distinct routing patterns: Pages and App router. We‚Äôll use the App router in our example since it‚Äôs the preferred method moving forward.\n\nUnder your /app directory, create a new folder called /open-pickle-jar containing a route.ts .\n\n(We‚Äôre using TypeScript here, but feel free to use normal JavaScript if you prefer!)\n\nInside route.ts we‚Äôll flesh out our endpoint:\n\nThe function openPickleJar that we‚Äôre importing contains our resource-intensive task, which in this case is extracting images from a .zip file, resizing them all to the new dimensions, and returning the new image URLs.\n\nThe POST function is how one define routes for specific HTTP methods in Next.js, and ours implements a function delegateToWorker that accepts the path of the current endpoint (/open-pickle-jar) our resource-intensive function, and the same request parameters. This function doesn‚Äôt yet exist, so let‚Äôs build that next!\n\n\n## Creating our wrapper function\n\nNow that we‚Äôve set up our endpoint, let‚Äôs flesh out the wrapper function that delegates our request to a more powerful machine.\n\nWe haven‚Äôt defined our process groups just yet, but if you recall, the plan is to have two:\n\nHere‚Äôs what we want this wrapper function to do:\n\nInside your /utils directory, create a file called delegateToWorker.ts with the following content:\n\nIn our else section, you‚Äôll notice that while developing locally (aka, when NODE_ENV is development) we define the hostname of our worker process to be localhost:3001. Typically Next.js apps run on port 3000, so while testing our app locally, we can have two instances of our process running in different terminal shells:\n\nAlso, if you‚Äôre wondering about the FLY_PROCESS_GROUP and FLY_APP_NAME constants, these are Fly.io-specific runtime environment variables available on all apps.\n\n\n## Accessing our worker Machines (.internal)\n\nNow, when this code is running in production (aka NODE_ENV is NOT development) you‚Äôll see that we‚Äôre using a unique hostname to access our worker Machine.\n\nApps belonging to the same organization on Fly.io are provided a number of internal addresses. These .internal addresses let you point to different Apps and Machines in your private network. For example:\n\nSince our worker process group is running the same process as our web process (in our case, npm run start), we‚Äôll also need to make sure we use the same internal port (3000).\n\n\n## Defining our process groups and Machines\n\nThe last thing to do will be to define our two process groups and their respective Machine specs. We‚Äôll do this by editing our fly.toml configuration.\n\nIf you don‚Äôt have this file, go ahead and create a blank one and use the content below, but replace app = open-pickle-jar with your app‚Äôs name, as well as your preferred primary_region. If you don‚Äôt know what region you‚Äôd like to deploy to, here‚Äôs the list of them.\n\nBefore you deploy: Note that deploying this example app will spin up billable machines. Please feel free to alter the Machine ([[vm]]) specs listed here to ones that suit your budget or app‚Äôs needs.\n\nAnd that‚Äôs it! With our fly.toml finished, we‚Äôre ready to deploy our app!\n\n\n## Discussion\n\nToday we built a machine pattern on top of Fly.io. This pattern allows us to have a lighter request server that can delegate certain tasks to a stronger server, meaning that we can have one Machine do all the heavy lifting that could block everything else while the other handles all the simple tasks for users. With this in mind, this is a fairly na√Øve implementation, and we can make this much better:\n\n\n## Using a queue for better resiliency\n\nIn its current state, our code isn‚Äôt very resilient to failed requests. For this reason, you may want to consider keeping track of jobs in a queue with Redis (similar to Sidekiq in Ruby-land). When you have work you want to do, put it in the queue. Your queue worker would have to write the result somewhere (e.g., in Redis) that the application could fetch when it‚Äôs ready.\n\n\n## Starting/stopping worker Machines\n\nThe benefit of this pattern is that you can limit how many ‚Äúbeefy‚Äù Machines you need to have available at any given time. Our demo app doesn‚Äôt dictate how many worker Machines to have at any given time, but by adding timeouts you could elect to start and stop them as needed.\n\nNow, you may think that constantly starting and stopping Machines might incur higher response times, but note that we are NOT talking about creating/destroying Machines. Starting and stopping Machines only takes as long as it takes to start your web server (i.e. npm run start). The best part is that Fly.io does not charge for the CPU and RAM usage of stopped Machines. We will charge for storage of their root filesystems on disk, starting April 25th, 2024. Stopped Machines will still be much cheaper than running ones.\n\n\n## What about serverless functions?\n\nThis ‚Äúdelegate to a beefy machine‚Äù pattern is similar to serverless functions with platforms like AWS Lambda. The main difference is that serverless functions usually require you to segment your application into a bunch of small pieces, whereas the method discussed today just uses the app framework that you deploy to production. Each pattern has its own benefits and downsides.\n\n\n## Conclusion\n\nThe pattern outlined here is one more tool in your arsenal for scaling applications. By utilizing Fly.io‚Äôs private network and .internal domains, it‚Äôs quick and easy to pass work between different processes that run our app. If you‚Äôd like to learn about more methods for scaling tasks in your applications, check out Rethinking Serverless with FLAME by Chris McCord and Print on Demand by Sam Ruby.\n\nFly.io has fast booting machines at the ready for your dynamic workloads. It‚Äôs easy to get started. You can be off and running in minutes."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/macaroons-escalated-quickly/",
    "content": "We‚Äôre Fly.io and we transmute containers into VMs, running them on our hardware around the world. We built a new security token system, and can I tell you the good news about our lord and savior the Macaroon?\n\n\n## 1\n\nLet‚Äôs implement an API token together. It‚Äôs a design called ‚ÄúMacaroons‚Äù, but don‚Äôt get hung up on that yet.\n\nFirst some throat-clearing. Then:\n\nBearer tokens: like cookies, blobs you attach to a request (usually in an HTTP header).\n\nWe‚Äôre going to build a minimally-stateful bearer token, a blob signed with HMAC. Nothing fancy so far. Rails has done this for a decade and a half.\n\nThere‚Äôs a fashion in API security for stateless tokens, which encode all the data you‚Äôd need to check any request accompanied by that token ‚Äì without a database lookup. Stateless tokens have some nice properties, and some less-nice. Our tokens won‚Äôt be stateless: they carry a user ID, with which we‚Äôll look up the HMAC key to verify it. But they‚Äôll stake out a sort of middle ground.\n\nLet‚Äôs add some stuff.\n\nThe meat of our tokens will be a series of claims we call ‚Äúcaveats‚Äù. We call them that because each claim restricts further what the token authorizes. After {'path': '/images'}, this token only allows operations that happen underneath the /images directory. Then, after {'op': 'read'}, it allows only reads, not writes.\n\n(I guess we‚Äôre building a file sharing system. Whatever.)\n\nSome important things about things about this design. First: by implication from the fact that caveats further restrict tokens, a token with no caveats restricts nothing. It‚Äôs a god-mode token. Don‚Äôt honor it.\n\nIn other words: the ordering of caveats doesn‚Äôt matter.\n\nSecond: the rule of checking caveats is very simple: every single caveat must pass, evaluating True against the request that carries it, in isolation and without reference to any other caveat. If any caveat evaluates False, the request fails. In that way, we ensure that adding caveats to a token can only ever weaken it.\n\nWith that in mind, take a closer look at this code:\n\nEvery caveat is HMAC-signed independently, which is weird. Weirder still, the key for that HMAC is the output of the last HMAC. The caveats chain together, and the HMAC of the last caveat becomes the ‚Äútail‚Äù of the token.\n\nCreating a new blank token for a particular user requires a key that the server (and probably only the server) knows. But adding a caveat doesn‚Äôt! Anybody can add a caveat. In our design, you, the user, can edit your own API token.\n\nFor completeness, and to make a point, there‚Äôs the verification code. Look up the original secret key from the user ID, and then it‚Äôs chained HMAC all the way down. The point I‚Äôm making is that Macaroons are very simple.\n\n\n## 2\n\nBack in 2014, Google published a paper at NDSS introducing ‚ÄúMacaroons‚Äù, a new kind of cookie. Since then, they‚Äôve become a sort of hipster shibboleth. But they‚Äôre more talked about than implemented, which is a nice way to say that practically nobody uses them.\n\nUntil now! I dragged Fly.io into implementing them. Suckers!\n\nWe had a problem: our API tokens were much too powerful. We needed to scope them down and let them express roles, and I scoped up that project to replace OAuth2 tokens altogether. We now have what I think is one of the more expansive Macaroon implementations on the Internet.\n\nI dragged us into using Macaroons because I wanted us to use a hipster token format. Google designed Macaroons for a bigger reason: they hoped to replace browser cookies with something much more powerful.\n\nThe problem with simple bearer tokens, like browser cookies or JWTs, is that they‚Äôre prone to being stolen and replayed by attackers.\n\ngame-over: pentest jargon for ‚Äúvery bad‚Äù\n\nWorse, a stolen token is usually a game-over condition. In most schemes, a bearer token is an all-access pass for the associated user. For some applications this isn‚Äôt that big a deal, but then, think about banking. A banking app token that authorizes arbitrary transactions is a recipe for having a small heart attack on every HTTP request.\n\n(Perfectly minimized API tokens: a software security holy grail)\n\nMacaroons are user-editable tokens that enable JIT-generated least-privilege tokens. With minimal ceremony and no additional API requests, a banking app Macaroon lets you authorize a request with a caveat like, I don‚Äôt know, {'maxAmount': '$5'}. I mean, something way better than that, probably lots of caveats, not just one, but you get the idea: a token so minimized you feel safe sending it with your request. Ideally, a token that only authorizes that single, intended request.\n\n\n## 3\n\nThat‚Äôs not why we like Macaroons. We already assume our tokens aren‚Äôt being stolen.\n\nIn most systems, the developers come up with a permissions system, and you‚Äôre stuck with it. We run a public cloud platform, and people want a lot of different things from our permissions. The dream is, we (the low-level platform developers on the team) design a single permission system, one time, and go about our jobs never thinking about this problem again.\n\nInstead of thinking of all of our ‚Äúroles‚Äù in advance, we just model our platform with caveats:\n\n(this is a vibes-based notation, don‚Äôt think too hard about it)\n\nSimplistic. But it expresses admin tokens:\n\nAnd it expresses normal user tokens:\n\nAnd also an auditor-only token for that user:\n\n(our deploy tokens are more complicated than this)\n\nOr a deployment-only token, for a CI/CD system:\n\nThose are just the roles we came up with. Users can invent others. The important thing is that they don‚Äôt have to bother me about them.\n\n\n## 4\n\nAstute readers will have noticed by now that we haven‚Äôt shown any code that actually evaluates a caveat. That‚Äôs because it‚Äôs boring, and I‚Äôm too lazy to write it out. Got an Organization token for image-hosting that allows Reads? Ok; check and make sure the incoming request is for an asset of image-hosting, and that it‚Äôs a Read. Whatever code you came up with, it‚Äôd be fine.\n\nThese straightforward restrictions are called ‚Äúfirst party caveats‚Äù. The first party is us, the platform. We‚Äôve got all the information we need to check them.\n\nLet‚Äôs kit out our token format some more.\n\nUp till now, we‚Äôve gotten by with nothing but HMAC, which is one of the great charms of the design. Now we need to encrypt. There‚Äôs no authenticated encryption in the Python standard library, but that won‚Äôt stop us. Ready to make some candy? Hand me that brake fluid!\n\nWith ‚Äúthird-party‚Äù caveats comes a cast of characters. We‚Äôre still the first party. You‚Äôll play the second party. The third party is any other system in the world that you trust: an SSO system, an audit log, a revocation checker, whatever.\n\nHere‚Äôs the trick of the third-party caveat: our platform doesn‚Äôt know what your caveat means, and it doesn‚Äôt have to. Instead, when you see a third-party caveat in your token, you tear a ticket off it and exchange it for a ‚Äúdischarge Macaroon‚Äù with that third party. You submit both Macaroons together to us.\n\nLet‚Äôs attenuate our token with a third-party caveat hooking it up to a ‚Äúcanary‚Äù service that generates a notice approximately any time the token is used.\n\nTo build that canary caveat, you first make a ticket that users of the token will hand to your canary, and then a challenge that Fly.io will use to verify discharges your checker spits out. The ticket and the challenge are both encrypted. The ticket is encrypted under KA, so your service can read it. The challenge is encrypted under the previous Macaroon tail, so only Fly.io can read it. Both hide yet another key, the random HMAC key CRK (‚Äúcaveat root key‚Äù).\n\nIn addition to CRK, the ticket contains a message, which says whatever you want it to; Fly.io doesn‚Äôt care. Typically, the message describes some kind of additional checking you want your service to perform before spitting out a discharge token.\n\nTo authorize a request with a token that includes a third-party caveat for the canary service, you need to get your hands on a corresponding discharge Macaroon. Normally, you do that by POSTing the ticket from the caveat to the service.\n\nDischarging is simple. The service, which holds KA, uses it to decrypt the ticket. It checks the message and makes some decisions. Finally, it mints a new macaroon, using CRK, recovered from the ticket, as the root key. The ticket itself is the nonce.\n\nIf it wants, the third-party service can slap on a bunch of first-party caveats of its own. When we verify the Macaroon, we‚Äôll copy those caveats out and enforce them. Attenuation of a third-party discharge macaroon works like a normal macaroon.\n\nTo verify tokens that have third-party caveats, start with the root Macaroon, walking the caveats like usual. At each third-party caveat, match the ticket from the caveat with the nonce on the discharge Macaroon. The key for root Macaroon decrypts the challenge in the caveat, recovering CRK, which cryptographically verifies the discharge.\n\n(The Macaroons paper uses different terms: ‚Äúcaveat identifier‚Äù or cId for ‚Äúticket‚Äù, and ‚Äúverification-key identifier‚Äù or vId for ‚Äúchallenge‚Äù. These names are self-evidently bad and our contribution to the state of the art is to replace them.)\n\nThere‚Äôs two big applications for third-party caveats in Popular Macaroon Thought. First, they facilitate microservice-izing your auth logic, because you can stitch arbitrary policies together out of third-party caveats. And, they seem like fertile ground for an ecosystem of interoperable Macaroon services: Okta and Google could stand up SSO dischargers, for instance, or someone can do a really good revocation service.\n\nNeither of these light us up. We‚Äôre allergic to microservices. As for public protocols, well, it‚Äôs good to want things. So we almost didn‚Äôt even implement third-party caveats.\n\n\n## 5\n\nI‚Äôm glad we did though, because they‚Äôve been pretty great.\n\nThe first problem third-party caveats solved for us was hazmat tokens. To the extent possible, we want Macaroon tokens to be safe to transmit between users. Our Macaroons express permissions, but not authentication, so it‚Äôs almost safe to email them.\n\nThe way it works is, our Macaroons all have a third-party caveat pointing to a ‚Äúlogin service‚Äù, either identifying the proper bearer as a particular Fly.io user or as a member of some Organization. To allow a request with your token, you first need to collect the discharge from the login service, which requires authentication.\n\nThe login discharge is very sensitive, but there isn‚Äôt much reason to pass it around. The original permissions token is where all the interesting stuff is, and it‚Äôs not scary. So that‚Äôs nice.\n\nBen then came up with third-party caveats that require Google or Github SSO logins. If your token has one of those caveats, when you run flyctl deploy, a browser will pop up to log you into your SSO IdP (if you haven‚Äôt done so recently already).\n\nWe‚Äôve put a bunch of work into getting the guts of our SSO system working, but that work has mostly been invisible to customers. But Macaroon-ized SSO has a subtle benefit: you can configure Fly.io to automatically add SSO requirements to specific Organizations (so, for instance, a dev environment might not need SSO at all, and prod might need two).\n\nSSO requirements in most applications are a brittle pain in the ass. Ours are flexible and straightforward, and that happened almost by accident. Macaroons, baby!\n\nHere‚Äôs a fun thing you can do with a Macaroon system: stand up a Slack bot, and give it an HTTP POST handler that accepts third-party tickets. Then:\n\nSo, the bot is cute, but any platform could do that. What‚Äôs cool is the way our platform doesn‚Äôt work with Slack; in fact, nothing on our platform knows anything about Slack, and Slack doesn‚Äôt know anything about us. We didn‚Äôt reach out to a Slack endpoint. Everything was purely cryptographic.\n\nThat bot could, if I sunk some time into it, enforce arbitrary rules: it could selectively add caveats for the requests it authorizes, based on lookups of the users requesting them, at specific times of day, with specific logging. Theoretically, it could add third-party caveats of its own.\n\nThe win for us for third-party caveats is that they create a plugin system for our security tokens. That‚Äôs an unusual place to see a plugin interface! But Macaroons are easy to understand and keep in your head, so we‚Äôre pretty confident about the security issues.\n\n\n## 6\n\nObviously, we didn‚Äôt write our Macaroon code in Python, or with HMAC-SHA256-CTR.\n\nWe landed on a primary implementation Golang (Ben subsequently wrote an Elixir implementation). Our hash is SHA256, our cipher is Chapoly. We encode in MsgPack.\n\nWe didn‚Äôt use the pre-existing public implementation because we were warned not to. The Macaroon idea is simple, and it exists mostly as an academic paper, not a standard. The community that formed around building open source ‚Äústandard‚Äù Macaroons decided to use untyped opaque blobs to represent caveats. We need things to be as rigidly unambiguous as they can be.\n\nThe big strength of Macaroons as a cryptographic design ‚Äî that it‚Äôs based almost entirely on HMAC ‚Äî makes it a challenge to deploy. If you can verify a Macaroon, you can generate one. We have thousands of servers. They can‚Äôt all be allowed to generate tokens.\n\nWhat we did instead:\n\nNow buckle up, because I‚Äôm about to try to get you to care about service tokens.\n\nWe operate ‚Äúworker servers‚Äù all over the world to host apps for our customers. To do that, those workers need access to customer secrets, like the key to decrypt a customer volume. To retrieve those secrets, the workers have to talk to secrets management servers.\n\nWe manage a lot of workers. We trust them. But we don‚Äôt trust them that much, if you get my drift. You don‚Äôt want to just leave it up to the servers to decide which secrets they can access. The blast radius of a problem with a single worker should be no greater than the apps that are supposed to run there.\n\nThe gold standard for approving access to customer information is, naturally, explicit customer authorization. We almost have that with Macaroons! The first time an app runs on a worker, the orchestrator code has a token, and it can pass that along to the secret stores.\n\nThe problem is, you need that token more than once; not just when the user does a deploy, but potentially any time you restart the app or migrate it to a new worker. And you can‚Äôt just store and replay user Macaroons. They have expirations.\n\nThis is like dropping privilege with things like pledge(2), but in a distributed system.\n\nSo our token verification service exposes an API that transforms a user token into a ‚Äúservice token‚Äù, which is just the token with the authentication caveat and expiration ‚Äústripped off‚Äù.\n\nWhat‚Äôs cool is: components that receive service tokens can attenuate them. For instance, we could lock a token to a particular worker, or even a particular Fly Machine. Then we can expose the whole Fly Machines API to customer VMs while keeping access traceable to specific customer tokens. Stealing the token from a Fly Machine doesn‚Äôt help you since it‚Äôs locked to that Fly Machine by a caveat attackers can‚Äôt strip.\n\n\n## 7\n\nIf a customer loses their tokens to an attacker, we can‚Äôt just blow that off and let the attacker keep compromising the account!\n\nThis cancels every token derived through attenuation by that nonce.\n\nEvery Macaroon we issue is identified by a unique nonce, and we can revoke tokens by that nonce. This is just a basic function of the token verification service we just described.\n\nWe host token caches all over our fleet. Token revocation invalidates the caches. Anything with a cache checks frequently whether to invalidate. Revocation is rare, so just keeping a revocation list and invalidating caches wholesale seems fine.\n\n\n## 8\n\nI get it, it‚Äôs tough to get me to shut up about Macaroons.\n\nA couple years ago, I wrote a long survey of API token designs, from JWTs (never!) to Biscuits. I had a bunch to say about Macaroons, not all of it positive, and said we‚Äôd be plowing forward with them at Fly.io.\n\nMy plan had been to follow up soon after with a deep dive on Macaroons as we planned them for Fly.io. I‚Äôm glad I didn‚Äôt do that, not just because it would‚Äôve been embarrassing to announce a feature that took us over 2 years to launch, but also because the process of working on this with Ben Toews changed a lot of my thinking about them.\n\nI think if you asked Ben, he‚Äôd say he had mixed feelings about how much complexity we wrangled to get this launched. On the other hand: we got a lot of things out of them without trying very hard:\n\nThere are downsides and warts! I‚Äôm mostly not telling you about them! Pure restrictive caveats are an awkward way to express some roles. And, blinded by my hunger to get Macaroons deployed, I spat in the face of science and used internal database IDs as our public caveat format, an act for which JP will never forgive me.\n\nIf i‚Äôve piqued your interest, the code for this stuff is public, along with some more detailed technical documentation."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-i-fly-yoko-li/",
    "content": "Hello all, and welcome to another episode of How I Fly, a series where I interview developers about what they do with technology, what they find exciting, and the unexpected things they‚Äôve learned along the way. This time I‚Äôm talking with Yoko Li, an investment partner at A16Z who‚Äôs also an open-source AI developer. She works on some of the most exciting AI projects in the world. I‚Äôm excited to share them with you today, with fun stories about the lessons she‚Äôs learned along the way.\n\n\n## Cool Experiments\n\nOne of Yoko‚Äôs most thought-provoking experiments is AI Town, a virtual town populated by AI agents that talk with each other. It takes advantage of the randomness of AI responses to create emergent behavior. When you open it, it looks like this:\n\nYou can see the AI agents talking with each other and watch how the relationships between them form and change over time. It‚Äôs also a lot of fun to watch.\n\nOne of Yoko‚Äôs other experiments is AI Tamago, a Tamagochi virtual pet implemented with a large language model instead of the state machine that we‚Äôre all used to. AI Tamago uses an unmodified version of LLaMA 2 7B to take in game state and user inputs, then it generates what happens next. Every time you interact with your pet, it feeds data to LLaMA 2 and then uses Ollama‚Äôs JSON mode to generate unexpected output.\n\nIt‚Äôs all the fun of the classic Tamagochi toys from the 90‚Äôs (including the ability to randomly discipline your virtual pet) without any of the coin cell batteries or having to carry around the little egg-shaped puck.\n\nBut that‚Äôs just something you can watch, not something that‚Äôs as easy to play with on your own machine. Yoko has also worked on the Local AI Starter Kit that lets you go from zero to AI in minutes. It‚Äôs a collection of chains of models that let you ingest a bunch of documents, store them in a database, and then use those documents as context for a language model to generate responses. It‚Äôs everything you need to implement a ‚Äúchat with a knowledge base‚Äù feature.\n\n\n## The dark of AI experiments\n\nThe Local AI Starter Kit is significant because normally to do this, you need to set up billing and API keys for at least four different API providers, and then you need to write a bunch of (hopefully robust) code to tie it all together. With the Local AI Starter Kit, you can do this on your own hardware, with your own data, and your own models privately. It‚Äôs a huge step forward for democratizing access to this technology.\n\nDocument search is one of my favorite usecases for AI, and it‚Äôs one of the most immediately useful ones. It‚Äôs also one of the most fiddly and annoying to get right. To help illustrate this, I‚Äôve made a diagram of the steps involved with setting up document search by hand:\n\nYou start with your Markdown documents. Most Markdown documents are easily broken up into sections where each section will focus on a single aspect of the larger topic of the document. You can take advantage of this best practice by letting people search for each section individually, which is typically a lot more useful than just searching the entire document.\n\nOkay, okay, fine. Language encircles concepts instead of defining them directly. The point still stands that we‚Äôre operating at a level ‚Äúbelow‚Äù words and sentences, I don‚Äôt want to bog this down in a bunch of linear algebra that neither of us understand well enough to explain in a single paragraph like I am here. The main point is that it lets you ‚Äúfuzzy match‚Äù relevant documents in a way that exact word search queries never could on their own.\n\nEssentially, the vector embeddings that you generate from an embedding model are a mathematical representation of the ‚Äúconcepts‚Äù that the embedding model uses that are adjacent to the text of your documents. When you use the same model to generate embeddings for your documents and user queries, this lets you find documents that are similar to the query, but not precisely the same exact words. This is called ‚Äúfuzzy searching‚Äù and it is one of the most difficult problems in computer science (right next to naming things).\n\nWhen a user comes to search the database, you do the same thing as ingestion:\n\nThe user query comes into your API endpoint. You use the same embedding model from earlier (omitted from the diagram for brevity) to turn that query into a vector. Then you query the same vector database to find documents that are similar to the query. Then you have a list of documents with metadata like the URL to the documentation page or section fragment in that page. From here you have two options. You can either use the documents to return a list of results to the user, or you can do the more fun thing: using those documents as context for a large language model to generate a response grounded in the relevant facts in those documents.\n\nI think it‚Äôs also how OpenAI‚Äôs custom GPTs work, but they haven‚Äôt released technical details about how they work so this is outright speculation on my part.\n\nThis basic pattern is called Retrieval-augmented Generation (RAG), and it‚Äôs how Bing‚Äôs copilot chatbot works. The Local AI Starter Kit makes setting this pipeline up effortless and fast. It‚Äôs a huge step forward for making this groundbreaking technology accessible to everyone.\n\n\n## The struggles\n\nWhen I was trying to get the AI models in AI Town to output JSON, I tried a bunch of different things. I got some good results by telling the model to ‚Äúonly reply in JSON, no prose‚Äù, but we ended up using a model tuned for outputting code. I think I inspired Ollama to add their JSON output feature.\n\nOne of the main benefits of large language models is that they are essentially stochastic models of the entire Internet. They have a bunch of patterns formed that can let you create surprisingly different outputs from similar inputs. This is also one of the main drawbacks of large language models: they are essentially stochastic models of the entire Internet. They have a bunch of patterns formed that can let you create surprisingly different outputs from similar inputs. The outputs of these models are usually correct-ish enough (more correct if you ground the responses in document fact like you do with a Retrieval-augmented Generation system), but they are not always aligned with our observable reality.\n\nA lot of the time you will get outputs that don‚Äôt make any logical or factual sense. These are called ‚Äúhallucinations‚Äù and they are one of the main drawbacks of large language models. If a hallucination pops in at the worst times, you‚Äôve accidentally told someone how to poison themselves with chocolate chip cookies. This is, as the kids say, ‚Äúbad‚Äù.\n\nThe inherent randomness of the output of a large language model means that it can be difficult to get an exactly parsable format. Most of the time, you‚Äôd be able to coax the model to get usable JSON output, but without schema it can sometimes generate wildly different JSON responses. Only sometimes. This isn‚Äôt deterministic and Yoko has found that this is one of the most frustrating parts of working with large language models.\n\nThis works by making any offending ungrammatical tokens weighted to negative infinity. It‚Äôs amazingly hacky but the hilarious part is that it works.\n\nHowever, there are workarounds. llama.cpp offers a way to use a grammar file to strictly guide the output of a large language model by using context-free grammar. This lets you get something more deterministic, but it‚Äôs still not perfect. It‚Äôs a lot better than nothing, though.\n\nOne of the fun things that can happen with this is that you can have the model fail to generate anything but an endless stream of newlines in JSON mode. This is hilarious and usually requires some special detection logic to handle and restart the query. There‚Äôs work being done to let you use JSON schema to guide the generation of large language model outputs, but it‚Äôs not currently ready for the masses.\n\nIf it‚Äôs dumb and it works, is it really dumb?\n\nHowever, one of the easiest ways to hack around this is by using a model that generates code instead of text. This is how Yoko got the AI Town and AI Tamago models to output JSON that was mostly valid. It‚Äôs a hack, but it works. This was made a lot easier for AI town when one of the tools they use (Ollama) added support for JSON output from the model. This is a lot better than the code generation model hack, but research continues.\n\n\n## The simple joy of unexpected outputs\n\nWhen I was making AI Town, I was inspired by The Lifecycle of Software Objects by Ted Chiang. It‚Äôs about a former zookeeper that trained AI agents to be pets, kinda like how we use Reinforcement Learning from Human Feedback to train AI models like ChatGPT.\n\nHowever, at the same time, there are cases where hallucinations are not only useful, but they are what make the implementation of a system possible. If large language models are essentially massive banks of the word frequencies of a huge part of culture, then the emergent output can create unexpected things that happen frequently. This lets you have emergent behavior form, this can be the backbone of games and is the key thing that makes AI Town work as well as it does.\n\nAI Tamago is also completely driven off of the results of large language model hallucinations. They are the core of what drives user inputs, the game loop, and the surprising reactions you get when disciplining your pet. The status screen takes in the game state and lets you know what your pet is feeling in a way that the segment displays of the Tamagochi toys could never do.\n\nThese enable you to build workflows that are augmented by the inherent randomness of the hallucinations instead of seeing them as drawbacks. This means you need to choose outputs that can have the hallucinations shine instead of being ugly warts you need to continuously shave away. Instead of using them for doing pathfinding, have them drive the AI of your characters or writing the A* pathfinding algorithm so you don‚Äôt have to write it again for the billionth time.\n\nI‚Äôm not saying that large language models can replace the output of a human, but they are more like a language server for human languages as well as programming languages. They are best used when you are generating the boilerplate you don‚Äôt want to do yourself, or when you are throwing science at the wall to see what sticks.\n\n\n## In conclusion\n\nYoko is showing people how to use AI today, on local machines, with models of your choice, that allow you to experiment, hack and learn.\n\nI can‚Äôt wait to see what‚Äôs next!\n\nIf you want to follow what Yoko does, here‚Äôs a few links to add to your feeds:\n\n(insert standard conclusion diatribe here)"
  },
  {
    "title": "Not interested in GPUs?",
    "url": "https://fly.io/blog/not-midjourney-bot/",
    "content": "Fly.io has Enterprise-grade GPUs and servers all over the globe (or disk, depending on which side of the flat Earth debate you fall on) making it a great place to deploy your next disruptive AI app.\n\nSome people daydream about normal things, like coffee machines or raising that Series A round (those are normal things to dream about, right?). I daydream about commanding a fleet of chonky NVIDIA Lovelace L40Ss. Also, totally normal. Well, fortunately for me and anyone else wanting to explore the world of generative AI ‚Äî Fly.io has GPUs now!\n\nSure, this technology will probably end up with the AI talking to itself while we go about our lives ‚Äî but it seems like it‚Äôs here to stay, so we should at least have some fun with it. In this post we‚Äôll put these GPUs to task and you‚Äôll learn how to build your very own AI image-generating Discord bot, kinda like Midjourney. Available 24/7 and ready to serve up all the pictures of cats eating burritos your heart desires. And because I‚Äôd never tell you to draw the rest of the owl, I‚Äôll link to working code that you can deploy today.\n\n\n## Latent Diffusion Models Have Entered the Chat\n\nIn the realm of AI image generation, two names have become prominent: Midjourney and Stable Diffusion. Both are image generating software that allow you to synthesize an image from a textual prompt. One is a closed source paid service, while the other is open source and can run locally. Midjourney gained popularity because it allowed the less technically-inclined among us to explore this technology through its ease of use. Stable Diffusion democratized access to the technology, but it can be quite tricky to get good results out of it.\n\nEnter Fooocus (pronounced focus), an open source project that combines the best of both worlds and offers a user-friendly interface to Stable Diffusion. It‚Äôs hands down the easiest way to get started with Stable Diffusion. Sure there are more popular tools like Stable Diffusion web UI and ComfyUI, but Fooocus adds some magic to reduce the need to manually tweak a bunch of settings. The most significant feature is probably GPT-2-based ‚Äúprompt expansion‚Äù to dynamically enhance prompts.\n\nThe point of Fooocus is to focus on your prompt. The more you put into it, the more you get out. That said, a very simple prompt like ‚Äúforest elf‚Äù can return high-quality images without the need to trawl the web for prompt ideas or fiddle with knobs and levers (although they‚Äôre there if you want them).\n\nSo, what can this thing do? Well, this‚Ä¶\n\nHere‚Äôs the full command I‚Äôve used to generate this image: /imagine prompt:¬†sketch of hot-air balloon over a mountain range¬†style1:¬†Pencil Sketch Drawing¬†quality:¬†true¬†ar:¬†1664√ó576\n\n\n## What We‚Äôre Building\n\nWe‚Äôll deploy two applications. The code to run the bot itself will run on normal VM hardware, and the API server doing all the hard work synthesizing alpacas out of thin air will run on GPU hardware.\n\nFooocus is served up as a web UI by default, but with a little elbow grease we can interact with it as a REST API. Fortunately, with more than 25k stars on GitHub at the time of writing, the project has a lively open-source community, so we don‚Äôt need to do much work here ‚Äî it‚Äôs already been done for us. Fooocus-API is a project that shoves FastAPI in front of a Fooocus runtime. We‚Äôll use this for the API server app.\n\nThe Python-based bot connects to the Discord Gateway API using the Pycord library. When it starts up, it maintains an open pipe for data to flow back and forth via WebSockets. The bot app also includes a client that knows how to talk to the API server using Flycast and request the image it needs via HTTP.\n\nWhen we request an image from Discord using the /imagine slash command, we immediately respond using Pycord‚Äôs defer() function to let Discord know that the request has been received and the bot is working on it ‚Äî it‚Äôll take a few seconds to process your prompt, fabricate an image, upload it to Discord and let you share it with your friends. This is a blocking operation, so it won‚Äôt perform well if you have hundreds of people on your Discord Server using the command. For that, you‚Äôll want to jiggle some wires to make the code non-blocking. But for for now, this gives us a nice UX for the bot.\n\nWhen the API server returns the image, it gets saved to disk. We‚Äôll use the fantastic Sqids library to generate collision-free file names:\n\nWe‚Äôll also use asyncio to check if the image is ready every second, and when it is, we send it off to Discord to complete the request:\n\nNeither of these two apps will be exposed to the Internet, yet they‚Äôll still be able to communicate with each other. One of the undersold stories about Fly.io is the ease with which two applications can communicate over the private network. We assign special IPv6 private network (6pn) addresses within the same organizational space and applications can effortlessly discover and connect to one another without any additional configuration.\n\nBut what about load balancing and this ‚Äúscale-to-zero‚Äù thing? We don‚Äôt just want our two apps to talk to each other, we want the Fly Proxy to start our Machine when a request comes in, and stop it when idle. For that, we‚Äôll need Flycast, our private load balancing feature.\n\nWhen you assign a Flycast IP to your app, you can route requests using a special .flycast domain. Those requests are routed through the Fly Proxy instead of directly to instances in your app. Meaning you get all the load balancing, rate limiting and other proxy goodness that you‚Äôre accustomed to. The Proxy runs a process which can automatically downscale Machines every few minutes. It‚Äôll also start them right back up when a request comes in ‚Äî this means we can take advantage of scale-to-zero, saving us a bunch of money!\n\n\n## The /imagine Command\n\nThe slash command is the heart of your bot, enabling you to generate images based on your prompt, right from within Discord. When you type /imagine into the Discord chat, you‚Äôll see some command options pop up.\n\nYou‚Äôll need to input your base prompt (e.g. ‚Äúan alpaca sleeping in a grassy field‚Äù) and optionally pick some styles (‚ÄúPencil Sketch Drawing‚Äù, ‚ÄúFuturistic Retro Cyberpunk‚Äù, ‚ÄúMRE Dark Cyberpunk‚Äù etc). With Fooocus, combining multiple styles ‚Äî ‚Äústyle-chaining‚Äù ‚Äî can help you achieve amazing results. Set the aspect ratio or provide negative prompts if needed, too.\n\nAfter you execute the command, the bot will request the image from the API, then send it as a response in the chat. Let‚Äôs see it in action!\n\n\n## Deployment Speedrun\n\nFirst, we‚Äôll deploy the API server. For convenience (and to speed things up), we‚Äôll use a pre-built image when we deploy. With dependencies like torch and torchvision bundled in, it‚Äôs a hefty image weighing in just shy of 12GB. With a normal Fly Machine this would not only be a bad idea, but not even possible due to an 8GB limit for the VMs rootfs. Fortunately the wizards behind Fly GPUs have accounted for our need to run huge models and their dependencies, and awarded us 50GB of rootfs.\n\nFly GPUs use Cloud Hypervisor and not Firecracker (like a regular Fly Machine) for virtualization. But even with a 12GB image, this doesn‚Äôt stop the Machine from booting in seconds when a new request comes in through the Proxy.\n\nTo start, clone the template repository. You‚Äôll need this for both the bot and server apps. Then deploy the server with the Fly CLI:\n\nThis command tells Fly.io to deploy your application based on the configuration specified in the fly.toml, while the --no-public-ips flag secures your app by not exposing it to the public Internet.\n\nRemember Flycast? To use it, we‚Äôll allocate a private IPv6:\n\nNow, let‚Äôs take a look at our fly.toml config:\n\nThere are a few key things to note here:\n\nThe README for this project has detailed instructions about setting up your Discord bot and adding it to a Server. After setting up the permissions and privileged intents, you‚Äôll get an OAuth2 URL. Use this URL to invite your bot to your Discord server and confirm the permissions. Once that‚Äôs done, grab your Discord API token, you‚Äôll need it for the next step.\n\nWith the API server up and running, it‚Äôs time to deploy the Discord bot. This app will run on a normal Fly Machine, no GPU required. First, set the DISCORD_TOKEN and FOOOCUS_API_URL (the Flycast endpoint for the API server) secrets, using the Fly CLI. Then deploy:\n\nNotice that the bot app doesn‚Äôt need to be publicly visible on the Internet either. Under the hood, the WebSocket connection to Discord‚Äôs Gateway API allows the bot to communicate freely without the need to define any services in our fly.toml. This also means that the Fly Proxy will not downscale the app like it does the GPU Machine ‚Äî the bot will always appear ‚Äúonline‚Äù.\n\nYou can still deploy apps on Fly.io today and be up and running in a matter of minutes.\n\n\n## How Do I Know This Thing Is Using GPU for Reals?\n\nThat‚Äôs easy! NVIDIA provides us with a neat little command-line utility called nvidia-smi which we can use to monitor and get information about NVIDIA GPU devices.\n\nLet‚Äôs SSH to the running Machine for the API server app and run an nvidia-smi query in one go. It‚Äôs a little clunky, but you‚Äôll get the point:\n\nWhat we‚Äôve done is run the command on a loop while the bot is actually doing work synthesizing an image and we get to see it ramp up and consume more wattage and VRAM. The card is barely breaking a sweat!\n\n\n## How Much Will These Alpaca Pics Cost Me?\n\nLet‚Äôs talk about the cost-effectiveness of this setup. On Fly.io, an L40S GPU costs $2.50/hr. Tag on a few cents per hour for the VM resources and storage for our models and you‚Äôre looking at about $3.20/hr to run the GPU Machine. It‚Äôs on-demand, too ‚Äî if you‚Äôre not using the compute, you‚Äôre not paying for it! Keep in mind that some of these checkpoint models can be several gigabytes and if you create a volume, you will be charged for it even when you have no Machines running. It‚Äôs worth noting too, that the non-GPU bot app falls into our free allowance.\n\nRates are on-demand, with no minimum usage requirements. Discounted rates for reserved GPU Machines and dedicated hosts are also available if you email sales@fly.io\n\nIn comparison, Midjourney offers several subscription tiers with the cheapest plan costing $10/mo and providing 3.3 hours of ‚Äúfast‚Äù GPU time (roughly equivalent to an enterprise-grade Fly GPU). This works out to about $3/hr give or take a few cents.\n\n\n## Where Can I Take This?\n\nThere is a lot you can do to build out the bot‚Äôs functionality. You control the source code for the bot, meaning that you can make it do whatever you want. You might decide to mimic Midjourney‚Äôs /blend command to splice your own images into prompts (AKA img2img diffusion). You can do this by adding more commands to your Cog, Pycord‚Äôs way of grouping similar commands. You might decide to add a button to roll the image if you don‚Äôt like it, or even specify the number of images to return. The possibilities are endless and your cloud bill‚Äôs the limit!\n\nThe full code for the bot and server (with detailed instructions on how to deploy it on Fly.io) can be found here."
  },
  {
    "title": "Choose your own Linux Distribution",
    "url": "https://fly.io/blog/fly-with-alpine/",
    "content": "Reduce image sizes and improve startup times by switching your base image to Alpine Linux.\n\nBefore proceeding, a caution. This is an engineering trade-off. Test carefully before deploying to production.\n\nBy the end of this blog post you should have the information you need to make an informed decision.\n\n\n## Introduction\n\nAlpine Linux is a Linux distribution that advertises itself as Small. Simple. Secure.\n\nIt is indisputably smaller than the alternatives ‚Äì when measured by image size. More on that in a bit. Some claim that this results in less memory usage and better performance. Others dispute these claims. For these, it is best that you test the results for yourself with your application.\n\nSimple is harder to measure. Some of the larger differences, like OpenRC vs SystemD, are less relevant in container environments. Others, like BusyBox are implementation details. Essentially what you get is a Linux distribution with perhaps a number of standard packages (e.g., bash) not installed by default, but these can be easily added if needed.\n\nSecure is definitely an important attribute. The alternatives make comparable claims in this area. Do your own research in this area and come to your own conclusions.\n\nNot mentioned is the downside: Alpine Linux has a smaller ecosystem that the alternatives, particularly when compared to Debian.\n\n\n## Baseline\n\nLet‚Äôs start with a baseline consisting of the Dockerfiles produced by fly launch for some of the most popular frameworks:\n\nWhat may not be obvious to the naked eye from these results is that the base image for these is one of the following:\n\nOnce you factor in that Ubuntu is based on Debian, the conclusion is that Debian is effectively the default distribution for fly IO. Rest assured that this isn‚Äôt the result of a devious conspiracy by Fly.io, but rather a reflection of the default choices made independently by the developers of a number of frameworks and runtimes. Beyond this, all Fly.io is doing is choosing the ‚Äúslim‚Äù version of the default distribution for each framework as the base.\n\nWhat‚Äôs likely going on here is a virtuos circle: people choose Debian because of the ecosystem, and ecosystem grows because people chose Debian.\n\nNow lets compare base image sizes:\n\nAnd these numbers are just the for the base images. I‚Äôve measured a minimal Rails/Postgresql/esbuild application at 304MB on Alpine and 428MB on Debian Slim. A minimal Bun application at 110MB on Alpine and 173MB on Debian Slim. And a minimal Node application at 142MB on Alpine and 207MB on Debian Slim.\n\nIn each case, corresponding Alpine images are consistently smaller than their Debian slim equivalent.\n\n\n## Switching Distributions\n\nSwitch distributions (and switching back!) is easy.\n\nThe first change is to replace -slim with -alpine in FROM statements in your Dockerfile.\n\nNext is to replace apt-get update with apk update and apt-get install with apk add. Delete any options you may have like -y and --no-install-recommends - they aren‚Äôt needed.\n\nNow review the names of the packages you are installing. Many are named the same. A few are different. You can use alpine packages to look for ones to use. Some examples of differences:\n\nNote: the above is just an approximation. For example, while libsqlite3-0 and sqlite-dev include everything you need to build an application that uses sqlite3, all that is needed at runtime is sqlite-lib. This relentless attention to detail contributes to smaller final image sizes.\n\nNote: For Bun, Node, and Rails users, knowledge of how to apply the above changes are included in recent versions of the dockerfile generators that we provide. After all, computers are good at if statements:\n\nDeploy your project in a few minutes with Fly Launch. Then do more with Fly Machines.\n\n\n## Potential issues\n\nOver time, we‚Äôve noted a number of issues.\n\n\n## Conclusion\n\nWhile not as large a community as Debian, there is a substantial number of happy Alpine users.\n\nFor the forseeable future, the default for both frameworks and there fly.io will remain Debian, but we make it easy to switch.\n\nTry it out! Hopefully this blog has provided insight into what you should evaluate for before you switch."
  },
  {
    "title": "Not invested in K8s?",
    "url": "https://fly.io/blog/fks/",
    "content": "We‚Äôre Fly.io, and if you‚Äôve been following us awhile you probably just did a double-take. We‚Äôre building a new public cloud that runs containerized applications with virtual machine isolation on our own hardware around the world. And we‚Äôve been doing it without any K8s. Until now!\n\nUpdate, March 2024: FKS does more stuff now, and you can read about it in Fly Kubernetes does more now\n\nWe‚Äôll own it: we‚Äôve been snarky about Kubernetes. We are, at heart, old-school Unix nerds. We‚Äôre still scandalized by systemd.\n\nTo make matters more complicated, the problems we‚Äôre working on have a lot of overlap with K8s, but just enough impedance mismatch that it (or anything that looks like it) is a bad fit for our own platform.\n\nBut, come on: you never took us too seriously about K8s, right? K8s is hard for us to use, but that doesn‚Äôt mean it‚Äôs not a great fit for what you‚Äôre building. We‚Äôve been clear about that all along, right? Sure we have!\n\nWell, good news, everybody! If K8s is important for your project, and that‚Äôs all that‚Äôs been holding you back from trying out Fly.io, we‚Äôve spent the past several months building something for you.\n\n\n## Fly.io For Kubernetians\n\nFly.io works by transmogrifying Docker containers into filesystems for lightweight hypervisors, and running them on servers we rack in dozens of regions around the world.\n\nYou can build something like Fly.io with ‚Äústandard‚Äù orchestration tools like K8s. In fact, that‚Äôs what we did to start, too. To keep things simple, we used Nomad, and instead of K8s CNIs, we built our own Rust-based TLS-terminating Anycast proxy (and designed a WireGuard/IPv6-based private network system based on eBPF). But the ideas are the same.\n\nThe way we look at it, the signature feature of a ‚Äústandard‚Äù orchestrator is the global scheduler: the global eye in the sky that keeps track of vacancies on servers and optimized placement of new workloads. That‚Äôs the problem we ran into. We‚Äôre running over 200,000 applications, and we‚Äôre doing so on every continent except Antarctica. The speed of light (and a globally distributed network of backhoes) has something to say about keeping a perfectly consistent global picture of hundreds of thousands of applications, and it‚Äôs not pleasant.\n\nThe other problem we ran into is that our Nomad scheduler kept trying to outsmart us, and, worse, our customers. It turns out that our users have pretty firm ideas of where they‚Äôd like their apps to run. If they ask for S√£o Paulo, they want S√£o Paulo, not Rio. But global schedulers have other priorities, like optimally bin-packing resources, and sometimes GIG looks just as good as GRU to them.\n\nTo escape the scaling and DX problems we were hitting, we rethought orchestration. Where orchestrators like K8s tend to work through distributed consensus, we keep state local to workers. Each racked server in our fleet is a source of truth about the apps running on it, and provide an API to a market-style ‚Äúscheduler‚Äù that bids on resources in regions. You can read more about here, if you‚Äôre interested. We call this system the Fly Machines API.\n\nAn important detail to grok about how this all works ‚Äì¬†a reason we haven‚Äôt, like, beaten the CAP theorem by doing this ‚Äì is that Fly Machines API calls can fail. If Nomad or K8s tries to place a workload on some server, only to find out that it‚Äôs filled up or thrown a rod, it will go hunt around for some other place to put it, like a good little robot. The Machines API won‚Äôt do this. It‚Äôll just fail the request. In fact, it goes out of its way to fail the request quickly, to deliver feedback; if we can‚Äôt schedule work in JNB right now, you might want instead to quickly deploy to BOM.\n\n\n## Pluggable Orchestration and FKS\n\nIn a real sense what we‚Äôve done here is extract a chunk of the scheduling problem out of our orchestrator, and handed it off to other components. For most of our users, that component is flyctl, our intrepid CLI.\n\nBut Fly Machines is an API, and anything can drive it. A lot of our users want quick answers to requests to schedule apps in specific regions, and flyctl does a fine job of that. But it‚Äôs totally reasonable to want something that works more like the good little robots inside of K8s.\n\nYou can build your own orchestrator with our API, but if what you‚Äôre looking for is literally Kubernetes, we‚Äôve saved you the trouble. It‚Äôs called Fly Kubernetes, or FKS for short.\n\nFKS is an implementation of Kubernetes that runs on top of Fly.io. You start it up using flyctl, by running flyctl ext k8s create.\n\nUnder the hood, FKS is a straightforward combination of two well-known Kubernetes projects: K3s, the lightweight CNCF-certified K8s distro, and Virtual Kubelet.\n\nVirtual Kubelet is interesting. In K8s-land, a kubelet is a host agent; it‚Äôs the thing that runs on every server in your fleet that knows how to run a K8s Pod. Virtual Kubelet isn‚Äôt a host agent; it‚Äôs a software component that pretends to be a host, registering itself with K8s as if it was one, but then sneakily proxying the Kubelet API elsewhere.\n\nIn FKS, ‚Äúelsewhere‚Äù is Fly Machines. All we have to do is satisfy various APIs that virtual kubelet exposes. For example, the API for the lifecycle of a pod:\n\nThis interface is easy to map to the Fly Machines API. For example:\n\nK3s, meanwhile, is a stripped-down implementation of all of K8s that fits into a single binary. K3s does a bunch of clever things to be as streamlined as it is, but the most notable of them is kine, an API shim that switches etcd out with databases like SQLite. Because of kine, K3s can manage multiple servers, but also gracefully runs on a single server, without distributed state.\n\nSo that‚Äôs what we do. When you create a cluster, we run K3s and the Virtual Kubelet on a single Fly Machine. We compile a kubeconfig, with which you can talk to your K3s via kubectl. We set the whole thing up to run Pods on individual Fly Machines, so your cluster scales out directly using our platform, but with K8s tooling.\n\nOne thing we like about this design is how much of the lifting is already done for us by the underlying platform. If you‚Äôre a K8s person, take a second to think of all the different components you‚Äôre dealing with: etcd, specifically provisioned nodes, the kube-proxy, a CNI binary and configuration and its integration with the host network, containerd, registries. But Fly.io already does most of those things. So this project was mostly chipping away components until we found the bare minimum: CoreDNS, SQLite persistence, and Virtual Kubelet.\n\nWe ended up with something significantly simpler than K3s, which is saying something.\n\nFly Kubernetes has some advantages over plain flyctl and fly.toml:\n\nThis is a different way to do orchestration and scheduling on Fly.io. It‚Äôs not what everyone is going to want. But if you want it, you really want it, and we‚Äôre psyched to give it to you: Fly.io‚Äôs platform features, with Kubernetes handling configuration and driving your system to its desired state.\n\nWe‚Äôve kept things simple to start with. There are K8s use cases we‚Äôre a strong fit for today, and others we‚Äôll get better at in the near future, as K8s users drive the underlying platform (and particularly our proxy) forward.\n\nInterested in getting early access? Email us at sales@fly.io and we‚Äôll hook you up.\n\nNothing has to change for you! You can deploy apps on Fly.io today, in a matter of minutes, without talking to Sales.\n\n\n## What It All Means\n\nOne obvious thing it means is that you‚Äôve got an investment in Kubernetes tooling, you can keep it while running things on top of Fly.io. So that‚Äôs pretty neat. Buy our cereal!\n\nBut the computer science story is interesting, too. We placed a bet on an idiosyncratic strategy for doing global orchestration. We replaced global consensus, which is how Borg, Kubernetes, and Nomad all work, with a market-based system. That system was faster and, importantly, dumber than the consensus system it replaced.\n\nThis had costs! Nomad‚Äôs global consensus would do truly heroic amounts of work to make sure Fly Apps got scheduled somewhere, anywhere. Like a good capitalist, Fly Machines will tell you in no uncertain terms how much work it‚Äôs willing to do for you (‚Äúless than a Nomad‚Äù).\n\nBut that doesn‚Äôt mean you‚Äôre stuck with the answers Fly Machines gives by itself. Because Fly Machines is so simple, and tries so hard to be predictable, we hoped you‚Äôd be able to build more sophisticated scheduling and orchestration schemes on top of it. And here you go: Kubernetes scheduling, as a plugin to the platform.\n\nMore to come! We‚Äôre itching to see just how many different ways this bet might pay off. Or: we‚Äôll perish in flames! Either way, it‚Äôll be fun to watch."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-io-has-gpus-now/",
    "content": "We‚Äôre Fly.io, we‚Äôre a new public cloud that lets you put your compute where it matters: near your users. Today we‚Äôre announcing that you can do this with GPUs too, allowing you to do AI workloads on the edge. Want to find out more? Keep reading.\n\n\n## AI is pretty fly\n\nAI is apparently a bit of a thing (maybe even an thing come to think about it). We‚Äôve seen entire industries get transformed in the wake of ChatGPT existing (somehow it‚Äôs only been around for a year, I can‚Äôt believe it either). It‚Äôs likely to leave a huge impact on society as a whole in the same way that the Internet did once we got search engines. Like any good venture-capital funded infrastructure provider, we want to enable you to do hilarious things with AI using industrial-grade muscle.\n\nFly.io lets you run a full-stack app‚Äîor an entire dev platform based on the Fly Machines API‚Äîclose to your users. Fly.io GPUs let you attach an Nvidia A100 to whatever you‚Äôre building, harnessing the full power of CUDA with more VRAM than your local 4090 can shake a ray-traced stick at. With these cards (or whatever you call a GPU attached to SXM fabric), AI/ML workloads are at your fingertips. You can recognize speech, segment text, summarize articles, synthesize images, and more at speeds that would make your homelab blush. You can even set one up as your programming companion with your model of choice in case you‚Äôve just not been feeling it with the output of other models changing over time.\n\nIf you want to find out more about what these cards are and what using them is like, check out What are these ‚ÄúGPUs‚Äù really? It covers the history of GPUs and why it‚Äôs ironic that the cards we offer are called ‚ÄúGraphics Processing Units‚Äù in the first place.\n\n\n## Fly.io GPUs in Action\n\nWe want you to deploy your own code with your favorite models on top of Fly.io‚Äôs cloud backbone. Fly.io GPUs make this really easy.\n\nYou can get a GPU app running Ollama (our friends in text generation) in two steps:\n\nPut this in your fly.toml:\n\nRun fly apps create sandwich_ai \u0026\u0026 fly deploy.\n\nIf you want to read more about how to start your new sandwich empire, check out Scaling Large Language Models to zero with Ollama, it explains how to set up Ollama so that it automatically scales itself down when it‚Äôs not in use.\n\n\n## The speed of light is only so fast\n\nBeing able to spin up GPUs is great, but where Fly.io really shines is inference at the edge.\n\nLet‚Äôs say you have an app that lets users enter ingredients they have in their kitchen and receive a sandwich recipe. Your users expect their recipes instantly (or at least as fast as the other leading apps). Seconds count when you need an emergency sandwich.\n\nIt‚Äôs depressingly customary in the AI industry to cherry-pick outputs. This was not cherry-picked. I used yi:34b to generate this recipe. I‚Äôm not sure what a taco salad sandwich is, but I might be willing to try it.\n\nIn the previous snippet, we deployed our app to ord (primary_region = \"ord\"). The good news is that our model returns a result really quickly and users in Chicago get instant sandwich recipes. It‚Äôs a good experience for users near your datacentre, and you can do this on any half decent cloud provider.\n\nBut surely people outside of Chicago need sandwiches too. Amsterdam has sandwich fiends as well. And sometimes it takes too long to have their requests leap across the pond. The speed of light is only so fast after all. Don‚Äôt worry, we‚Äôve got your back. Fly.io has GPUs in datacentres all over the world. Even more, we‚Äôll let you run the same program with the same public IP address and the same TLS certificates in any regions with GPU support.\n\nDon‚Äôt believe us? See how you can scale your app up in Amsterdam with one command:\n\nIt‚Äôs that easy.\n\n\n## Actually On-Demand\n\nGPUs are powerful parallel processing packages, but they‚Äôre not cheap! Once we have enough people wanting to turn their fridge contents into tasty sandwiches, keeping a GPU or two running makes sense. But we‚Äôre just a small app still growing our user base while also funding the latest large sandwich model research. We want to only pay for GPUs when a user makes a request.\n\nLet‚Äôs open up that fly.toml again, and add a section called services, and we‚Äôll include instructions on how we want our app to scale up and down:\n\nNow when no one needs sandwich recipes, you don‚Äôt pay for GPU time.\n\n\n## The Deets\n\nWe have GPUs ready to use in several US and EU regions and Sydney. You can deploy your sandwich, music generation, or AI illustration apps to:\n\nBy default, anything you deploy to GPUs will use eight heckin‚Äô AMD EPYC CPU cores, and you can attach volumes up to 500 gigabytes. We‚Äôll even give you discounts for reserved instances and dedicated hosts if you ask nicely.\n\nWe hope you have fun with these new cards and we‚Äôd love to see what you can do with them! Reach out to us on X (formerly Twitter) or the community forum and share what you‚Äôve been up to. We‚Äôd love to see what we can make easier!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/what-are-these-gpus-really/",
    "content": "Fly.io runs containerized apps with virtual machine isolation on our own hardware around the world, so you can safely run your code close to where your users are. We‚Äôre in the process of rolling out GPU support, and that‚Äôs what this post is about, but you don‚Äôt have to wait for that to try us out: your app can be up and running on us in minutes.\n\nGPU hardware will let our users run all sorts of fun Artificial Intelligence and Machine Learning (AI/ML) workloads near their users. But, what are these ‚ÄúGPUs‚Äù really? What can they do? What can‚Äôt they do?\n\nListen here for my tale of woe as I spell out exactly what these cards are, are not, and what you can do with them. By the end of this magical journey, you should understand the true irony of them being called ‚ÄúGraphics Processing Units‚Äù and why every marketing term is always bad forever.\n\n\n## How does computer formed?\n\nIn the early days of computing, your computer generally had a few basic components:\n\nTaking the Commodore 64 as an example, it had a CPU, a chip to handle video output, a chip to handle audio output, and a chip to glue everything together. The CPU would read instructions from the RAM and then execute them to do things like draw to the screen, solve sudoku puzzles, play sounds, and so on.\n\nHowever, even though the CPU by itself was fast by the standards of the time, it could only do a million clock cycles per second or so. Imagine a very small shouting crystal vibrating millions of times per second triggering the CPU to do one part of a task and you‚Äôll get the idea. This is fast, but not fast enough when executing instructions can take longer than a single clock cycle and when your video output device needs to be updated 60 times per second.\n\nThe main way they optimized this was by shunting a lot of the video output tasks to a bespoke device called the VIC-II (Video Interface Chip, version 2). This allowed the Commodore 64 to send a bunch of instructions to the VIC-II and then let it do its thing while the CPU was off doing other things. This is called ‚Äúoffloading‚Äù.\n\nAs technology advanced, the desire to do bigger and better things with both contemporary and future hardware increased. This came to a head when this little studio nobody had ever heard of called id Software released one of the most popular games of all time: DOOM.\n\nNow, even though DOOM was a huge advancement in gaming technology, it was still incredibly limited by the hardware of the time. It was actually a 2D game that used a lot of tricks to make it look (and feel) like it was 3D. It was also limited to a resolution of 320x200 and a hard cap of 35 frames per second. This was fine for the time (most movies were only at 24 frames per second), but it was clear that there was a lot of room for improvement.\n\nOne of the main things that DOOM did was to use a pair of techniques to draw the world at near real-time. It used a combination of ‚Äúraycasting‚Äù and binary-space partitioning to draw the world. This basically means that they drew a bunch of imaginary lines to where points in the map would be to figure out what color everything would be and then eliminated the parts of the map that were behind walls and other objects. This is a very simplified explanation, and if you want to know more, Fabien Sanglard explains the rendering of DOOM in more detail.\n\n\n## The dream of 3D\n\nHowever, a lot of this was logic that ran very slowly on the CPU, and while the CPU was doing the display logic, it couldn‚Äôt do anything else, such as enemy AI or playing sounds. Hence the idea of a ‚Äú3D accelerator card‚Äù. The idea: offload the 3D rendering logic to a separate device that could do it much faster than the CPU could, and free the CPU to do other things like AI, sound, and so on.\n\nThis was the dream, but it was a long way off. Then Quake happened.\n\nReally, Half-Life is based on Quake so much that the pattern for blinking lights has carried forward 25 years later to Half-Life: Alyx in VR. If it ain‚Äôt broke, don‚Äôt fix it.\n\nUnlike Doom, Quake was fully 3D on unmodified consumer hardware. Players could look up and down (something previously thought impossible without accelerator hardware!) and designers could make levels with that in mind. Quake also allowed much more complex geometry and textures. It was a huge leap forward in 3D gaming and it was only possible because of the massive leap in CPU power at the time. The Pentium family of processors was such a huge leap that it allowed them to bust through and do it in ‚Äúreal time‚Äù. Quake has since set the standard for multiplayer deathmatch games, and its source code has lineage to Call of Duty, Half-Life, Half-Life 2, DotA 2, Titanfall, and Apex Legends.\n\nHowever, the thing that really made 3D accelerator cards leap into the public spotlight was another little-known studio called Crystal Dynamics and their 1996 release of Tomb Raider. It was built from the ground up to require the use of 3D accelerator cards. The cards flew off the shelves.\n\n‚Äú3D accelerator cards‚Äù would later become known as ‚ÄúGraphics Processing Units‚Äù or GPUs because of how synonymous they became with 3D gaming, engineering tasks such as Computer-Aided Drafting (CAD), and even the entire OS environment with compositors like DWM on Windows Vista, Compiz on GNU+Linux, and Quartz on macOS. Things became so much easier for everyone when 2D and 3D graphics were integrated into the same device so you didn‚Äôt need to chain your output through your 3D accelerator card!\n\n\n## The GPU as we know it\n\nWhen GPUs first came out, they were very simple devices. They had a few basic components:\n\nThis basic architecture has remained the same for the past 20 years or so. The main differences are that as technology advanced, the capabilities of those cards increased. They got faster, more parallel, more capable, had more memory, were made cheaper, and so on. This gradually allowed for more and more complex games like Half-Life 2, Crysis, The Legend of Zelda: Breath of the Wild, Baudur‚Äôs Gate 3, and so on.\n\nOver time, as more and more hardware was added, GPUs became computers in their own rights (sometimes even bigger than the rest of the computer thanks for the need to cool things more aggressively). This new hardware includes:\n\nBut, at the same time, that AI/ML hardware started to get noticed by more and more people. It was discovered that the shader cores and then the CUDA cores could be used to do AI/ML workloads at ludicrous speeds. This enabled research and development of models like GPT-2, Stable Diffusion, DLSS, and so on. This has led to a Cambrian Explosion of AI/ML research and development that is continuing to this day.\n\n\n## The ‚ÄúGPUs‚Äù that Fly.io is using\n\nI‚Äôve mostly been describing consumer GPUs and their capabilities up to this point because that‚Äôs what we all have the biggest understanding of. There is a huge difference between the ‚ÄúGPUs‚Äù that you can get for server tasks and normal consumer tasks from a place like Newegg or Best Buy. The main difference is that enterprise-grade Graphics Processing Units do not have any of the hardware needed to process graphics.\n\nAuthor‚Äôs note: This will not be the case in the future. Fly.io is going to add Lovelace L40S GPUs that do have 3D rendering, video encoding, shader cores, and so on. But, that‚Äôs not what we‚Äôre talking about today.\n\nYes. Really. They don‚Äôt have rasterization hardware, shader cores, display outputs, or anything useful for trying to run games on them. They are AI/ML accelerator cards more than anything. It‚Äôs kinda beautifully ironic that they‚Äôre called Graphics Processing Units when they have no ability to process graphics.\n\n\n## What can you do with them?\n\nThese GPUs are really good at massively parallel tasks. This naturally translates to being very good at AI/ML tasks such as:\n\nOr any combination/chain of these tasks. A lot of this is pretty abstract building blocks that can be combined in a lot of different ways. This is why AI/ML stuff is so exciting right now. We‚Äôre in the early days of understanding what these things are, what they can do, and how to use them properly.\n\nImagine being able to load articles about the topic you are researching into your queries to find where someone said something roughly similar to what you‚Äôre looking for. Queries like ‚Äúthat one recipe with eggs that you fold over with ham in it‚Äù. That‚Äôs the kind of thing that‚Äôs possible with AI/ML (and tools like vector databases) but difficult to impossible with traditional search engines.\n\n\n## How to use AI for reals\n\nFortunately and unfortunately, we‚Äôre in the Cambrian Explosion days of this industry. Key advances happen constantly. Exact models and tooling changes almost as often. This is both a very good thing and a very bad thing.\n\nIf you want to get started today, here‚Äôs a few models that you can play with right now:\n\nFor a practical example, imagine that you have a set of conference talks that you‚Äôve given over the years. You want to take those talk videos, extract the audio, and transform them into written text because some people learn better from text than video. The overall workflow would look something like this:\n\nThen bam, you don‚Äôt just have a portfolio piece, you have the recipe for winning downtime from visitors of orange websites clicking on your link so much. You can also use this to create transcripts for your videos so that people who can‚Äôt hear can still enjoy your content.\n\nThe true advantage of these is not using them as individual parts on themselves, but as a cohesive whole in a chain. This is where the real power of AI/ML comes from. It‚Äôs not the individual models, but the ability to chain them together to do something useful. This is where the true opportunities for innovation lie.\n\n\n## Conclusion\n\nSo that‚Äôs what these ‚ÄúGPUs‚Äù are really: they‚Äôre AI/ML accelerator cards. The A100 cards incapable of processing graphics or encoding video, but they‚Äôre really, really good at AI/ML workloads. They allow you to do way more tasks per watt than any CPU ever could.\n\nI hope you enjoyed this tale of woe as I spilled out the horrible truths about marketing being awful forever and gave you ideas for how to actually use these graphics-free Graphics Processing Units to do useful things. But sadly, not for processing graphics unless you wait for the Lovelace L40S cards early in 2024.\n\nSign up for Fly.io today and try our GPUs! I can‚Äôt wait to see what you build with them."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/scaling-llm-ollama/",
    "content": "We‚Äôre Fly.io. We have powerful servers worldwide to run your code close to your users. Including GPUs so you can self host your own AI.\n\nOpen-source self-hosted AI tools have advanced a lot in the past 6 months. They allow you to create new methods of expression (with QR code generation and Stable Diffusion), easy access to summarization powers that would have made Google blush a decade ago (even with untuned foundation models such as LLaMa 2 and Yi), to conversational assistants that enable people to do more with their time, and to perform speech recognition in real time on moderate hardware (with Whisper et al). With all these capabilities comes the need for more and more raw computational muscle to be able to do inference on bigger and bigger models, and eventually do things that we can‚Äôt even imagine right now. Fly.io lets you put your compute where your users are so that you can do machine learning inference tasks on the edge with the power of enterprise-grade GPUs such as the Nvidia A100. You can also scale your GPU nodes to zero running Machines, so you only pay for what you actually need, when you need it.\n\nIt‚Äôs worth mentioning that ‚Äúscaling to zero‚Äù doesn‚Äôt mean what you may think it means. When you ‚Äúscale to zero‚Äù in Fly.io, you actually stop the running Machine. This means the Machine is still laying around on the same computer box that it runs on, but it‚Äôs just put to sleep. If there is a capacity issue then your app may be unable to wake back up. We are working on a solution to this, but for now you should be aware that scaling to zero is not the same as spinning down your Machine and spinning it back up again on a new computer box when you need it.\n\nThis is a continuation of the last post in this series about how to use GPUs on Fly.io.\n\n\n## Why scale to zero?\n\nRunning GPU nodes on top of Fly is expensive. Sure, GPUs enable you to do things a lot faster than CPUs ever could on their own, but you mostly will have things run idle between uses. This is where scaling to zero comes in. With scaling to zero, you can have your GPU nodes shut down when you‚Äôre not using them. When your Machine stops, you aren‚Äôt paying for the GPU any more. This is good for the environment and your wallet.\n\nIn this post, we‚Äôre going to be using Ollama to generate text. Ollama is a fancy wrapper around llama.cpp that allows you to run large language models on your own hardware with your choice of model. It also supports GPU acceleration, meaning that you can use Fly.io‚Äôs huge GPUs to run your models faster than your RTX 3060 at home ever would on its own.\n\nOne of the main downsides of using Ollama in a cloud environment is that it doesn‚Äôt have authentication by default. Thanks to the power of about 70 lines of Go, we are able to shim that in after the fact. This will protect your server from random people on the internet using your GPU time (and spending your money) to generate text and integrate it into your own applications.\n\nCreate a new folder called ollama-scale-to-0:\n\n\n## Fly app setup\n\nFirst, we need to create a new Fly app:\n\nAfter selecting a name and an organization to run it in, this command will create the app and write out a fly.toml file for you:\n\nThis is the configuration file that Fly.io uses to know how to run your application. We‚Äôre going to be modifying the fly.toml file to add some additional configuration to it, such as enabling GPU support:\n\nWe don‚Äôt want to expose the GPU to the internet, so we‚Äôre going to create a flycast address to expose it to other services on your private network. To create a flycast address, run this command:\n\nThe fly ips allocate-v6 command makes a unique address in your private network that you can use to access Ollama from your other services. Make sure to add the --private flag, otherwise you‚Äôll get a globally unique IP address instead of a private one.\n\nNext, you may need to remove all of the other public IP addresses for the app to lock it away from the public. Get a list of them with fly ips list and then remove them with fly ips release \u003cip\u003e. Delete everything but your flycast IP.\n\nNext, we need to declare the volume for Ollama to store models in. If you don‚Äôt do this, then when you scale to zero, your existing models will be destroyed and you will have to re-download them every time the server starts. This is not ideal, so we‚Äôre going to create a persistent volume to store the models in. Add the following to your fly.toml:\n\nThis will create a 100GB volume in the ord region when the app is deployed. This will be used to store the models that you download from the Ollama library. You can make this smaller if you want, but 100GB is a good place to start from.\n\nNow that everything is set up, we can deploy this to Fly.io:\n\nThis will take a minute to pull the Ollama image, push it to a Machine, provision your volume, and kick everything else off with hypervisors, GPUs and whatnot. Once it‚Äôs done, you should see something like this:\n\nThis is a lie because we just deleted the public IP addresses for this app. You can‚Äôt access it from the internet, and by extension, random people can‚Äôt access it either. For now, you can run an interactive session with Ollama using an ephemeral Fly Machine:\n\nAnd then you can pull an image from the ollama library and generate some text:\n\nIf you want a persistent wake-on-use connection to your Ollama instance, you can set up a connection to your Fly network using WireGuard. This will let you use Ollama from your local applications without having to run them on Fly. For example, if you want to figure out the safe cooking temperature for ground beef in Celsius, you can query that in JavaScript with this snippet of code:\n\n\n## Scaling to zero\n\nThe best part about all of this is that when you want to scale down to zero running Machines: do nothing, it will automatically shut down when it‚Äôs idle. Wait a few minutes and then verify it with fly status:\n\nThe app has been stopped. This means that it‚Äôs not running and you‚Äôre not paying for it. When you want it to start up again, just make a request. It will automatically start up and you can use it as normal with the CLI or even just arbitrary calls to the API.\n\nYou can also upload your own models to the Ollama registry by creating your own Modelfile and pushing it (though you will need to install Ollama locally to publish your own models). At this time, the only way to set a custom system prompt is to use a Modelfile and upload your model to the registry.\n\n\n## Conclusion\n\nOllama is a fantastic way to run large language models of your choice and the ability to use Fly.io‚Äôs powerful GPUs means you can use bigger models with more parameters and a larger context window. This lets you make your assistants more lifelike, your conversations have more context, and your text generation more realistic.\n\nOh, by the way, this also lets you use the new json mode to have your models call functions, similar to how ChatGPT would. To do this, have a system prompt that looks like this:\n\nThen you can use the JSON format to receive a JSON response from Ollama (hint: ‚Äîformat=json in the CLI or format: \"json\" in the API). This is a great way to make your assistants more lifelike and more useful. You will need to use something like Langchain or manual iterations to properly handle the cases where the user doesn‚Äôt want to call a function, but that‚Äôs a topic for another blog post.\n\nFor the best results you may want to use a model with a larger context window such as vicuna:13b-v1.5-16k-fp16 (16k == 16,384 token window) as JSON is very token-expensive. Future advances in the next few weeks (such as the Yi models gaining ludicrous token windows on the line of 200,000 tokens at the cost of ludicrous amounts of VRAM usage) will make this less of an issue. You can also get away with minifying the JSON in the functions and examples a lot, but you may need to experiment to get the best results.\n\nHappy hacking, y'all."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/rethinking-serverless-with-flame/",
    "content": "The pursuit of elastic, auto-scaling applications has taken us to silly places.\n\nServerless/FaaS had a couple things going for it. Elastic Scale‚Ñ¢ is hard. It‚Äôs even harder when you need to manage those pesky servers. It also promised pay-what-you-use costs to avoid idle usage. Good stuff, right?\n\nWell the charade is over. You offload scaling concerns and the complexities of scaling, just to end up needing more complexity. Additional queues, storage, and glue code to communicate back to our app is just the starting point. Dev, test, and CI complexity balloons as fast as your costs. Oh, and you often have to rewrite your app in proprietary JavaScript ‚Äì even if it‚Äôs already written in JavaScript!\n\nAt the same time, the rest of us have elastically scaled by starting more webservers. Or we‚Äôve dumped on complexity with microservices. This doesn‚Äôt make sense. Piling on more webservers to transcode more videos or serve up more ML tasks isn‚Äôt what we want. And granular scale shouldn‚Äôt require slicing our apps into bespoke operational units with their own APIs and deployments to manage.\n\nEnough is enough. There‚Äôs a better way to elastically scale applications.\n\n\n## The FLAME pattern\n\nHere‚Äôs what we really want:\n\nImagine if we could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of the app.\n\nEnter the FLAME pattern.\n\nWith FLAME, you treat your entire application as a lambda, where modular parts can be executed on short-lived infrastructure.\n\nNo rewrites. No bespoke runtimes. No outrageous layers of complexity. Need to insert the results of an expensive operation to the database? PubSub broadcast the result of some expensive work? No problem! It‚Äôs your whole app so of course you can do it.\n\nThe Elixir flame library implements the FLAME pattern. It has a backend adapter for Fly.io, but you can use it on any cloud that gives you an API to spin up an instance with your app code running on it. We‚Äôll talk more about backends in a bit, as well as implementing FLAME in other languages.\n\nFirst, lets watch a realtime thumbnail generation example to see FLAME + Elixir in action:\n\nNow let‚Äôs walk thru something a little more basic. Imagine we have a function to transcode video to thumbnails in our Elixir application after they are uploaded:\n\nOur generate_thumbnails function accepts a video struct. We shell out to ffmpeg to take the video URL and generate thumbnails at a given interval. We then write the temporary thumbnail paths to durable storage. Finally, we insert the generated thumbnail URLs into the database.\n\nThis works great locally, but CPU bound work like video transcoding can quickly bring our entire service to a halt in production. Instead of rewriting large swaths of our app to move this into microservices or some FaaS, we can simply wrap it in a FLAME call:\n\nThat‚Äôs it! FLAME.call accepts the name of a runner pool, and a function. It then finds or boots a new copy of our entire application and runs the function there. Any variables the function closes over (like our %Video{} struct and interval) are passed along automatically.\n\nWhen the FLAME runner boots up, it connects back to the parent node, receives the function to run, executes it, and returns the result to the caller. Based on configuration, the booted runner either waits happily for more work before idling down, or extinguishes itself immediately.\n\nLet‚Äôs visualize the flow:\n\nWe changed no other code and issued our DB write with Repo.insert_all just like before, because we are running our entire application. Database connection(s) and all. Except this fleeting application only runs that little function after startup and nothing else.\n\nIn practice, a FLAME implementation will support a pool of runners for hot startup, scale-to-zero, and elastic growth. More on that later.\n\n\n## Solving a problem vs removing the problem\n\nThe FaaS labyrinth of complexity defies reason. And it‚Äôs unavoidable. Let‚Äôs walkthrough the thumbnail use-case to see how.\n\nWe try to start with the simplest building block like request/response AWS Lambda Function URL‚Äôs.\n\nThe complexity hits immediately.\n\nWe start writing custom encoders/decoders on both sides to handle streaming the thumbnails back to the app over HTTP. Phew that‚Äôs done. Wait, is our video transcoding or user uploads going to take longer than 15 minutes? Sorry, hard timeout limit¬†‚Äì¬†time to split our videos into chunks to stay within the timeout, which means more lambdas to do that. Now we‚Äôre orchestrating lambda workflows and relying on additional services, such as SQS and S3, to enable this.\n\nAll the FaaS is doing is adding layers of communication between your code and the parts you want to run elastically. Each layer has its own glue integration price to pay.\n\nUltimately handling this kind of use-case looks something like this:\n\nThis is nuts. We pay the FaaS toll at every step. We shouldn‚Äôt have to do any of this!\n\nFaaS provides a bunch of offerings to build a solution on top of. FLAME removes the problem entirely.\n\n\n## FLAME Backends\n\nBy default, FLAME ships with a LocalBackend and FlyBackend, but any host that provides an API to provision a server and run your app code can work as a FLAME backend. Erlang and Elixir primitives are doing all the heavy lifting here. The entire FLAME.FlyBackend is \u003c 200 LOC with docs. The library has a single dependency, req, which is an HTTP client.\n\nBecause Fly.io runs our applications as a packaged up docker image, we simply ask the Fly API to boot a new Machine for us with the same image that our app is currently running. Also thanks to Fly infrastructure, we can guarantee the FLAME runners are started in the same region as the parent. This optimizes latency and lets you ship whatever data back and forth between parent and runner without having to think about it.\n\n\n## Look at everything we‚Äôre not doing\n\nWith FaaS, just imagine how quickly the dev and testing story becomes a fate worse than death.\n\nTo run the app locally, we either need to add some huge dev dependencies to simulate the entire FaaS pipeline, or worse, connect up our dev and test environments directly to the FaaS provider.\n\nWith FLAME, your dev and test runners simply run on the local backend.\n\nRemember, this is your app. FLAME just controls where modular parts of it run. In dev or test, those parts simply run on the existing runtime on¬†your laptop or CI server.\n\nUsing Elixir, we can even send a file across to the remote FLAME application thanks to the distributed features of the Erlang VM:\n\nOn line 2 we open a file on the parent node to the video path. Then in the FLAME child, we stream the file from the parent node to the FLAME server in only a couple lines of code. That‚Äôs it! No setup of S3 or HTTP interfaces required.\n\nWith FLAME it‚Äôs easy to miss everything we‚Äôre not doing:\n\n\n## FLAME outside Elixir\n\nElixir is fantastically well suited for the FLAME model because we get so much for free like process supervision and distributed messaging. That said, any language with reasonable concurrency primitives can take advantage of this pattern. For example, my teammate, Lubien, created a proof of concept example for breaking out functions in your JavaScript application and running them inside a new Fly Machine: https://github.com/lubien/fly-run-this-function-on-another-machine\n\nSo the general flow for a JavaScript-based FLAME call would be to move the modular executions to a new file, which is executed on a runner pool. Provided the arguments are JSON serializable, the general FLAME flow is similar to what we‚Äôve outlined here. Your application, your code, running on fleeting instances.\n\nA complete FLAME library will need to handle the following concerns:\n\nFor the rest of this post we‚Äôll see how the Elixir FLAME library handles these concerns as well as features uniquely suited to Elixir applications. But first, you might be wondering about your background job queues.\n\n\n## What about my background job processor?\n\nFLAME works great inside your background job processor, but you may have noticed some overlap. If your job library handles scaling the worker pool, what is FLAME doing for you? There‚Äôs a couple important distinctions here.\n\nFirst, we reach for these queues when we need durability guarantees. We often can turn knobs to have the queues scale to handle more jobs as load changes. But durable operations are separate from elastic execution. Conflating these concerns can send you down a similar path to lambda complexity. Leaning on your worker queue purely for offloaded execution means writing all the glue code to get the data into and out of the job, and back to the caller or end-user‚Äôs device somehow.\n\nFor example, if we want to guarantee we successfully generated thumbnails for a video after the user upload, then a job queue makes sense as the dispatch, commit, and retry mechanism for this operation. The actual transcoding could be a FLAME call inside the job itself, so we decouple the ideas of durability and scaled execution.\n\nOn the other side, we have operations we don‚Äôt need durability for. Take the screencast above where the user hasn‚Äôt yet saved their video. Or an ML model execution where there‚Äôs no need to waste resources churning a prompt if the user has already left the app. In those cases, it doesn‚Äôt make sense to write to a durable store to pick up a job for work that will go right into the ether.\n\n\n## Pooling for Elastic Scale\n\nWith the Elixir implementation of FLAME, you define elastic pools of runners. This allows scale-to-zero behavior while also elastically scaling up FLAME servers with max concurrency limits.\n\nFor example, lets take a look at the start/2 callback, which is the entry point of all Elixir applications. We can drop in a FLAME.Pool for video transcriptions and say we want it to scale to zero, boot a max of 10, and support 5 concurrent ffmpeg operations per runner:\n\nWe use the presence of a FLAME parent to conditionally start our Phoenix webserver when booting the app. There‚Äôs no reason to start a webserver if we aren‚Äôt serving web traffic. Note we leave other services like the database MyApp.Repo alone because we want to make use of those services inside FLAME runners.\n\nElixir‚Äôs supervised process approach to applications is uniquely great for turning these kinds of knobs.\n\nWe also set our pool to idle down after 30 seconds of no caller operations. This keeps our runners hot for a short while before discarding them. We could also pass a min: 1 to always ensure at least one ffmpeg runner is hot and ready for work by the time our application is started.\n\n\n## Process Placement\n\nIn Elixir, stateful bits of our applications are built around the process primitive ‚Äì¬†lightweight greenthreads with message mailboxes. Wrapping our otherwise stateless app code in a synchronous FLAME.call‚Äòs or async FLAME.cast‚Äôs works great, but what about the stateful parts of our app?\n\nFLAME.place_child exists to take an existing process specification in your Elixir app and start it on a FLAME runner instead of locally. You can use it anywhere you‚Äôd use Task.Supervisor.start_child , DynamicSupervisor.start_child, or similar interfaces. Just like FLAME.call, the process is run on an elastic pool and runners handle idle down when the process completes its work.\n\nAnd like FLAME.call, it lets us take existing app code, change a single LOC, and continue shipping features.\n\nLet‚Äôs walk thru the example from the screencast above. Imagine we want to generate video thumbnails for a video as it is being uploaded. Elixir and LiveView make this easy. We won‚Äôt cover all the code here, but you can view the full app implementation.\n\nOur first pass would be to write a LiveView upload writer that calls into a ThumbnailGenerator:\n\nAn upload writer is a behavior that simply ferries the uploaded chunks from the client into whatever we‚Äôd like to do with them. Here we have a ThumbnailGenerator.open/1 which starts a process that communicates with an ffmpeg shell. Inside ThumbnailGenerator.open/1, we use regular elixir process primitives:\n\nThe details aren‚Äôt super important here, except line 10 where we call {:ok, pid} = DynamicSupervisor.start_child(@sup, spec), which starts a supervisedThumbnailGenerator process. The rest of the implementation simply ferries chunks as stdin into ffmpeg and parses png‚Äôs from stdout. Once a PNG delimiter is found in stdout, we send the caller process (our LiveView process) a message saying ‚Äúhey, here‚Äôs an image‚Äù:\n\nThe caller LiveView process then picks up the message in a handle_info callback and updates the UI:\n\nThe send(caller, {ref, :image, state.count, encode(state)} is one magic part about Elixir. Everything is a process, and we can message those processes, regardless of their location in the cluster.\n\nIt‚Äôs like if every instantiation of an object in your favorite OO lang included a cluster-global unique identifier to work with methods on that object. The LiveView (a process) simply receives the image message and updates the UI with new images.\n\nNow let‚Äôs head back over to our ThumbnailGenerator.open/1 function and make this elastically scalable.\n\nThat‚Äôs it! Because everything is a process and processes can live anywhere, it doesn‚Äôt matter what server our ThumbnailGenerator process lives on. It simply messages the caller with send(caller, ‚Ä¶) and the messages are sent across the cluster if needed.\n\nOnce the process exits, either from an explicit close, after the upload is done, or from the end-user closing their browser tab, the FLAME server will note the exit and idle down if no other work is being done.\n\nCheck out the full implementation if you‚Äôre interested.\n\n\n## Remote Monitoring\n\nAll this transient infrastructure needs failsafe mechanisms to avoid orphaning resources. If a parent spins up a runner, that runner must take care of idling itself down when no work is present and handle failsafe shutdowns if it can no longer contact the parent node.\n\nLikewise, we need to shutdown runners when parents are rolled for new deploys as we must guarantee we‚Äôre running the same code across the cluster.\n\nWe also have active callers in many cases that are awaiting the result of work on runners that could go down for any reason.\n\nThere‚Äôs a lot to monitor here.\n\nThere‚Äôs also a number of failure modes that make this sound like a harrowing experience to implement. Fortunately Elixir has all the primitives to make this an easy task thanks to the Erlang VM. Namely, we get the following for free:\n\nWe‚Äôll cover the internal implementation details in a future deep-dive post. For now, feel free to poke around the flame source.\n\n\n## What‚Äôs Next\n\nWe‚Äôre just getting started with the Elixir FLAME library, but it‚Äôs ready to try out now. In the future look for more advance pool growth techniques, and deep dives into how the Elixir implementation works. You can also find me @chris_mccord to chat about implementing the FLAME pattern in your language of choice.\n\nHappy coding!\n\n‚ÄìChris"
  },
  {
    "title": "Fly.io also offer GPUs",
    "url": "https://fly.io/blog/the-risks-of-building-apps-on-chatgpt/",
    "content": "If AI will play an essential role in your application, then consider using a self-hosted, open source model instead of a proprietary and externally hosted one. In this post we explore some of the risks for the latter option. We‚Äôre Fly.io. We put your code into lightweight microVMs on our own hardware around the world. Check us out‚Äîyour app can be deployed in minutes.\n\nThe topic of ‚ÄúAI‚Äù gets a lot of attention and press. Coverage ranges from apocalyptic warnings to Utopian predictions. The truth, as always, is likely somewhere in the middle. As developers, we are the ones that either imagine ways that AI can be used to enhance our products or the ones doing the herculean tasks of implementing it inside our companies.\n\nI believe the following statement to be true:\n\nAI won‚Äôt replace humans ‚Äî but humans with AI will replace humans without AI.\n\nI believe this can be extended to many products and services and the companies that create them. Let‚Äôs express it this way:\n\nAI won‚Äôt replace businesses ‚Äî but businesses with AI will replace businesses without AI.\n\nToday I‚Äôm assuming your business would benefit from using AI. Or, at the very least, your C-levels have decreed from on high that thou must integrateth with AI. With that out of the way, the next question is how you‚Äôre meant to do it. This post is an argument to build on top of open source language models instead of closed models that you rent access to. We‚Äôll take a look at what convinced me.\n\n\n## But OpenAI is the market leader‚Ä¶\n\nOpenAI, the creators of the famous ChatGPT, are the strong market leaders in this category. Why wouldn‚Äôt you want to use the best in the business?\n\nEarly on, stories of private corporate documents being uploaded by employees and then finding that private information leaking out to general ChatGPT users was a real black eye. Companies began banning employees from using ChatGPT for work. It exposed that people‚Äôs interactions with ChatGPT were being used as training data for future versions of the model.\n\nIn response, OpenAI recently announced an Enterprise offering promising that no Enterprise customer data is used for training.\n\nWith the top objection addressed, it should be smooth sailing for wide adoption, right?\n\nNot so fast.\n\nWhile an Enterprise offering may address that concern, there are other subtle reasons to not use OpenAI, or other closed models, that can‚Äôt be resolved by vague statements of enterprise privacy.\n\n\n## What are the risks for building on top of OpenAI?\n\nLet‚Äôs briefly outline the risks we take on when relying on a company like OpenAI for critical AI features in our applications.\n\nLet‚Äôs look a bit closer at the ‚ÄúSingle provider risk‚Äù.\n\n\n## Single provider risk\n\nFor hobby usage, proof of concept work, and personal experiments, by all means, use ChatGPT! I do and I expect to continue to as well. It‚Äôs fantastic for prototyping, it‚Äôs trivial to set up, and it allows you to throw ink on canvas so much more quickly than any other option out there.\n\nUp until recently, I was all gung-ho for ChatGPT being integrated into my apps. What happened? November 2023 happened. It was a very bad month for OpenAI.\n\nI created a Personal AI Fitness Trainer powered by ChatGPT and on the morning of November 8th, I asked my personal trainer about the workout for the day and it failed. OpenAI was having a bad day with an outage.\n\nI don‚Äôt fault someone for having a bad day. At some point, downtime happens to the best of us. And given enough time, it happens to all of us. But when possible, I want to prevent someone else‚Äôs bad day from becoming my bad day too.\n\n\n## Evaluating a critical dependency\n\nIn my case, my personal fitness trainer being unavailable was a minor inconvenience, but I managed. However, it gave me pause. If I had built an AI fitness trainer as a service, that outage would be a much bigger deal and there would be nothing I could have done to fix it until the ChatGPT API came back up.\n\nWith services like a Personal AI Fitness Trainer, the AI component is the primary focus and main value proposition of the app. That‚Äôs pretty darn critical! If that AI service is interrupted, significantly altered (say, by the model suddenly refusing my requests for fitness information in ways that worked before) or my desired usage is denied (without warning or reason), the application is useless. That‚Äôs an existential threat that could make my app evaporate overnight without warning.\n\nThis highlights the risk of having a critical dependency on an external service.\n\nModern applications depend on many services, both internal and external. But how critical that dependency is matters.\n\nLet‚Äôs take a very simple application as an example. The application has a critical dependency on the database and both the app and database have a critical dependency on the underlying VMs/machines/provider. These critical dependencies are so common that we seldom think about them because we deal with them every day we come to work. It‚Äôs just how things are.\n\nThe danger comes when we draw a critical dependency line to an external service. If the service has a hiccup or the network between my app and their service starts dropping all my packets, the entire application goes down. Someone else‚Äôs bad day gets spread around when that happens. üòû\n\nIn order to protect ourselves from a risk like that, we should diversify our reliance away from a single external provider. How do we do that? We‚Äôll come back to this later.\n\n\n## We are not without dependencies\n\nIt‚Äôs really common for apps to have external dependencies. The question is how critical to our service are those dependencies?\n\nWhat happens to the application when the external log aggregation service, email service, and error reporting services are all unreachable? If the app is designed well, then users may have a slightly degraded experience or, best case, the users won‚Äôt even notice the issues at all!\n\nThe key factor is these external services are not essential to our application functioning.\n\n\n## Regulation or Policy change risk\n\nOur industry has a lot of misconceptions, fear, uncertainty, and doubt around the idea of regulation, but sometimes it‚Äôs justified. I don‚Äôt want you to think about regulation as a scary thing that yanks away control. Instead, let‚Äôs think about regulation as when a government body gets involved to disallow businesses from doing or engaging in specific activities. Given that our industry has been so self-defined for so long, this feels like an existential threat. However, this is a good thing when we think about vehicle safety standards (you don‚Äôt want your 4-ton mass of metal exploding while traveling at 70 mph), pollution, health risks, and more. It‚Äôs a careful balance.\n\nIronically, Sam Altman has been a major proponent for government regulation of the AI industry. Why would he want that?\n\nIt turns out that regulation can also be used as a form of protectionism. Or, put another way, when the people with an early lead see that they aren‚Äôt defensible against advances with open source AI models, they want to pull up the ladders behind them and have the government make it legally harder, or impossible, for competitors to catch up to them.\n\nIf Altman‚Äôs efforts are successful, then companies who create AI can expect government involvement and oversight. Added licensing requirements and certifications would raise the cost of starting a competing business.\n\nAt this point you may be thinking something like ‚Äúbut all of that is theoretical Mark, how would this affect my business‚Äô use of AI today?‚Äù\n\nIntroducing an external organization that can dictate changes to an AI product risks breaking an existing company‚Äôs applications or significantly reducing the effectiveness of the application. And those changes may come without notice or warning.\n\nAdditionally, if my business is built on an external AI system protected from competition by regulators, that adds a significant risk. If they are now the only game in town, they can set whatever price they want.\n\n\n## Governance and leadership risk\n\nIn the week following the OpenAI outage (November 17th to be precise), the entire tech industry was upended for most of a week following a blog post on the OpenAI blog announcing that the OpenAI board fired the co-founder and CEO, Sam Altman. Then Greg Brockman, co-founder and acting President resigned in protest.\n\nOpenAI is partnered with Microsoft and on Nov 20, 2023, Satya Nadella (CEO of Microsoft) posted the following on X (formerly Twitter):\n\nWe remain committed to our partnership with OpenAI (OAI) and have confidence in our product roadmap, our ability to continue to innovate with everything we announced at Microsoft Ignite, and in continuing to support our customers and partners. We look forward to getting to know Emmett Shear and OAI‚Äôs new leadership team and working with them. And we‚Äôre extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team. We look forward to moving quickly to provide them with the resources needed for their success.\n\nMicrosoft nearly acqui-hired OpenAI for $0! That‚Äôs some serious business Jujutsu.\n\nIn the end, after 12 days of very public corporate chaos, Sam Altman and Greg Brockman returned to OpenAI at their previous leadership positions as if nothing happened (save the firing of the rest of the board).\n\nWith all the drama and uncertainty resolved, you may say, ‚Äúit all worked out in the end, right? So what‚Äôs the problem?‚Äù\n\nThis highlights the risk of building any critical business system on a product offered and hosted by an external company. When we do that, we implicitly take on all of that company‚Äôs risks in addition to the risks our business already has! In this case, it‚Äôs taking on all the risks of OpenAI while getting none of their financial benefits!\n\n\n## What‚Äôs the alternative?\n\nThe thing big AI providers like OpenAI and Google seem to fear most is competition from open source AI models. And they should be afraid. Open source AI models continue to develop at a rapid pace (there‚Äôs huge incremental improvements on a weekly basis) and, most importantly, they can be self-hosted.\n\nAdditionally, it‚Äôs not out of reach for us to fine tune a general model to better fit our needs by adding and removing capabilities rather than hope that the capabilities we need suddenly manifest for us.\n\nDoesn‚Äôt this all sound like the classic argument in favor of open source?\n\nIf we have the model and can host it ourselves, no one can take it away. When we self-host it, we are protected from:\n\nUsing an open source and self-hosted model insulates us from these external risks.\n\n\n## I still need GPUs!\n\nGetting dedicated access to a GPU is more expensive than renting limited time on OpenAI‚Äôs servers. That‚Äôs why a hobby or personal project is better off paying for the brief bits of time when needed.\n\nBut let‚Äôs face it.\n\nIf you really want to integrate AI into your business, you need to host your own models. You can‚Äôt control third party privacy policies, but you can control your own policies when you are the one doing your own inference with your own models. Ideally this means getting your own GPUs and incurring the capital expenditure and operations expenditures, but thankfully we‚Äôre in the future. We have the cloud now. There‚Äôs many options you can use for renting GPU access from other companies. This is supported in the big clouds as well as Fly.io. You can check out our GPU offerings here.\n\nRunning inference on your own hosted models can help de-risk critical AI integrations.\n\n\n## Closing thoughts\n\nIt‚Äôs important to take advantage of AI in our applications so we can reap the benefits. It can give us an important edge in the market! However, we should be extra cautious of building any critical features on a product offered by a proprietary external business. Others are considering the risks of building on OpenAI as well.\n\nYour specific level of risk depends on how central the AI aspect is to your business. If it‚Äôs a central component like in my Personal AI Fitness Trainer, then I risk losing all my customers and even the company if any of the above mentioned risk factors happen to my AI provider. That‚Äôs an existential risk that I can‚Äôt do anything about without taking emergency heroic efforts.\n\nIf the AI is sprinkled around the edges of the business, then suddenly losing it won‚Äôt kill the company. However, if the AI isn‚Äôt being well utilized, then the business may be at risk to competitors who place a bigger bet and take a bigger swing with AI.\n\nOh, what interesting times we live in! üôÉ"
  },
  {
    "title": "Scale at your own pace",
    "url": "https://fly.io/blog/print-on-demand/",
    "content": "Save money by using appliance machines to only allocate memory and other machine resources when you actually need them.\n\nScaling discussions often lead to recommendations to add more memory, more CPU, more machines, more regions, more, more, more.\n\nThis post is different. It focuses instead on the idea of decomposing parts of your applications into event handlers, starting up Machines to handle the events when needed, and stopping them when the event is done. Along the way we will see how a few built in Fly.io primitives make this easy.\n\nTo make the discussion concrete, we are going to focus on a common requirement: generation of PDFs from web pages. The code that we will introduce isn‚Äôt merely an example produced in support of a blog post - rather it is code that was extracted from a production application, and packaged up into an appliance that you can deploy in minutes to add PDF generation to your existing application.\n\nBut before we dive in, let‚Äôs back up a bit.\n\n\n## Motivation\n\nNormally the way this is approached is to start with a tool like Puppeteer, Grover, Playwright, ChromicPDF, or BrowserShot. These and other tools ultimately launch a browser like Chrome headless.\n\nNow a few things about Chrome itself:\n\nTaken together, this makes splitting PDF generation into a completely separate application an easy win. With a smaller image, your application will start faster. Memory usage will be more predictable, and the memory needed to generate PDFs will only be allocated when needed and can be scaled separately.\n\n\n## Diving in\n\nWithout further ado, the entire application is available on GitHub as fly-apps/pdf-appliance. Installation is a simple matter of: clone repository, create app, adjust config, deploy, and scale.\n\nNext, you will need to integrate this into your application. All that is needed is to reply to requests that are intended to produce a PDF with a fly-replay response header. This can either be done on individual application routes / controller actions, or it can be done globally via either middleware or a front end like NGINX. You can find a few examples in the README.\n\nAnd, that‚Äôs it. The most you might consider doing is issuing an additional HTTP request in anticipation of the user selecting what they want to print as this will preload the machine.\n\nDeploy your project in a few minutes with Fly Launch. Then do more with Fly Machines.\n\nIf you don‚Äôt have an application handy, you can try a demo. Go to smooth.fly.dev. Click on Demo, then on Publish, and finally on Invoices to see a PDF. The PDF you see will likely be underwhelming as you would need to enter students, entries, packages and options to fill out the page. But click refresh anyway and see how fast it responds. If you want to explore further, links to the documentation and code can be found on the front page.\n\n\n## Implementation Details\n\nThe basic flow starts with a request comes into your app for a PDF. That request is replayed to the PDF appliance. A Chrome instance in that app then issues a second request to your app for the same URL minus the .pdf extension and then converts the HTML which it receives in response to a PDF. That PDF is then returned as the response to the original request.\n\nA single Google Chrome instance per machine will be reused across all requests, which itself is faster than starting a new instance per request. As all HTTP headers will be passed back to your application, this will seamlessly work with your existing session, cookies, and basic authentication.\n\nStarting up a machine on demand is handled by the auto_stop_machines setting in your fly.toml. With this in place, machines can confidently exit when idle, secure in the knowledge that they will be restarted when needed. See the README for more information on scaling.\n\nNote that different machines can use different languages and frameworks. This code is written in JavaScript and runs on Bun. It was designed to support a Ruby on Rails app, but can be used with any app.\n\n\n## A Reusable Pattern\n\nIf your app is small and your usage is low, scaling may not be much of a concern, but as your need grow your first instinct shouldn‚Äôt merely be to throw more hardware at the problem, but rather to partition the problem so that each machine has a somewhat predictable capacity.\n\nDo this by taking a look at your application, and look for requests that are somehow different than the rest. Streaming audio and video files, handling websockets, converting text to speech or performing other AI processing, long running ‚Äúbackground‚Äù computation, fetching static pages, producing PDFs, and updating databases all have different profiles in terms of server load.\n\nIt might even be helpful ‚Äì purely as a thought experiment ‚Äì to think of replacing your main server with a proxy that does nothing more than route requests to separate machines based on the type of workload performed.\n\nOnce you have come up with an allocation of functions performed to pools of machines, Fly-Replay is but one tool available to you. There is also a Machines API that will enable you to orchestrate whatever topology you can come up with. Cost-Effective Queue Workers With Fly.io Machines gives a preview of what that would look like with Laravel."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/new-launch/",
    "content": "Fly.io is the new public cloud for running your applications near your users so it can be faster than ever. When you create a new application, you use the fly launch command to give the platform all the information it needs to send it out into the sky. We‚Äôve made steps towards making launching a new app even easier because first impressions matter. Try the new fly launch now; you can have an app up and running in mere minutes.\n\nPreviously when you ran fly launch, you got asked a bunch of hopefully relevant questions to help you get your app up and running. We‚Äôve taken a lot of the guesswork out of the process and made it a lot more streamlined. It turns out that even though Fly.io developers use a variety of frameworks, languages, and toolchains you can fold most of them into a few basic infrastructure shapes.\n\n\n## The new launch\n\nNow when you run fly launch, the CLI will infer what you want based on the source code of your application. For example, if you have a Rails app with SQLite, it‚Äôll give you an opinionated set of defaults that you can build from. If you don‚Äôt, it‚Äôll give you other options so you can craft the infrastructure you need. I took one of my older applications named douglas-adams-quotes and launched it with the new flow. Here‚Äôs what it looks like:\n\nIf the settings it guessed are good enough, you can launch it into the cloud. If not, then you‚Äôll be taken to a webpage where you can confirm or change the settings it guessed.\n\nOnce you say yes or confirm on the web, your app will get built and deployed (unless you asked it not to with --no-deploy). You‚Äôll get a link to your app so you can go check it out. It‚Äôs that easy.\n\n\n## Conclusion\n\nWe hope that this can help you look before you fly launch into the wild unknowns of the cloud.\n\nGot any ideas or comments on how we can make this even smoother? Get in touch on our community forum. We‚Äôd love to hear from you."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-i-fly/",
    "content": "We are Fly.io. We make it easy to run your programs close to your users. We make it easy to update your programs whenever you need to and communicate between your services in an end-to-end encrypted fashion. Today, Xe is going to tell you what they do to use Fly.io effectively. Deploy your first app for free and scale it up to production. That‚Äôs what Xe did.\n\nI‚Äôm Xe Iaso. I‚Äôm a writer, technical educator, and philosopher who focuses on making technology easy to understand and scale to your needs. I use Fly.io to host my website and in nearly all of my personal projects now. Fly.io allows me to experiment with new ideas quickly and then deploy them to the world with ease.\n\n\n## What is Fly.io?\n\nFly.io lets you host your applications in data centers close to your users. Fly.io also lets you have rolling updates of your programs and facilitates easy communication between your services inside and outside of your organization‚Äôs private network.\n\nI use Fly.io to host my blog, its CDN (named XeDN for reasons which are an exercise for the reader), and a bunch of other supporting services that help make it run. It is easily the most fun I‚Äôve had deploying things since I worked at Heroku.\n\n\n## My blog\n\nMy blog is made up of several parts: the backend blog server and the CDN. Both are written in Go, my favorite programming language. The back-end blog server runs in Toronto, but XeDN runs in 35 datacenters worldwide. I plan to eventually move my blog to be served from XeDN, but for right now it‚Äôs still comfortably running off of a single server in Toronto.\n\nOverall, my website‚Äôs architecture looks like this. My website listens for updates from Patreon and GitHub to trigger rebuilds because of its dystatic nature. When I am working on new posts or building new assets, I upload them to Backblaze B2. Anytime someone tries to access one of the files on a XeDN node, it will download it from Backblaze B2 if it doesn‚Äôt have it locally already.\n\nWith Fly.io, I don‚Äôt have to worry about the user experience being degraded when servers go down. If any individual XeDN server goes down, I can rely on the other XeDN servers worldwide to pick up the slack thanks to the fact that Fly.io will shunt the traffic to the servers that aren‚Äôt down. Combine this with some very aggressive caching logic for things like video assets, I can make sure that my blog is fast for everyone, no matter where they are in the world.\n\nOf course, it doesn‚Äôt end here. My CDN server is the back end that helps make my other projects work too. I spent some time working on a custom font for all of my web properties, and I serve it from my CDN so that I can use it in every project of mine. This allows me to integrate it into other projects like Ars√®ne without having to do anything special.\n\n\n## Building on top of projects with Fly.io\n\nI like making projects that aren‚Äôt entirely serious. I love using these projects to explore aspects and bits of technology that I would have never gotten to play with before. One of these is Ars√®ne, a project I used to explore what a ‚Äúdead internet‚Äù powered by AI could look like.\n\nEvery 12 hours, Ars√®ne will have the ChatGPT API generate new posts and then use Stable Diffusion to create a (hopefully relevant) illustration for that post. I run a copy of the Automatic1111 Stable Diffusion API in my private network. When Ars√®ne generates an image, it reaches out to that Stable Diffusion API directly over that private network to make the calls it needs. Since XeDN is in the same private network, I can also have Ars√®ne send the images there to be cached and served all over the world.\n\nHere‚Äôs what the total flow looks like:\n\nThis means that when I am creating things, I am not just making one-off things that don‚Äôt work with each other. I am creating individual building blocks that interoperate with each other. I am creating opportunities for me to reuse my infrastructure to create brand new things that are robust and scalable with minimal effort on my end.\n\n\n## My other projects\n\nI have some other projects that I‚Äôm working on that I don‚Äôt want to get into too much detail about yet, but it‚Äôs going to mostly involve transforming the basic ideas of using my CDN for distributing things and a webserver for sending HTML to users in new and interesting ways. I love using Fly.io for this because I am just allowed to create things instead of having to worry about how to implement it, where state is going to be stored, or how I‚Äôm going to scale it.\n\nFly.io is the only platform where I‚Äôve used where I can spin up 35 copies of a program as easily as one copy of a program.\n\n\n## Conclusion\n\nIf you haven‚Äôt given Fly.io a try yet, you‚Äôre really missing out. It is utterly trivial to deploy your application across the globe. Not to mention, when your applications are idle, you can have them scale down to zero copies. This means that you only pay for what you actually use. I don‚Äôt have to worry about overpaying for my blog by having a giant server in Helsinki running 24/7, even though I‚Äôm only using a small sliver of it.\n\nIf you want to learn more about Fly.io, you can check out fly.io. My CDN cost me nothing until I started adding cover art per post and the conversation snippets with furry stickers. It definitely went over the bar when I started uploading video. I can see it scaling in the future as my demands scale too.\n\nOf course, this is barely even scratching the surface. Stay tuned for secret tricks you can use to dynamically spin up and spin down machines as you need. Imagine uploading an image, automatically creating a machine to handle compressing it, and uploading it to your storage back end. Imagine what you could do if compute was a faucet that you could turn on and off as you needed it.\n\nYou can do it on Fly.io. Try it today, you can run an app on a 256 MB Machine for free. XeDN ran on three 256 MB Machines for a year. Ars√®ne still runs on a 256 MB Machine to this day. It‚Äôs more than enough for what you‚Äôre going to do. And when it isn‚Äôt, scaling up is cheaper than you can imagine."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/transcribing-on-fly-gpu-machines/",
    "content": "Fly.io has GPUs! If you want to run AI (or whatever) workloads, checkout how to get started with GPU Machines!\n\nFly.io has GPU Machines, which means we can finally play games mine bitcoin baghold NFTs run AI workloads with just a few API calls.\n\nThis is exciting! Running GPU workloads yourself is useful when the community‚Ñ¢ builds upon available models to make them faster, more useful, or less restrictive than first-party APIs.\n\nOne such tool is the Whisper Webservice, which is conveniently packaged in a way that makes it a good candidate to use on Fly GPU Machines.\n\nLet‚Äôs see how to use Fly.io GPU by spinning up Whisper Webservice.\n\n\n## Whisper Webservice\n\nWhisper is OpenAI‚Äôs voice recognition service - it‚Äôs used for audio transcription. To use it anywhere that‚Äôs not OpenAI‚Äôs platform, you need some Python, a few GB of storage, and (preferably) a GPU.\n\nThe aforementioned Whisper Webservice packages this up for us, while making Whisper faster, more useful, and less restricted than OpenAI‚Äôs API:\n\nLuckily for us, and totally not why I chose this as an example - the project provides GPU-friendly Docker images. We‚Äôll use those to spin up Fly GPU Machines and process some audio files.\n\n(I‚Äôll also show examples of making your own Docker image!)\n\n\n## Running a GPU Machine\n\nSpinning up a GPU Machine is very similar to any other Machine. The main difference is the new ‚ÄúGPU kind‚Äù option (--vm-gpu-kind), which takes 2 possible values:\n\nThese are 2 flavors of Nvidia A100 GPUs, the difference worth caring about is 40 vs 80 GB of memory (here‚Äôs pricing).\n\nWe‚Äôll create machines using a100-pcie-40gb because we don‚Äôt need 80 freakin‚Äô GB for what we‚Äôre doing.\n\nUsing flyctl is a great way to run a GPU Machine. We‚Äôll make an app and run the conveniently created Whisper Webservice Docker image that supports Nvidia GPUs. The flyctl commands will default us into a performance-8x server size (8 CPUs, 16G ram) unless we specify something different.\n\nOne caveat: AI model files are big. Docker images ideally aren‚Äôt big - sending huge layers across the network angers the spiteful networking gods. If you shove models into your Docker images, you might have a bad time.\n\nWe suggest creating a Fly Volume and making your Docker image download needed models when it first spins up. The Whisper service (and in my experience, OpenAI‚Äôs Python library) does that for us.\n\nSo, we‚Äôll create a volume to house (and cache) the models. In the case of the Whisper project, the models get placed in /root/.cache/whisper on its first boot, and so we‚Äôll mount our disk there.\n\nAlright, let‚Äôs create a GPU Machine. Here‚Äôs what the process looks like:\n\nThat‚Äôs all pretty standard for Fly Machines, except for the --vm-gpu-kind flags used both for volume and Machine creation. Volumes are pinned to specific hosts - using this flag tells Fly.io to create the volume on a GPU host. Assuming we set the same region (-r ord), creating a GPU Machine with the just-created volume will tell Fly.io to place the Machine on the same host as the volume.\n\nNote: As my machine started up, I saw a log line WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available., which ended up being an issue of timing. Once everything is running, I was able to see things were working by using fly ssh console -a $APP_NAME and running command nvidia-smi to confirm that the VM had a GPU. It also listed the running web service (Python in this case) was running as a GPU process.\n\nOnce everything is running, you should be able to head to $APP_NAME.fly.dev and view it in the browser.\n\nThe Whisper Webservice UI will let you try out individual calls in its API. This will also give you the information you need to make those calls from your code. There‚Äôs a link to the API specification (e.g. $APP_NAME.fly.dev/openapi.json) you can use to, say, have ChatGPT generate a client in your language of choice.\n\n\n## Automating GPU Machines\n\nIf you want to automate this, you can use the Machines API (spec here).\n\nAn easy way to get started is to spy on the API requests flyctl is making:\n\nThis helped me figure out why my own initial API attempts failed - it turns out we need some extra parameters in the compute portion of the request JSON for creating a volume, and the guest section for creating a Machine.\n\nFor both volumes and Machines, we set the gpu_kind the same way we did in our flyctl command. However we also need the cpu_kind to be set. Additionally, when creating a Machine, we need to set cpus and memory_mb to valid values for performance Machines.\n\nAfter that we can assign the app some IPs. You can use flyctl for this, or the graphql API. You can once again use debug mode with flyctl to see what API calls it makes. Side note: Eventually the Machines REST API will include the ability to allocate IP addresses.\n\nIf you‚Äôre doing this type of work for your business, you may want to keep these Machines inside a private network anyway, in which case you won‚Äôt be assigning it IP addresses.\n\n\n## Making Your Own Images\n\nThere is, luckily (for me, a hardware ignoramus) less dark magic to making GPU-friendly Docker images than you might think. Basically you need to just install the correct Nvidia drivers.\n\nA way to cheat at this is to run Nvidia cuda base images, but you‚Äôre made of sterner stuff, you can also start with a base Ubuntu image and install your own.\n\nWhile the Whisper webservice image is based on nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04, I got Whisper (plain, not the webservice) working with ubuntu:22.04:\n\nYou can find a full, working version of this here.\n\n\n## This time it‚Äôs different, I guess\n\nAI feels a bit different than previous trends in that it has immediately-obvious benefits. No one needs to throw around catchy phrases with a wink-wink nudge-nudge (‚Äúwe like the art‚Äù) for us to find value.\n\nSince AI workloads work most efficiently in GPUs, they remain a hot commodity. For those of us who didn‚Äôt purchase enough $NVDA to retire, we can bring more value to our businesses by adding in AI.\n\nFly Machines have always been a great little piece of tech to run ‚Äúephemeral compute workloads‚Äù (wait, do I work at AWS!?) - and this is what I like about GPU Machines. You can mix and match all sorts of AI stuff together to make a chain of useful tools!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/skip-the-api/",
    "content": "With Fly.io, you can get your app running globally in a matter of minutes, and with LiteFS, you can run SQLite alongside your app! Now we‚Äôre introducing LiteFS Cloud: managed backups and point-in-time restores for LiteFS. Try it out for yourself!\n\nMy favorite part about building tools is discovering their unintended uses. It‚Äôs like starting to write a murder mystery book but you have no idea who the killer is!\n\nHistory is filled with examples of these accidental discoveries: WD-40 was originally used to protect ICBMs from rust and now it fixes your squeaky doorknob. Bubble wrap was originally sold as wallpaper and now it protects your Amazon packages.\n\nWhen we started writing LiteFS, a distributed SQLite database, we thought it would be used to distribute data geographically so users in, say, Bucharest see response times as fast as users in San Jose. And for the most part, that‚Äôs what LiteFS users are doing.\n\nBut we discovered another unexpected use: replacing the API layer between services with SQLite databases.\n\n\n## How it started\n\nIn the early days of LiteFS development, we wanted to find a real-world test bed for our tool so we could smoke out any bugs that we didn‚Äôt find during automated tests. Part of our existing infrastructure is a program called Corrosion that gossips state between all our servers. Corrosion tracks VM statuses, health checks, and a plethora of other information for each server and communicates this info with other servers so they can make intelligent decisions about request routing and VM placement. Corrosion keeps a fast, local copy of all this data in a SQLite database.\n\nSo we set up a Corrosion instance that also ran on top of LiteFS. This helped root out some bugs but we also found another use for it: making Corrosion accessible to our internal services.\n\n\n## Shipping the kitchen sink\n\nThe typical approach to making data available between services is to spend weeks designing an API and then building a service around it. Your API design needs to take into account the different use cases of each consuming service so that it can deliver the data it needs efficiently. You don‚Äôt want your clients making a dozen API calls for every request!\n\nA different approach is to skip the API design entirely and just ship the entire database to your client. You don‚Äôt need to consider the consuming service‚Äôs access patterns as they can use vanilla SQL to query and join whatever data their heart desires. That‚Äôs what we did using LiteFS.\n\nWhile we could have set up each downstream service as a Corrosion node, gossip protocols can be chatty and we really just needed a one-way stream of updates. Setting up a read-only LiteFS instance for a new service is simple‚Äîit just needs the hostname of the upstream primary node to connect to:\n\nAnd voila! You have a full, read-only copy of the database on your app.\n\n\n## Moving compute to the client\n\nAPI design is notoriously difficult as it‚Äôs hard to know what your consuming services will need. Query languages such as GraphQL have even been invented for this specific problem!\n\nHowever, GraphQL has its own limitations. It‚Äôs good for fetching raw data but it lacks built-in aggregation \u0026 advanced querying capabilities like windowing. GraphQL is typically layered on top of an existing relational database that uses SQL. So why not just use SQL?\n\nAdditionally, performing queries on your service means that you need to handle multiple tenants competing for compute resources. Managing these tenants involves rate limiting and query timeouts so that no one client consumes all the resources.\n\nBy pushing a read-only copy of the database to clients, these restrictions aren‚Äôt a concern anymore. A tenant can use 100% of its CPU for hours if it wants to. It won‚Äôt adversely affect any other tenant because the query is running on its own hardware.\n\n\n## So what‚Äôs the downside?\n\nThere‚Äôs always trade-offs with any technology and shipping read-only replicas is no different. One obvious limitation of read-only replicas is that they‚Äôre read-only. If your clients need to update data, they‚Äôll still need an API for those mutations.\n\nA less obvious downside is that the contract for a database can be less strict than an API. One benefit to an API layer is that you can change the underlying database structure but still massage data to look the same to clients. When you‚Äôre shipping the raw database, that becomes more difficult. Fortunately, many database changes, such as adding columns to a table, are backwards compatible so clients don‚Äôt need to change their code. Database views are also a great way to reshape data so it stays consistent‚Äîeven when the underlying tables change.\n\nFinally, shipping a database limits your ability to restrict access to data. If you have a multi-tenant database, you can‚Äôt ship that database without the client seeing all the data. One workaround for this is to use a database per tenant. SQLite databases are lightweight since they are just files on disk. This also has the added benefit of preventing queries in your application from accidentally fetching data across tenants.\n\n\n## Where do we take this next?\n\nWhile this approach has worked well for some internal tooling, how does this look in the broader world of software? APIs are likely stick around for the foreseeable future so providing read-only database replicas make sense for specific use cases where those APIs aren‚Äôt a great fit.\n\nImagine being able to query all your Stripe data or your GitHub data from a local database. You could join that data on to your own dataset and perform fast queries on your own hardware.\n\nWhile companies such as Stripe or GitHub likely colocate their tenant data into one database, many companies run an event bus using tools like Kafka which could allow them to generate per-tenant SQLite databases to then stream to customers.\n\nPushing queries out to the end user has huge benefits for both the data provider \u0026 the data consumer in terms of flexibility and power."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/sentry-partnership/",
    "content": "We‚Äôre Fly.io. We put your code into lightweight microVMs on our own hardware around the world, close to your users. We partnered with Sentry to bring error and performance monitoring to your apps. Deploy your first app, and automatically get a year‚Äôs worth of credits to Sentry‚Äôs Team Plan credits. Check us out‚Äîyour app can be deployed and instrumented in minutes.\n\nWe‚Äôve been using Sentry since the dawn of the internet. Or at least as far back as the discovery of the Higgs boson. Project to project, the familiar Sentry issue detail screen has been our faithful debugging companion.\n\nToday it‚Äôs no exception: All of our Golang, Elixir, Ruby and Rust services report dutifully to Sentry.\n\nSo, it felt natural to integrate Sentry as the default error monitoring tool. All new deployments on Fly.io get a Sentry project provisioned automatically. Existing apps can grab theirs with flyctl ext sentry create.\n\nEach Fly.io organization receives, for one year, a generous monthly quota:\n\nOnce your app is instrumented, you‚Äôll automatically get notified of production errors, latency issues, and crashes as soon as they occur in production. Sentry‚Äôs Team plan also gives you access to over 40 integrations, unlimited seats, and custom alerting.\n\n\n## Auto-instrumenting Rails\n\nTo see Sentry in action, let‚Äôs launch our Boomer Rails App. Yes kids, Rails is old school, and it‚Äôs the easiest framework to auto-instrument.\n\nWhen flyctl launch detects a Rails app, it‚Äôs automatically setup to use a freshly minted Sentry project. Gems are installed, initializers planted, and finally, the SENTRY_DSN secret is set for deployment. We redacted some output for brevity.\n\nNow, having Sentry configured at launch time means that deployment errors are captured early. This is useful for situations where apps fail to boot, run out of memory, and so on.\n\nNow let‚Äôs force an application exception. We visit the app root, which goes Boom, thanks to some hastily written Ruby code.\n\nOh shucks. Something went wrong. But, I got an email about this error.\n\nWe could click ‚ÄúView on Sentry‚Äù. Instead, let‚Äôs use flyctl to send us to the Sentry issues dashboard.\n\nWe click through to this specific issue.\n\nWe successfully debugged our issue. The takeaway: don‚Äôt raise when you can call.\n\nError tracking on Sentry is just scratching the surface. Check out their performance monitoring, session replay, alerting and much more.\n\n\n## Next Steps for Fly.io and Sentry\n\nFor our next trick, we‚Äôll be tracking Fly.io releases in Sentry, so Sentry can link issues to their release tracking feature. We‚Äôll also send events like out-of-memory errors to Sentry. The possibilities are endless.\n\nGot ideas or comments? Get in touch on our community forum."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/tracking-consistency-with-litefs/",
    "content": "With Fly.io, you can get your app running globally in a matter of minutes, and with LiteFS, you can run SQLite alongside your app! Now we‚Äôre introducing LiteFS Cloud: managed backups and point-in-time restores for LiteFS. Try it out for yourself!\n\nWhen we started the LiteFS project a year ago, we started more with an ideal in mind rather than a specific implementation. We wanted to make it possible to not only run distributed SQLite but we also wanted to make it‚Ä¶ gasp‚Ä¶ easy!\n\nThere were hurdles that we expected to be hard, such as intercepting SQLite transaction boundaries via syscalls or shipping logs around the world while ensuring data integrity. But there was one hurdle that was unexpectedly hard: maintaining a consistent view from the application‚Äôs perspective.\n\nLiteFS requires write transactions to only be performed at the primary node and then those transactions are shipped back to replicas instantaneously. Well, almost instantaneously. And therein lies the crux of our problem.\n\nLet‚Äôs say your user sends a write request to write to the primary node in Madrid and the user‚Äôs next read request goes to a local read-only replica in Rio de Janeiro. Most of the time LiteFS completes replication quickly and everything is fine. But if your request arrives a few milliseconds before data is replicated, then your user sees the database state from before the write occurred. That‚Äôs no good.\n\nHow exactly do we handle that when our database lives outside the user‚Äôs application?\n\n\n## Our initial series of failures, or how we tried to teach distributed systems to users\n\nOur first plan was to let LiteFS users manage consistency themselves. Every application may have different needs and, honestly, we didn‚Äôt have a better plan at the time. However, once we started explaining how to track replication state, it became obvious that it was going to be an untenable approach. Let‚Äôs start with a primer and you‚Äôll understand why.\n\nEvery node in LiteFS maintains a replication position for each database which consists of two values:\n\nYou can read the current position from your LiteFS mount from the -pos file:\n\nThis example shows that we are at TXID 0x42478b (or 4,343,691 in decimal) and the checksum of our whole database after the transaction is 8b73bc1d07d84988. A replica can detect how far it‚Äôs lagging behind by comparing its position to the primary‚Äôs position. Typically, a monotonic transaction ID doesn‚Äôt work in asynchronous replication systems like LiteFS but when we couple it with a checksum it allows us to check for divergence so the pair works surprisingly well.\n\nLiteFS handles the replication position internally, however, it would be up to the application to check it to ensure that its clients saw a consistent view. This meant that the application would have needed to have its clients track the TXID from their last write to the primary and then the application would have to wait until its local replication caught up to that position before it could serve the request.\n\nThat would have been a lot to manage. While you may find the nuts and bolts of replication interesting, sometimes you just want to get your app up and running!\n\n\n## Let‚Äôs use a library! Er, libraries.\n\nTeaching distributed systems to each and every LiteFS user was not going to work. So instead, we thought we could tuck that complexity away by providing a LiteFS client library. Just import a package and you‚Äôre done!\n\nLibraries are a great way to abstract away the tough parts of a system. For example, nobody wants to roll their own cryptography implementation so they use a library. But LiteFS is a database so it needs to work across all languages which means we needed to implement a library for each language.\n\nActually, it‚Äôs worse than that. We need to act as a traffic cop to redirect incoming client requests to make sure they arrive at the primary node for writes or that they see a consistent view on a replica for reads. We aren‚Äôt able to redirect writes at the data layer so it‚Äôs typically handled at the HTTP layer. Within each language ecosystem there can be a variety of web server implementations: Ruby has Rails \u0026 Sinatra, Go has net/http, gin, fasthttp, and whatever 12 new routers came out this week.\n\n\n## Moving up the abstraction stack\n\nAbstraction often feels like a footgun. Generalizing functionality across multiple situations means that you lose flexibility in specific situations. Sometimes that means you shouldn‚Äôt abstract but sometimes you just haven‚Äôt found the right abstraction layer yet.\n\nFor better or for worse, HTTP \u0026 REST-like applications have become the norm in our industry and some of the conventions provide a great layer for LiteFS to build upon. Specifically, the convention of using GET requests for reading data and the other methods (POST, PUT, DELETE, etc) for writing data.\n\nInstead of developers injecting a LiteFS library into their application, we built a thin HTTP proxy that lives in front of the application.\n\nThis approach has let us manage both the incoming client side via HTTP as well as the backend data plane via our FUSE mount. It lets us isolate the application developer from the low-level details of LiteFS replication while making it feel like they‚Äôre developing against vanilla SQLite.\n\n\n## How it works\n\nThe LiteFS proxy design is simple but effective. As an example, let‚Äôs start with a write request. A user creates a new order so they send a POST /orders request to your web app. The LiteFS proxy intercepts the request \u0026 parses the HTTP headers to see that it‚Äôs a POST write request. If the local node is a replica, the proxy forwards the request to the primary node.\n\nIf the local node is the primary, it‚Äôll pass the request through to the application‚Äôs web server and the request will be processed normally. When the response begins streaming out to the client, the proxy will attach a cookie with the TXID of the newly-written commit.\n\nWhen the client then sends a GET read request, the LiteFS proxy again intercepts it and parses the headers. It can see the TXID that was set in the cookie on the previous write and the proxy will check it against the replication position of the local replica. If replication has caught up to the client‚Äôs last write transaction, it‚Äôll pass through the request to the application. Otherwise, it‚Äôll wait for the local node to catch up or it will eventually time out. The proxy is built into the litefs binary so communication with the internal replication state is wicked fast.\n\n\n## Preventing laggards\n\nThe proxy provides another benefit: health checks. Networks and servers don‚Äôt always play nice when they‚Äôre communicating across the world and sometimes they get disconnected. The proxy hooks into the LiteFS built-in heartbeat system to detect lag and it can report the node as unhealthy via a health check URL when this lag exceeds a threshold.\n\nIf you‚Äôre running on Fly.io, we‚Äôll take that node out of rotation when health checks begin reporting issues so users will automatically get routed to a different, healthy replica. When the replica reconnects to the primary, the health check will report as healthy and the node will rejoin.\n\n\n## The Tradeoffs‚Ä¶ there‚Äôs always tradeoffs!\n\nDespite how well the LiteFS proxy works in most situations, there‚Äôs gonna be times when it doesn‚Äôt quite fit. For example, if your application cannot rely on cookies to track application state then the proxy won‚Äôt work for you.\n\nThere are also frameworks, like Phoenix, which can rely heavily on websockets for live updates so this circumvents your traditional HTTP request/response approach that LiteFS proxy depends on. Finally, the proxy provides read-your-writes guarantees which may not work for every application out there.\n\nIn these cases, let us know how we can improve the proxy to make it work for more use cases! We‚Äôd love to hear your thoughts.\n\n\n## Diving in further\n\nThe LiteFS proxy makes it easy to run SQLite applications in multiple regions around the world. You can even run many legacy applications with little to no change in the code.\n\nIf you‚Äôre interested in setting up LiteFS, check out our Getting Started guide. You can find additional details about configuring the proxy on our Built-in HTTP Proxy docs page."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/redundant-logs/",
    "content": "You‚Äôve done everything right. You are well aware of Murphy‚Äôs Law. You have multiple redundant machines. You‚Äôve set up a regular back up schedule for your database, perhaps even are using LiteFS CLoud. You ship your logs to LogTail or perhaps some other provider so you can do forensic analysis should anything go wrong‚Ä¶\n\nThen the unexpected happens. A major network outage causes your application to misbehave. What‚Äôs worse is that your logs are missing crucial data from this point, perhaps because of the same network outage. Maybe this time you are lucky and you can find the data you need by using copies of your logs via flyctl logs or the monitoring tab on the flyctl dashboard before they disappear forever.\n\nSo, what is going on here? Let‚Äôs look at the steps. Your application writes logs to STDOUT. Fly.io will take that output and send it to NATS. The Log Shipper will take that data and hand it to Vector. From there it is shipped to your third party logging provider. That‚Äôs a lot of moving parts.\n\nAll that is great, but just like how you have redundant machines in case of failures, you may want to have redundant logs in addition to the ones fly.io and the log shipper provide. Below are two strategies for doing just that. You can use either or both, and best of all the logs you create will be in addition to your existing logs.\n\n\n## Logging to multiple places\n\nThe following approach is likely the most failsafe, but often the least convenient: having your primary application on each machine write to a separate log file in addition to standard out. This does mean that when you need this data you will have to fetch it from each machine and it likely with be rather raw. But at least it will be there even in the face of network failures.\n\nFor best results put these logs on a volume so that it survives a restart, and be prepared to rotate logs as they grow in size so that they don‚Äôt eventually fill up that volume.\n\nThis approach is necessarily framework specific, but most frameworks provides some ability to do this. A Rails example:\n\nYou probably already have the first two lines already in your config/environments/production.rb file. Adjust and add the last two lines. That‚Äôs it! You now have redundant logs.\n\nSee the Ruby docs for Logger documentation on how to handle log rotation.\n\nSome pointers for other frameworks:\n\n\n## Custom log shipper\n\nThis approach is less bullet proof but may result in more immediately usable results. Instead of using Log Shipper, Vector, and a third party, it is easy to subscribe directly to NATS and process log entries yourself.\n\nWhat you are going to want is a separate app running on a separate machine so that it doesn‚Äôt go down there are problems with the machine you are monitoring, or even during the times when you are deploying a new version. If the code you write will be writing to disk, you will want a volume.\n\nAlso like with log shipper, you will want to set the following secret:\n\nHere‚Äôs a minimal JavaScript example that can be run using Node or Bun:\n\nThe above is pretty straightforward. It connects to NAT, opens a file, subscribes to logs, parses each message, and writes out selected data to disk. This example is in JavaScript, but feel free to reimplement this basic approach using your favorite language, as NATS supports plenty.\n\nThings to watch out for: you don‚Äôt want recursive errors when exceptions occur during write. You want to capture errors and reconnect to NATS when the connection goes down. You may even want to filter messages. A more complete example implementing a number of these features can be found here.\n\n\n## Conclusion\n\nLog failures are not common, and perhaps the redundant logs that fly.io already keeps will be sufficient for your needs. But it may be worth reviewing what your exposure is and how to mitigate that exposure should your logs fail at the worst possible time.\n\nHopefully the approaches listed above give you ideas on how to ensure that you will always have the log data you need even in the most hostile environment conditions."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/tokenized-tokens/",
    "content": "We‚Äôre Fly.io. We run apps for our users on hardware we host around the world. Building security for a platform like this is tricky, and that‚Äôs what the post is about. But you don‚Äôt have to read any of this to get an app running on here. See how to speedrun getting an app running on Fly.io here.\n\nWe built some little security thingies. We‚Äôre open sourcing them, and hoping you like them as much as we do. In a nutshell: it‚Äôs a proxy that injects secrets into arbitrary 3rd-party API calls. We could describe it more completely here, but that wouldn‚Äôt be as fun as writing a big long essay about how the thingies came to be, so: buckle up.\n\nThe problem we confront is as old as Rails itself. Our application started simple: some controllers, some models. The only secrets it stored were bcrypt password hashes. But not unlike a pet baby alligator, it grew up. Now it‚Äôs become more unruly than we‚Äôd planned.\n\nThat‚Äôs because frameworks like Rails make it easy to collect secrets: you just create another model for them, roll some kind of secret to encrypt them, jam that secret into the deployment environment, and call it a day.\n\nAnd, at least in less sensitive applications, or even the early days of an app like ours, that can work!\n\nFor what it‚Äôs worth, and to the annoyance of some of our Heroku refugees, we‚Äôve never stored customer app secrets this way; our Rails API can write customer secrets, but has never been able to read them. We‚Äôll talk more about how this works in a sec.\n\nBut for us, not anymore. At the stage we‚Äôre at, all secrets are hazmat. And Rails itself is the portion of our attack surface we‚Äôre least confident about ‚Äì¬†the rest of it is either outside of our trust boundaries, or written in Rust and Go, strongly-typed memory-safe languages that are easy to reason about, and which have never accidentally treated YAML as an executable file format.\n\nSo, a few months back, during an integration with a 3rd party API that relied on OAuth2 tokens, we drew a line: ‚ö° henceforth, hazmat shall only be removed from Rails, never added ‚ö°. This is easier said than done, though: despite prominent ‚Äúthis is not a place of honor‚Äù signs all over the codebase, our Rails API is still where much of the action in our system takes place.\n\n\n## How Apps Use Secrets: 3 Different Approaches\n\nWe just gave you one way, probably the most common. Stick ‚Äòem in a model, encrypt them with an environment secret, and watch Dependabot religiously for vulnerabilities in transitively-added libraries you‚Äôve never heard of before.\n\nHere‚Äôs a second way, probably the second-most popular: use a secrets management system, like KMS or Vault. These systems, which are great, keep secrets encrypted and allow access based on an intricate access control language, which is great.\n\nThat‚Äôs what we do for customer app secrets, like DATABASE_URL and API_KEY. We use HashiCorp Vault (for the time being). Our Rails API has an access token for Vault that allows it to set secrets, but not read any of them back, like a kind of diode. A game-over Rails vulnerability might allow an attacker to scramble secrets, but not to easily dump them.\n\nIn the happiest cases with secrets, systems like Vault can keep secret bits from ever touching the application. Customer app secrets are a happy case: Rails never needs to read them, just our orchestrator, to inject them into VM environments. In other happy cases, Vault operates on the app‚Äôs behalf: signing a time-limited request URL for AWS, or making a direct request to a known 3rd-party service. Vault calls these features ‚Äúsecret engines‚Äù, and when you can get away with using them, it‚Äôs hard to do better.\n\nThe catch is, sometimes you can‚Äôt get away with them. For most 3rd parties, Vault has no idea how to interact with them. And most secrets are bearer tokens, not request signatures. The only way to use those kinds of secrets is to read them into app memory. If good code can read a secret from Vault, so can a YAML vulnerability.\n\nStill: this is better than nothing: even if apps can read raw secrets, systems like Vault can provide an audit trail of which secrets were pulled when, and make it much easier to rotate secrets, which you‚Äôll want to do with raw secrets to contain their blast radius. HashiCorp Vault is great, so is KMS, we recommend them unreservedly.\n\nSo that‚Äôs why there‚Äôs a third way to handle this problem, which is: decompose your application into services so that the parts that have to handle secrets are tiny and well-contained. The bulk of our domain-specific business code can chug along in Rails, and the parts that trade bearer tokens with 3rd parties can be built in a couple hundred lines of Go.\n\nThis is a good approach, too. It‚Äôs just cumbersome, because a big application ends up dealing with lots of different kinds of secrets, making a trusted microservice for each of them is a drag. What you want is to notice some commonality in how 3rd party API secrets are used, and to come up with some possible way of exploiting that.\n\nWe thought long and hard on this and came up with:\n\n\n## Tokenizer: The Fabled 4th Way\n\nWe developed a multipurpose secret-using service called the Tokenizer.\n\nTokenizer is a stateless HTTP proxy that holds the private key of a Curve25519 keypair.\n\nWhen we get a new 3rd party API secret, we encrypt it to Tokenizer's public key; we ‚Äútokenize‚Äù it. Our API server can handle the (encrypted) tokenized secret, but it can‚Äôt read or use it directly. Only Tokenizer can.\n\nWhen it comes time to talk to the 3rd party API, Rails does so via Tokenizer. Here‚Äôs how that works:\n\nYou can think of Tokenizer as a sort of Vault-style ‚Äúsecret engine‚Äù that happens to capture virtually everything an app needs secrets for. It can even use decrypted secrets to selectively HMAC parts of requests, for APIs that authenticate with signatures instead of bearer tokens.\n\nCheck it out: it‚Äôs not super complicated.\n\nNow, our goal is to keep Rails from ever touching secret bits. But, hold on: a game-over Rails vulnerability would give attackers an easy way around Tokenizer: you‚Äôd just proxy requests for a particular secret to a service you ran that collected the plaintext.\n\nTo mitigate that, we built the obvious feature: you can lock requests for specific secrets down to a list of allowed hosts or host regexp patterns.\n\nWe think this approach to handling secrets is pretty similar to how payment processors tokenize payment card information, hence the name. The advantages are straightforward:\n\n\n## SSOkenizer: Tokenizing OAuth SSO\n\nWhen we created Tokenizer, we were motivated by the problem of OAuth2 tokens other services providers gave us, for partnership features we build for mutual customers.\n\nWe‚Äôd also dearly like our customers to use OAuth2/OIDC to log into Fly.io itself; it‚Äôs more secure for them, and gives them the full complement of Google MFA features, meaning we don‚Äôt immediately have to implement the full complement of Google MFA features. Letting people log into Fly.io with a Google OAuth token means we have to keep track of people‚Äôs OAuth tokens. That sounds like a job for the Tokenizer!\n\nBut there‚Äôs a catch: acquiring those OAuth tokens in the first place means doing the OAuth2 dance, which means that for a brief window of time, Rails is handling hazmat. We‚Äôd like to close that window.\n\nEnter the SSOkenizer.\n\nThe job of the SSOkenizer is to perform the OAuth2 dance on behalf of Rails, and then use the output of that process (the OAuth2 bearer token yielded from the OAuth2 code flow, which you can see in its cursed majesty here) to drive the Tokenizer.\n\nIn other words, where we‚Äôd otherwise explicitly encrypt secrets to be tokenized a-priori, the SSOkenizer does that on the fly, passing tokenized OAuth2 credentials back to Rails. Those‚Ä¶ tokenized tokens can only be used through the Tokenizer proxy, which is the only component in our system with the private key that unseals them.\n\nWe think this is a pretty neat trick. The SSOkenizer itself is tiny, even smaller than the Tokenizer (you can read it here), and essentially stateless; in fact, pretty much everything in this system is minimally stateful, except Rails, which is great at being stateful. We even keep almost all of OAuth2 out of Rails and confined to Go code (where it‚Äôs practically the hello-world of Go OAuth2 libraries).\n\nA nice side effect-slash-validation of this design: once we got it working for Google, it became a super easy project to get OAuth2 logins working for other providers.\n\n\n## Feel Free To Poach This\n\nWe‚Äôre psyched for a bunch of reasons:\n\nThese are standalone tools with no real dependencies on Fly.io, so they‚Äôre easy for us to open source. Which is what we did: if they sound useful to you, check out the¬†tokenizer¬†and¬†ssokenizer¬†repositories for instructions on deploying and using these services yourself."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/flydotio-heart-bun/",
    "content": "Bun 1.0 comes out September 7th. Fly.io is making preparations.\n\nPreviously, we stated that Fly.io ‚ù§Ô∏è JS, and we understandably started with Node.js. While that work is ongoing, it makes sense to start expanding to other runtimes.\n\nBun is the obvious next choice given it aims for complete Node.js API compatibility.\n\nStarting with flyctl version 0.1.54 and @flydotio/dockerfile version 0.3.3, you can launch and deploy bun applications using fly launch and fly deploy, provided:\n\nBasically, if you can run Bun‚Äôs Quickstart and Fly‚Äôs hands-on walk-through, you have all you need to deploy your application on fly.io.\n\nWe also have a sample that you can deploy.\n\nBe forewarned that everything is beta at this point. Some issues we encountered while preparing this support:\n\nUndoubtedly there will be bugs in fly‚Äôs dockerfile generator too. But as Node.js and Bun share the same generator, fixes that are made for either framework will generally benefit both.\n\nIf you see a problem, start a discussion, open an issue, or create a pull request."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/litefs-cloud/",
    "content": "With Fly.io, you can get your app running globally in a matter of minutes, and with LiteFS, you can run SQLite alongside your app! Now we‚Äôre introducing LiteFS Cloud: managed backups and point-in-time restores for LiteFS‚Äîwhether your app is running on Fly.io or anywhere else. Try it out for yourself!\n\nWe love SQLite in production, and we‚Äôre all about running apps close to users. That‚Äôs why we created LiteFS: an open source distributed SQLite database that lives on the same filesystem as your application, and replicates data to all the nodes in your app cluster.\n\nWith LiteFS, you get the simplicity, flexibility, and lightning-fast local reads of working with vanilla SQLite, but distributed (so it‚Äôs close to your users)! It‚Äôs especially great for read-heavy web applications. Learn more about LiteFS in the LiteFS docs and in our blog post introducing LiteFS.\n\nAt Fly.io we‚Äôve been using LiteFS internally for a while now, and it‚Äôs awesome!\n\nHowever, something is missing: disaster recovery. Because it‚Äôs local to your app, you don‚Äôt need to‚Äîindeed can't‚Äîpay someone to manage your LiteFS cluster, which means no managed backups. Until now, you‚Äôve had to build your own: take regular snapshots, store them somewhere, figure out a retention policy, that sort of thing.\n\nThis also means you can only restore from a point in time when you happen to have taken a snapshot, and you likely need to limit how frequently you snapshot for cost reasons. Wouldn‚Äôt it be cool if you could have super-frequent reliable backups to restore from, without having to implement it yourself?\n\nWell, that‚Äôs why we‚Äôre launching, in preview, LiteFS Cloud: backups and restores for LiteFS, managed by Fly.io. It gives you painless and reliable backups, with the equivalent of a snapshot every five minutes (8760 snapshots per month!), whether your database is hosted with us, or anywhere else.\n\n\n## How do I use LiteFS Cloud?\n\nThere‚Äôs a few steps to get started:\n\nThere are some docs here, but that‚Äôs literally it. Then your database will start automagically backing up, we‚Äôll manage the backups for you, and you‚Äôll be able to restore your database near instantaneously to any point in time in the last 30 days (with 5 minute granularity).\n\nI want to say that again because I think it‚Äôs just wild ‚Äì you can restore your database to any point in time, with 5 minute granularity. Near instantaneously.\n\nSpeaking of restores‚Äîyou can do those in the dashboard too. You pick a date and time, and we‚Äôll take the most recent snapshot before that timestamp and restore it. This will take a couple of seconds (or less).\n\nWe‚Äôll introduce pricing in the coming months, but for now LiteFS Cloud is in preview and is free to use. Please go check it out, and let us know how it goes!\n\n\n## The secret sauce: LTX \u0026 compactions\n\nLiteFS is built on a simple file format called Lite Transaction File (LTX) which is designed for fast, flexible replication and recovery in LiteFS itself and in LiteFS Cloud.\n\nBut first, let‚Äôs start off with what an LTX file represents: a change set of database pages.\n\nWhen you commit a write transaction in SQLite, it updates one or more fixed-sized blocks called pages. By default, these are 4KB in size. An LTX file is simply a sorted list of these changed pages. Whenever you perform a transaction in SQLite, LiteFS will build an LTX file for that transaction.\n\nThe interesting part of LTX is that contiguous sets of LTX files can be merged together into one LTX file. This merge process is called compaction.\n\nFor example, let‚Äôs say you have 3 transactions in a row that update the following set of pages:\n\nWith LTX compaction, you avoid the duplicate work that comes from overwriting the same pages one transaction at a time. Instead, one LTX file for transactions A through C contains the last version of each page, so the pages are stored and updated only once:\n\nThat, in a nutshell, is how a single-level compaction works.\n\n\n## It‚Äôs LTX all the way down\n\nCompactions let us take changes for a bunch of transactions and smoosh them down into a single, small file. That‚Äôs cool and all but how does that give us fast point-in-time restores? By the magic of multi-level compactions!\n\nCompaction levels are progressively larger time intervals that we roll up transaction data. In the following illustration, you can see that the highest level (L3) starts with a full snapshot of the database. This occurs daily and it‚Äôs our starting point during a restore.\n\nNext, we have an hourly compaction level called L2 so there will be an LTX file with page changes between midnight and 1am, and then another file for 1am to 2am, etc. Below that is L1 which holds 5-minute intervals of data.\n\nWhen a restore is requested for a specific timestamp, we can determine a minimal set of LTX files to replay. For example, if we restored to January 10th at 8:15am we would grab the following files:\n\nSince LTX files are sorted by page number, we can perform a streaming merge of these twelve files and end up with the state of the database at the given timestamp.\n\n\n## Department of Redundancy Department\n\nOne of the primary goals of LiteFS is to be simple to use. However, that‚Äôs not an easy goal for a distributed database when our industry is moving more and more towards highly dynamic and ephemeral infrastructure. Traditional consensus algorithms require stable membership and adjusting the member set can be complicated.\n\nWith LiteFS, we chose to use async replication as the primary mode of operation. This has some trade-offs in durability guarantees but it makes the cluster much simpler to operate. LiteFS Cloud alleviates many of these trade-offs of async replication by writing data out to high-durability, high-availability object storage‚Äîfor now, we‚Äôre using S3.\n\nHowever, we don‚Äôt write every individual LTX file to object storage immediately. The latency is too high and it‚Äôs not cost effective when you write a lot of transactions. Instead, the LiteFS primary node will batch up its changes every second and send a single, compacted LTX file to LiteFS Cloud. Once there, LiteFS Cloud will batch these 1-second files together and flush them to storage periodically.\n\nWe track the ID of the latest transaction that‚Äôs been flushed, and we call this the ‚Äúhigh water mark‚Äù or HWM. This transaction ID is propagated back down to the nodes of the LiteFS cluster so we can ensure that the transaction file is not removed from any node until it is safely persisted in object storage. With this approach, we have multiple layers of redundancy in case your LiteFS cluster can‚Äôt communicate with LiteFS Cloud or if we can‚Äôt communicate with S3.\n\n\n## What‚Äôs next for LiteFS Cloud?\n\nWe have a small team dedicated to LiteFS Cloud, and we‚Äôre chugging away at new exciting features! Right now, LiteFS Cloud is really just backups and restores, but we are working on a lot of other cool stuff:\n\nWe‚Äôre really excited about the future of LiteFS Cloud, so we wanted to share what we‚Äôre thinking. We‚Äôd also love to hear any feedback you have about these ideas that might inform our work."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/we-raised-a-bunch-of-money/",
    "content": "This past July, we raised $25MM from A16Z and our existing investors, including Intel Capital and Dell. Recently, we raised an additional $70MM led by EQT Ventures.\n\nWhy do startups write announcements like these? We went back and forth on it. There are lots of reasons, most of them dumb.\n\nOur first reason is obvious, and mercenary. It‚Äôs the same reason we write anything: to woo customers. We‚Äôre all adults here, we can talk about this stuff, right? There are customers who are comfortable engaging with tiny Fly.io, and others who are comfortable engaging with the Fly.io that raised an additional $70MM led by EQT ventures. Alcoa: ring us up!\n\nMore compellingly, it‚Äôs an opportunity to gaze deeply into our own navels. We‚Äôve been talking to users, fans, and detractors about what we‚Äôve been doing, for years. We evolved, and got religion about a particular vision of what we‚Äôre building. We shared that with investors, and they bought it (suckers). Now we‚Äôll share with you.\n\n\n## The Two Hour Problem\n\nHere‚Äôs what we believed in 2020: apps work better when they run closer to their users. Some kinds of apps, like video or real-time presence, can‚Äôt be done without physical locality. So, that‚Äôs what we expected to talk about on our HN launch thread: WebRTC, edge caching, game servers.\n\nWhat people actually wanted to talk about, though? Databases.\n\nHere‚Äôs what we missed: we thought there was a particular kind of ‚Äúedgy‚Äù app that demanded global deployment. But it turns out, most apps want to be edgy‚Ä¶ if it‚Äôs easy.\n\nWhat‚Äôs going on here? Why is edge deployment table stakes for a game server and an untenable science project for an online bookstore? We think it‚Äôs because game servers have to be edgy, and online bookstores don‚Äôt. The game server team will bang on edge deployment until it‚Äôs solved. The bookstore team will try for about two hours, not find a clear path forward, and then give up and move on to other things.\n\nThe result of this is an Internet where all of the world‚Äôs CRUD apps are hosted in Loudoun County, VA (motto: ‚Äúwhere tradition meets innovation‚Äù), at Amazon‚Äôs us-east-1 in Ashburn, a city with so many Rails apps that one of them was elected to the county Board of Supervisors.\n\nWe think everybody understands that it‚Äôd be better to run close to users rather than in the Internet‚Äôs least worst data center. But with ordinary tooling, getting an app running in more than one city at the same time isn‚Äôt a two-hour problem: in two hours, you‚Äôll learn that it‚Äôs possible to run simultaneously in Sydney, Frankfurt, and Dallas, but not how to do it, or how long it‚Äôll take.\n\nSo our bet is simple: with the right platform and toolchain, people building bookstores, sandwich rating apps, music recommenders, mailing list managers for churches, and every other kind of app will build apps that run fast globally. Not just walking distance from Carolina Brothers BBQ in Ashburn, but in Chicago, or Sydney, or Singapore, or S√£o Paulo. Because being fast in more than one city at the same time is a super valuable feature!\n\nWe think this pattern holds for a lot of things. We‚Äôre going to track those things down and build them.\n\nFor example: sandboxing, code editors and REPLs, and CI/CD applications all have to figure out how to run untrusted customer code. They all figure out how to spin up locked down containers on demand. But being able to spin up a VM on the fly is a super valuable feature for all kinds of apps (as anyone who‚Äôs ever debugged a stuck job queue can attest). Why doesn‚Äôt everybody do it? Because it isn‚Äôt clear after two hours of investigation how to do it. So we built Fly Machines, which makes spinning up a VM as straightforward as calling a function.\n\nWe‚Äôve got more things like this coming. Real-time features and user presence are two-hour features. So is encryption and secret storage. And clustered databases. And hardware-accelerated inferencing.\n\nThere are other companies looking to solve ‚Äútwo hour window‚Äù problems for developers: distributed databases, data locality, storage, AI, app frameworks. If we get Fly.io right, we‚Äôll give those platforms new primitives to build on top of, get new ideas in front of users faster, and ratchet up the quality of every application anywhere.\n\nSounds like an investment pitch? Well, yeah, it was.\n\n\n## Why We Raised A Bunch Of Money\n\nHere‚Äôs what we think it takes to build this kind of platform:\n\nThose things are all capital intensive, and alongside them we‚Äôd like to place more bets: on advanced storage, on security capabilities, on new kinds of hardware. So you see where the money goes.\n\n\n## Here‚Äôs What‚Äôs Not Changing\n\nüé∂ There are two kinds of platform companies üé∂ : the kind where you can sign up online and be playing with them in 5 minutes, and the kind where you can sign up online and get a salesperson to call and quote you a price and arrange a demo.\n\nüé∂ There are two kinds of platform companies üé∂ : the kind you can figure out without reading the manual, and the kind where publishers have competing books on how to use them, the kind where you can get professionally certified in actually being able to boot up an app on them.\n\nüé∂ There are two kinds of platform companies üé∂ : the kind where you can get your Python or Rust or Julia code running nicely, and the kind where you find a way to recompile it to Javascript.\n\nThe kind of platform company we want to be hasn‚Äôt changed since 2020. Our features are all generally a command or two in flyctl, and they work for any app that can be packaged in a container.\n\nYou can take our word for that, but if you‚Äôve already got a working Docker container for your app, you can put us to the test. From a standing start, you should be able to get it running on Fly.io in single digit minutes, and on every continent in just a minute or two more."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/python-async-workers-on-fly-machines/",
    "content": "In this post we use Fly Machines to implement lightweight background jobs for a Python web application. Check it out: you can be up and running on Fly.io in just minutes.\n\nLast year, while working in what was my day job at the time (before I joined Fly.io!), we had just developed a new internal tool to help an adjacent team with their work. This adjacent team wrote technical content, and they had a lot of issues stemming from differences in library and language versions in the team members‚Äô local environments as compared to our production environment.\n\nThere are a lot of possible solutions to this problem, but because of the unique needs and skillset of this team, we decided to build an app for them to work in, and allow them to just get rid of their local environment entirely. This way, we could ensure that all the versions were exactly as expected, and over time we could also add more assistive features.\n\nAt the start, this was a super-hastily-thrown-together, barely an MVP tool that kinda sorta met the internal users‚Äô needs most of the time. The first version was only good enough because their previous workflow was just so awful ‚Äî it was difficult for us to do worse.\n\nOne thing our new app needed to do, was build and install libraries (the same ones our teammates had been installing locally), and we needed to rebuild them regularly (think, when a user clicks a ‚ÄúBuild‚Äù button in the app).\n\nInitially, we simply implemented these builds in the backend directly. This worked great for a little while, and it was nice to only have to deploy one thing. But then we discovered that (1) for some edge cases, our builds were very slow (occasionally over 30 minutes ‚Äî far too slow for the HTTP request cycle‚Ä¶), and (2) some builds took a lot of resources, so occasionally, even after over-provisioning, if two builds came in at once, our backend got killed (and the builds never completed).\n\nBased on this less-than-awesome experience, it became clear to us that we needed background jobs!\n\nWe ended up configuring Celery, as one does (when one is a Python developer anyway). However, this wasn‚Äôt as pain-free as it could have been. There‚Äôs some significant configuration required, and Celery was overkill for our very simple use case.\n\nPlus ‚Äì those expensive builds? We needed to have a worker (or several workers) available to run them any time, even though we only had a handful of team members using the tool, so most of the time the worker was idle. We were paying for resources we weren‚Äôt using most of the time ‚Äî not at all awesome for a bootstrapped startup!\n\nSo, how could we have implemented super simple background jobs, and avoid paying for resources we didn‚Äôt need?\n\nWell, it turns out that it‚Äôs really pretty easy to implement simple background jobs using Fly Machines! I‚Äôll show you how.\n\n\n## How it works\n\nFirst some background. Fly Machines are lightweight VMs based on Firecracker that start up super fast (you can read more details about Machines in our Machines documentation). They also have a convenient and simple API, making them easy to start, stop, and interact with from your code.\n\nFor the purposes of this post, we‚Äôll be building a demo app - a super minimal Flask web application which sends email in a background job (full code available here). You can also try out the application at darla-send-email.fly.dev. Note: for demonstration purposes, the application I‚Äôve deployed uses the dummy_send_email function, which doesn‚Äôt actually send an email! You can also deploy your own version with real Mailjet credentials, though.\n\nSo, here‚Äôs how our implementation works from a high level:\n\nOne really cool thing about this implementation is that you only pay for worker resources when your workers are actually, you know, doing work. For infrequent, expensive background jobs, this can make a huge difference in costs!\n\nBefore we get into the code, we‚Äôll need to set up a few bits of infrastructure. Let‚Äôs check how that‚Äôs done.\n\n\n## Infrastructure setup\n\nI‚Äôll assume you‚Äôve already set up your Fly.io account and installed the flyctl commandline tool. If you haven‚Äôt done that yet, follow these instructions to install flyctl, sign up, log in to fly.io, and then come back here!\n\nAfter you have your Fly.io account set up and flyctl installed locally, you‚Äôll need to create two pieces of infrastructure: a Fly.io App, which the Machines that run the background jobs will belong to, and a Redis instance, which we‚Äôll use to communicate between the web application and the background job Machines.\n\n\n## Create an app\n\nFly.io Machines need to be created in an app, so we‚Äôll need to create an app.\n\nWarning: app names are unique across all Fly.io users, so you‚Äôll need to pick something unique. You can also call fly apps create without an app name, and let it generate one for you, if you‚Äôre stuck.\n\n\n## Create an Upstash Redis instance\n\nTake note of the Redis url that‚Äôs printed after creation. If you forget it, you can see it again using fly redis status.\n\n\n## The worker code\n\nFirst, let‚Äôs take a look at the code that we‚Äôll run on the Machine:\n\nYou might notice something missing here ‚Äî the code that actually sends the email. You‚Äôll also need to implement the functions that do the work of the background jobs, and include them in the worker library. You can take a look at the send_email function in tasks.py in the demo code repo, to see the implementation for sending an email!\n\nHere‚Äôs an example of the task info that might be stored in Redis for sending an email from our demo app:\n\nWe‚Äôre sending the module and function name as strings in the task information in Redis. There are more sophisticated options here, but this approach works for our simple use case!\n\n\n## Code to call the worker\n\nThen, let‚Äôs take a look at the code that we‚Äôll use to set up the Machine and kick off the background job:\n\nWe‚Äôll call this code from our web application whenever the POST endpoint (to send an email) is called. This will kick off the job running on a Fly Machine, and return the task id, which is used to retrieve the results!\n\n\n## Code to retrieve results\n\nWhen we retrieve the results, we need to first check whether the Machine is still running. If it‚Äôs still running, we can just return a PENDING status, and expect the client will try again later.\n\nOnce the Machine is done, we can retrieve the result that the job wrote to Redis, and return it to the caller!\n\nIn our simple demo web application, we have a GET /status/{task_id} endpoint, which calls this function to retrieve the result and then displays it to the user. If the status is PENDING, the user can refresh the page to try again.\n\n\n## Code to clean up resources\n\nAfter results have been retrieved, you‚Äôll want to clean up: remove the Machine, and delete the values stored in Redis.\n\nAnd that‚Äôs it! Now we have a super-simple implementation of background jobs using Fly Machines. üéâ\n\n\n## What‚Äôs next?\n\nIn this post, I‚Äôve presented a very simple proof of concept implementation of background jobs on Fly.io Machines with Python. For some simple apps, you can use this approach as it is, but there‚Äôs a lot more you could do without very much effort! Here‚Äôs some ideas to get you started:"
  },
  {
    "title": "You can play with this right now.",
    "url": "https://fly.io/blog/vanilla-candy-sprinkles/",
    "content": "Recapping where we are to date:\n\nPicking up where we left off, this blog post will describe literally dozens (and that‚Äôs actually an understatement as you will soon see) of considerably more, dare I say it, vanilla frameworks that you can assemble on your own and deploy to fly.io and elsewhere.\n\nThis can be overwhelming, so to make things easier we are going to define a baseline application that will be reimplemented to take advantage of various tools. The result will be:\n\nLet‚Äôs get started!\n\n\n## Baseline requirements\n\nWhat we are looking for is a cross between Hello, World! and Rosetta Code, but for a full stack application. For our purposes, the baseline is a stateful web server. Ideally one that can be deployed around the globe, and can deliver real time updates. But for now we will start small and before you know it we will have grown into the full application.\n\nA simple application that meets these requirements is one that shows a visitors counter. A counter that starts at one, and increments each time you refresh the page, return to the page, or even open the page in another tab, window, browser, or on another machine. It looks something like this:\n\nAs previously discussed, key to deployment is a package.json file that lists all of your dependencies, optional build instructions, and how to start your application. We are going to start very simple, with no dependencies and no build process, so the package.json file will start out looking like the following:\n\nNow to complete this we are going to need not only a server.js file, but also HTML, CSS, and image(s). As with some of the cooking shows you see on the television, we are going to skip ahead and pull a completed meal out of the oven. Run the following commands on a machine that has node.js \u003e= 16 installed:\n\nOnce this command completes, you can launch the application with npm run start. If you have authenticated and have flyctl version 0.1.6 or later installed, you can launch this application with fly launch followed by fly deploy. When you run fly launch, consider saying yes to deploying a postgres and redis database as we will be using them later.\n\nDon‚Äôt have node installed or a fly.io login? Deploy using [Fly.io terminal](https://fly.io/terminal) or see our [Hands-on](https://fly.io/docs/hands-on/) guide that will walk you through the steps.\n\nIf you are running it locally, open http://localhost:3000/ in your browser. If you have deployed it on fly.io, try fly open. If you are running in a fly.io terminal, there is a handy link you can use on the left hand pane.\n\nNow take a look at server.js. It is all of 72 lines, including blank lines and comments. In subsequent sections we show how to make it smaller using available libraries, and how to add features. But before we proceed, lets save time and keystrokes by installing the node-demo package, which we will use repeatedly to generate variations on this application:\n\n\n## Starting small\n\nIf you look at the top of the server.js file you will see a number of calls to require(). This is Nodes CommonJS modules. Node also supports EMCAScript modules, which is what all the cool kids are using these days.\n\nThis requires opting in. You can let node-demo make the changes for you by running the following command:\n\nThis script will detect what changes need to be made, give you the option to show a diff of the changes, and to accept or reject the changes. This leads us to the second option: --force that will automatically apply the changes without prompting:\n\nRelaunch your application locally using npm run start or redeploy it remotely using fly deploy.\n\n\n## Using a real template\n\nInside the application you can see that the HTML response is produced by reading a template file and replacing a placeholder string with the current count:\n\nWhile this is fine for this example, larger projects would be better served with a real template. node-demo supports two such templating engines at the moment: ejs and mustache. Select your favorite, or switch back and forth:\n\nand\n\nBe sure to add --esm if you want to continue to use import statements.\n\n\n## A more substantial change\n\nWhile node:http provides the means for you to create a capable HTTP server, it requires you to be responsible for status codes, mime types, headers, and other protocol details. express will take care of all of this for you:\n\nBoth ejs and mustache have integrations with express. Try switching between the two to see how they differ.\n\n\n## A real database\n\nMaintaining a counter in a text file is good enough for a demo, but not suitable for production. Sqlite3 and PostgreSQL are better alternatives:\n\nand\n\nSqlite3 is great for development, and when used with litefs is great for deployment. PostgreSQL can be used in development, and currently is the best choice for production.\n\nTo run with PostgreSQL locally, you need to install and start the server and create a database. For MacOS:\n\n\n## Be as weird as you want to be\n\nThe next two options are frankly polarizing. People either love them or hate them. We won‚Äôt judge you.\n\nFirst tailwindcss is a CSS builder that works based on parsing your class attributes in your HTML:\n\nNext is typescript which adds type annotations:\n\nTypeScript should work with all of the options on this page, in many cases making use of development only @types. All of this should be handled automatically by node-demo.\n\nBoth of these require a build step, which can be run via npm run build. A change to the Dockerfile used to deploy is also required, which can be made using:\n\ndockerfile-node is actually a separate project with its own options for you to explore.\n\n\n## Object Relational Mappers (ORMs)\n\nAdding databases was the first change that we‚Äôve seen that actually makes the demo application noticeably larger, particularly with PostgreSQL once the code that handles reconnecting to the database after network failures is included. This can be handled by including still more libraries, this time Object Relational Managers (ORMs). Three popular ones:\n\nand\n\nand\n\nKnex runs just fine with vanilla JavaScript. Prisma can run with vanilla JavaScript, but works better with TypeScript. Drizzle requires TypeScript.\n\nPrisma and Drizzle also require a build step.\n\nA final note: if you switch back and forth between Sqlite3 and PostgreSQL, you may get into a state where the migrations generated are for the wrong database. Simply delete the prisma or src/db/migrations directory and rerun the npx demo command to regenerate the migrations.\n\n\n## Real Time Updates\n\nIf you open more than one browser window or tab, each will show a different number. This can be addressed by introducing websockets:\n\nThe server side of web sockets will be different based on whether or not you are using express. For the first time we are providing a client side script which is responsible for establishing (and reestablishing) the connection, and updating the DOM when messages are received. This is a chore, and htmx is one of the many libraries that can be used to handle this chore:\n\nThe next problem is that if you are running multiple servers, each will manage their own pool of WebSockets so that only clients in the same pool will be notified of updates. This can be addressed by using redis:\n\nAt this point, if you are using fly.io, postgres, and redis, you can go global:\n\n\n## Packaging alternatives\n\nSo far, we have been using npm, but yarn and pnpm are alternatives that may be better for some use cases:\n\nand\n\nEach package manager organizes the node_modules directory a bit differently, so for best results when switching, remove the node_modules directory before switching:\n\nWindows Powershell users will want to use the following command instead:\n\n\n## Future explorations\n\nWhile we have explored many options, this only scratches the surface. There are many alternatives to the libraries above, and many more things to explore. Examples:\n\nnode-demo is open source, so issues, pull requests, and discussions are always welcome!\n\nI hope you have found this blog post to be informative, and perhaps some of you will use this information to start your next application ‚Äúvanilla‚Äù with your personal selection of toppings. Yummy!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-to-fly-replay/",
    "content": "Fly.io is a platform for compute. You can do a bunch more than just run your average web app! Check out the Machines platform and see how your business might run on Fly.io.\n\nThe Fly Replay header is deceptively simple. All your app has to do is respond with a header, and the HTTP request gets re-ran somewhere else.\n\nIt‚Äôs behind-the-scenes of some pretty interesting apps on Fly.io (we wrote about using it with Globally Distributed Postgres).\n\nWe often bring it up when answering questions by those enamored with the Machines platform.\n\nSo, here‚Äôs a use case I think is pretty neat.\n\n\n## But first: What is it?\n\nAll public network traffic headed into Fly.io goes through the Fly Proxy. The proxy has features! One of those features involves looking for a fly-replay header in responses.\n\nThe fly-replay header tells the Fly Proxy to replay an HTTP request somewhere else. This gives your applications some power.\n\nDepending on the value your app gives the fly-replay header, the Fly Proxy can replay the initial HTTP request on another app, in a different region, on a specific VM, or a mix of those things. This only works for sending apps within the same Fly.io organization.\n\nHere‚Äôs what that looks like.\n\nReplay in a Different Region:\n\nI‚Äôm going to steal from the Globally Distributed Postgres article (and the corresponding docs).\n\nIf you have a ‚Äúleader‚Äù database with a bunch of read-replicas, you typically need write queries to go to the leader.\n\nIf an HTTP request (e.g. POST /foo) results in writes to your database, then sending that request to a VM near the leader database has benefits - it‚Äôs way faster than opening DB connection across the globe.\n\nTo do this, your application can return a header that looks like this:\n\nReplay in Other Apps:\n\nYou may have a bunch of apps - perhaps because each of your customers gets an app, or your have some micro services, or whatever crazy scheme you trapped yourself into.\n\nYou can route requests to specific apps:\n\nReplay in Specific VMs:\n\nMaybe you want requests to go to specific VM‚Äôs! I‚Äôve used this to make sure requests after a file upload landed on the same server.\n\nThe fly-replay was a quick way to accomplish that:\n\nSince Machines can scale down to zero (stop on exit), you can also use this as a tricky way to wake them up - just ship it an HTTP request!\n\nThere‚Äôs more you can do than just these examples, so definitely RTFM.\n\n\n## Something about a traffic cop?\n\nWe‚Äôre going to make a ‚Äúproxy‚Äù - a little app that just responds with a fly-replay header. It‚Äôll tell the Fly Proxy to replay the HTTP request on a different app.\n\nThis is useful if you, for example, point *.example.org to that router and have a specific app respond to a request - perhaps based on the hostname.\n\nThis particular use case of mine is a bit like a load balancer - a ‚Äúreverse proxy‚Äù, but with some code instead of configuration.\n\nI like Go for HTTP plumbing, so let‚Äôs do some of that. We‚Äôre going to write the type of ‚Äútoy‚Äù app that accidentally stays in production for 14 years.\n\nThis ‚Äúproxy‚Äù app will check the request hostname against a database of known apps, and route the request as needed.\n\nThe full(ish) code is here.\n\n\n## It‚Äôs basically just this\n\nThe important logic is this bit of standard Go HTTP stuff:\n\nGo‚Äôs HTTP library does a prefix match on HTTP URI‚Äôs, so \"/\" will match anything, which is just what we want.\n\nAll we do is find a customer (based on hostname) and respond with a replay header.\n\nThis is great when paired with a SQLite database, as (trigger warning) reads from the local disk are pretty quick relative to network stuff.\n\nThe Find function is just a sql query (but super verbose, because Golang):\n\nLocally, the whole round trip of the HTTP request + database lookup took ~4ms. In the real world, it added ~100ms to hit this proxy and replay the request against another Fly.io app (my crufty blog).\n\nTo test this out, I ran a few curl requests:\n\n\n## Preventing Direct Access\n\nIn this scenario, we want the ‚Äúproxy‚Äù app to be available publicly, while keeping customer apps private.\n\nHowever, the Fly Proxy needs to know where apps are listening when it directs HTTP requests to them. Therefore, we need to define services in the fly.toml file.\n\nYou also might be dynamically creating apps, in which case you don‚Äôt need a fly.toml file, but will be defining services via Machine API calls.\n\nLuckily, we can keep the apps private while still telling the Fly Proxy how to reach them. The easiest way is to create the app without any public IP addresses via the fly launch command:\n\nThe flag --no-public-ips is the key there. However, it requires the newer Machines-based apps platform. Also, if you‚Äôre creating apps via the Machines API, having no public IP‚Äôs is the default.\n\nNow the customer apps are private, and the Fly Proxy can still replay requests against them.\n\n\n## App Discovery\n\nI used a SQLite database to map domains to apps. If this proxy ran globally, I could have used LiteFS for distributed SQLite across multiple regions.\n\nAnother fun possibility is (ab)using Fly‚Äôs .internal addresses to check for the existence of apps (or application instances) via DNS.\n\nPerhaps we could have pinged this occasionally and created/updated an in-memory map of apps and hostnames! Here‚Äôs two DNS queries that would have been useful for that:\n\nSo this is pretty neat! The fly-replay header is a simple solution that gives you the ability to do some really neat stuff - particularly within globally distributed apps."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/js-ecosystem-delightfully-wierd/",
    "content": "Note, I‚Äôm not saying that JavaScript is weird, though it definitely is weird. But that‚Äôs not the point of this blog post.\n\nBear with me, instead of starting with how JavaScript ecosystem is weird, I‚Äôm going to start with why the JavaScript ecosystem is weird.\n\n\n## Historical Background\n\nLess that 10 years ago, JavaScript sucked bad. It had no imports, no classes, no async, no arrow functions, no template literals, no destructuring assignment, no default parameters, no rest/spread parameters. And the environment it predominately ran in, namely the browser‚Äôs DOM, sucked too. JQuery made it suck less. It still sucked, but was ‚Äî at that point in time ‚Äî relatively sane.\n\nBundling JS to run in the browser was the first sign of weirdness. In that process you would also want to both minimize and tree shake the source, and perhaps even code split. In general the process involved reading a number of JavaScript sources as input and then producing one or more JavaScript sources as output. This meant that the code you were executing wasn‚Äôt the code you wrote. Sourcemaps helped.\n\nThen CoffeeScript came along. Instead of writing in JavaScript, you would write in a language which was compiled into JavaScript. This is a bit different than languages like Elixir and Kotlin which compile into the same byte codes as another language, CoffeeScript actually compiles into the other language. C++ started out this way.\n\nThen came ECMAScript 6 in 2015. JavaScript improved rapidly in the next few years. This eventually mostly displaced CoffeeScript, but presented a different problem: for a while the implementations were not keeping up so transpilers like Babel came along that compiled current and future versions of JavaScript into older versions of JavaScript that ran on supported environments. Currently esbuild is rapidly rising in popularity as a Javascript bundler/transpiler.\n\nAlong the way, emscripten came along which compiled actual machine code into a subset of JavaScript, though these days the new target for this tool is generally Wasm.\n\nLately the pace of innovation in JavaScript has slowed, and JavaScript implementations are doing a better job of keeping up, so you would think that the need for transpilers would be waning, particularly on the server side where there is no need for bundlers. But that‚Äôs not happening. And the reason why is an interesting story.\n\n\n## Nobody Writes JavaScript Any More\n\nOK, the title above is clearly hyperbole, but I‚Äôll describe a number of the many ways that people aren‚Äôt writing JavaScript any more.\n\nIf you write a Rails application, you write it in Ruby. If you write a Django application, you write it in Python. Phoenix, Elixir. Lavavel, PHP. Rails gets a lot of flack for doing magic using meta-programming, and Elixir has macros, but all of the above stay within the boundaries of what can be done by the language.\n\nJavaScript, however, is different. While it nominally is standardized by EMCA TC39, if you are using a popular framework like Next.JS, Remix, or Svelte you are not coding in ECMAScript as standardized by ECMA TC39. Four examples:\n\nI mentioned earlier that Rails gets a lot of flack for its use of meta programming. Nobody bats an eye at any of the ‚Äúabuses‚Äù of the JavaScript language mentioned above. The JavaScript ecosystem is a Big Tent party.\n\n\n## ‚Äúuse server‚Äù;\n\nThe latest abuse of the bundler is by React Server Components (RSC). First demoed with express, it is now adopted by Next.js.\n\nWhat ‚Äúuse server\" and \"use client\" do, other than being a valid JavaScript statements that do absolutely nothing, is change the meaning of the code that follows them. This has gotten mixed reviews, but in my mind is very much in the spirit of \"use strict\"which also changes the meaning of the code that follows.\n\nWhile JSX often compiles to JS, the Server React DOM APIs enable compilation to HTML. RSC goes a different way, and compiles into a stream of tagged JSON. This is all very transparent to you, but what it does enable is a different style of programming. One that many are comparing to PHP and even Rails:\n\nIt is not clear to me whether these comparisons are meant in a positive way, but I will say that from my perspective it is a very good thing.\n\nFrom a fly.io perspective, RSC enabling an Update (Refetch) Sequence is very much of interest. We‚Äôve always been especially shiny for frameworks that benefit from geographic distribution, like Elixir‚Äôs LiveView, Laravel‚Äôs Livewire and Ruby on Rail‚Äôs Hotwire. We want those kinds of frameworks to succeed, because the better they do, the more valuable we are. Now we can add React‚Äôs RSC to that list.\n\nReturning to the topic at hand, the fact that such a feature is only made possible through cooperation with bundlers ‚Äî a statement tantamount to saying a change to the JavaScript language itself ‚Äî is profound and, dare I say it, delightful.\n\n\n## Another Dimension\n\nDan Abramov gave a talk at RemixConf entitled React from Another Dimension:\n\nIn Dan‚Äôs talk he imagines an alternate universe in which React was first implemented in the late 90s on the server and still managed to converge to where it is today. During the talk he launches a Windows 95 emulator and runs Internet Explorer (specifically, IE6) with React. He even manages to get nine out of ten steps working using that operating system and browser combination.\n\nThe mind bending parts of this presentation are where he first utilizes use server to implement a client side form action, and then later launches a client side alert from the server using use client.\n\nAnd he closes by saying that this requires new generation routers and new generation bundlers.\n\nAnd to think all of this is made possible by the fact that the JavaScript you write not only isn‚Äôt the JavaScript you run, but under closer examination isn‚Äôt even JavaScript at all."
  },
  {
    "title": "Fly.io ‚ù§Ô∏è all things Python.",
    "url": "https://fly.io/blog/deploying-langchain-to-fly-io/",
    "content": "In this post we deploy a minimal LangChain app to Fly.io using Flask. Check it out: you can be up and running on Fly.io in just minutes.\n\nI hear about Large Language Models (LLM) everywhere these days! Do you? ü§î\n\nLLMs are a type of natural language processing (NLP) technology that uses advanced deep learning techniques to generate human-like language. If you haven‚Äôt heard about LLMs, you probably heard about one of the most notable examples of it today: ChatGPT. ChatGPT is a language model developed by OpenAI and it was trained on a large amount of text data which allows it to understand the patterns and generate responses to inputs.\n\nLangChain is a Python framework that rapidly gained notoriety. It was launched as an open source project in October 2022 - yes, a few months ago. This framework was designed to simplify the creation of powerful applications providing ways to interact with LLMs.\n\nI recently created a minimal application using LangChain and deployed it to Fly.io. This article aims to share the process of how to deploy this minimal LangChain app to Fly.io using Flask.\n\nFlask is a Python micro framework for building web applications. That‚Äôs perfect for our example since it‚Äôs designed to make getting started quick and easy. That‚Äôs all we need for now.\n\nLet‚Äôs get to it! üòé\n\n\n## LangChain Models ü¶ú üîó\n\nLangChain provides an interface to interact with several LLMs.\n\nThe template is using the OpenAI LLM wrapper, which uses, at the time I‚Äôm writing this article, text-davinci-003 model by default - this model belongs to the GPT-3.5 family. Keep in mind that there are other alternatives to use more capable and less expensive models like gpt-3.5-turbo, which is the one recommended by OpenAI because of its lower cost. However, we won‚Äôt get into that in this article.\n\nLanguage models take text as input. This text is what we usually referred as a prompt. LangChain facilitates the use of those prompts. To make things a bit more interesting, the template makes use of the PromptTemplate: ask a question and also receive an input from the user.\n\n\n## Our Application üçΩ\n\nOur minimal application receives a place (city, country, etc.) as an input and give us 3 options where to eat in that place. The default value for place is Berlin.\n\nOut prompt:\n\nWhat are the 3 best places to eat in \u003cplace\u003e?\n\nYou can define your own input variable by calling the url:\n\nFor example:\n\nTo illustrate, we are using the hello.html to display the results on the browser.\n\nSo, let‚Äôs start at the beginning‚Ä¶\n\n\n## Setting up ‚öíÔ∏è\n\nWe assume the initial setup is already done and you have Python installed.It‚Äôs recommended to use the latest version of Python. We are using Flask 2.2.3 and it supports Python 3.8 and newer.\n\nCreate and enter your project‚Äôs folder:\n\nWe can go ahead and clone the repository inside your project‚Äôs folder using either\n\nHTTPS:\n\nor SSH:\n\n\n## Virtual Environment\n\nChoose a virtual environment to manage our dependencies. For simplicity, we‚Äôre using venv for this project. Inside your project, create and activate it:\n\nFrom this point on, the commands won‚Äôt be displayed with (.venv) $ but we assume you have your Python virtual environment activated.\n\n\n## Install Dependencies from requirements.txt\n\nFor this minimal example, we have a few dependencies to be installed:\n\nGo ahead and install them by running:\n\nWe are using Flask, langchain and openai packages as minimal requirements for this example. gunicorn (Green Unicorn) is the pure Python WSGI server we will use in production instead of the built-in development server - other options can be found here. Finally, we use python-dotenv to use the environment variables set on .env file - more about in the next section.\n\n\n## Environment Variables\n\nThe template contains a .env.dist file. Go ahead and rename it to .env. Our local environment variables will be stored in this .env file:\n\nThe OpenAI API uses API keys for authentication. We will need an API Key to be able to use the API in your requests. Log in to your account and check OpenAI API Key page to create or retrieve your API key to be set as OPENAI_API_KEY.\n\nNote that OPENAI_API_KEY is required because we are using OpenAI LLM wrapper - other providers will have different requirements. Here is a list of multiple LLM providers.\n\nYou can find here other options to set the environment variables like setting them on the command line or creating .flaskenv file instead.\n\n.env file is only used for your local development.\n\n\n## Local Development\n\nNow that everything is set up we can run the project:\n\nNow, we can head over to http://127.0.0.1:5000 üéâ\n\nNote that flask run command works since we set FLASK_APP on .env file. In this case, it wasn‚Äôt necessary to run the command with --app option. If our FLASK_APP setting was not set, we would need to run: flask --app \u003capp\u003e run\n\nWith our LangChain app prepped and running on our local machine, let‚Äôs move to the next section and deploy our app to Fly.io!\n\n\n## Deploying to Fly.io üöÄ\n\nflyctl is the command-line utility provided by Fly.io.\n\nIf not installed yet, follow these instructions, sign up and log in to Fly.io.\n\nNew customers‚Äô organizations use V2 of the Fly Apps platform, running on Fly Machines. If you‚Äôre already a customer, you can flip the switch to start deploying your new apps to Apps V2 with fly orgs apps-v2 default-on \u003corg-slug\u003e.\n\n\n## Launching Our App\n\nBefore deploying our app, first we need to configure and launch our app to Fly.io by using the flyctl command fly launch. During the process, we will:\n\nThis is what it looks like when we run fly launch:\n\nIf you cloned the template mentioned in this article, you will see a similar message described above.\n\nThe template provides you with an existing fly.toml file, you can copy its configuration to your app.\n\nGo ahead and define your app name and select the organization to deploy our app.\n\nThe template also provides you with existing .dockerignore and Procfile files. Those files are generated for you if they don‚Äôt exist in your project. If so, make sure you update them to fit your needs.\n\nNote that the built-in Python builder used (paketobuildpacks/builder:base) will automatically copy over the contents of the directory to the deployable image.\n\nTo keep it simple, a Procfile is used to deploy and run Python applications - the minimal generated Procfile starts the Gunicorn server with our WSGI application.\n\nBy now, we are almost ready to deploy our app. Before we do that, we need to set the environment variables to be used in production. Let‚Äôs see how that‚Äôs done.\n\n\n## Environment Variables\n\nAs mentioned before, for our local development we are using .env file to set our environment variables. In production, we can‚Äôt share such file with sensitive values.\n\nWe can specify secret values for our app using flyctl secrets command by running:\n\nThat‚Äôs it! We are now ready to deploy our app!\n\n\n## Deploying Our App\n\nLet‚Äôs simply run:\n\nOur app should be up and running!\n\nLet‚Äôs try it: https://\u003cyour-app-name\u003e.fly.dev/\u003cyour-city\u003e\n\nYAY! üéâ We just deployed our LangChain app to production! Cool, right? üòé\n\nFly.io makes it easier to deploy your apps and move them closer to your users!\n\n\n## What‚Äôs Next?\n\nOur app does the job of finding new places to eat! Now that we gave it a try, you are probably wondering: what‚Äôs next?\n\nWe got some options where to eat tonight in Berlin, here where I live! That‚Äôs a great start for what is possible to do with LangChain. But that‚Äôs a LOT more!\n\nLet‚Äôs say that I‚Äôm meeting my best friend in Berlin for dinner tomorrow.\n\nFrom all the places I could get in Berlin, I want to get the name and address, with working hours of the ones that serve Italian food (because we all love Italian food, right?) and are closer to Neuk√∂lln - my best friend‚Äôs neighbourhood. The places also need to be top-rated, with rating higher than 4.5 on Google Maps and be open tomorrow at 7pm.\n\nAnd we could go on and on here.\n\nThat looks a bit more complex and‚Ä¶\n\nIt started to look like a chain (aha!) of calls that also depend on user‚Äôs input. That‚Äôs when simple applications start to become more powerful.\n\nNote that our chain depends on user's input. Not only that, but some of the real-time information like current working hours and rating on Google Maps are not available to us.\n\nAI language models don‚Äôt have access to real-time data neither the ability to browse the internet.\n\n\n## Agents joined the chat ‚õì\n\nFor these type of chains, we got to interact with the outside world to get some answers!\n\nThat‚Äôs when agents come into play.\n\nThe ‚Äúagent‚Äù has access to a set of tools. Depending on the user‚Äôs input, the agent can decide which, if any, of the available tools to call - you can also build your own custom tools and agents.\n\nThose tools are the way we interact with the rest of the world - in our case, using Google Places API to get real-time information such as working hours and rating.\n\nThat‚Äôs so neat and it doesn‚Äôt even scratch the surface. There are so much more out there - and that‚Äôs something for future articles! Here you can find a curated list of tools and projects using LangChain.\n\nHappy coding! ü§ñ\n\n\n## Got Feedback?\n\nFor more detailed information on how to deploy a Python App to Fly.io, you can check the Fly.io Docs.\n\nIf you have any question or comments, reach out on the Fly.io Community. That‚Äôs a great place to share knowledge, help and get help!\n\nüì¢ Now, tell me‚Ä¶ What are the cool ideas you have now using LangChain? üë©üèΩ‚Äçüíª"
  },
  {
    "title": "You can play with this right now.",
    "url": "https://fly.io/blog/flydotio-heart-js/",
    "content": "Fly.io is a great place to run fullstack applications. For most programming languages, there is a defacto default fullstack framework. For Ruby, there is Rails. For Elixir, there is Phoenix. For PHP there is Laravel. For Python, there is Django.\n\nIf you don‚Äôt know where to look, Node.js appears to be a mess. For starters there are plenty of js frameworks to choose from. Then there are three different package managers. Not to mention that Typescript as an alternative to JavaScript. And if that is not bad enough Bun and Deno are providing alternatives to Node itself.\n\nThe result is predictable. Fly.io has a number of community contributed templates for a small number of Node frameworks. Some have had more attention than others.\n\nIt is time to clean up the mess.\n\n\n## package.json enters the chat\n\nThe key sentence in the preceding section starts with If you don‚Äôt know where to look. The right place to start is package.json. It tells you what dependencies need to be installed. For most frameworks, it tells you how to start the web server. And if there is a build step. And if there are any development dependencies that may be needed to run the build, and removed prior to deployment.\n\nGiven this knowledge, a baseline Dockerfile can be built for any framework that follows these conventions. Handling different package managers can be accomplished by looking for yarn.lock and pnpm-lock.yaml files. TypeScript is a devDependency and handled by the build step. While Deno projects don‚Äôt typically have package.json files, some bun projects do.\n\nThe dockerfile-node project endeavors to do exactly that:\n\nThis will create (or replace!) your existing Dockerfile, as well as ensure that you have a .dockerignore file, and optionally may create a docker-entrypoint script. You can run with this Dockerfile locally, or use it to deploy on your favorite cloud provider. For Fly.io, you would get started by running:\n\nThe --dockerfile parameter is needed to tell fly launch to use your Dockerfile rather than trying to generate a new one.\n\nOf course, if you prefer to run your application on Google Cloud Run, Amazon ECS, MRSK, or even locally, you are welcome to do so.\n\nDeploy using [Fly.io terminal](https://fly.io/terminal) or see our [Hands-on](https://fly.io/docs/hands-on/) guide that will walk you through the steps.\n\n\n## Devils in the details\n\nNot all frameworks are alike.\n\nSome will, by default, start servers that only process requests that come from the localhost. That, of course, is entirely unsatisfactory.\n\nSome require extra steps, for example applications that make use of Prisma.\n\nOne (and I won‚Äôt mention the name) actually lists the package needed to run the production server as a development only dependency.\n\nFortunately, ejs templates can include if statements and/or make use of computed variables that customize the Dockerfiles produced.\n\nAs a starter set, I‚Äôve got templates working for the following frameworks: Express, Fastify, Gatsby, Nest, Next, Nuxt, and Remix. At the moment, I‚Äôve been focusing on breadth vs depth, so what I have working may not be able to handle much more than the splash screen, but my experience is that getting that far is often the hardest part, after that point you have all the scaffolding in place and can focus on any specific issue that may come up.\n\nThose are the successes so far. Here‚Äôs a list of frameworks that are still being worked on, along with the current blocking issue:\n\nIn the fullness of time, these will be picked off one by one. This code is all open source, so everybody with an interest in a particular framework can contribute via issues and pull requests. Interest and participation will definitely affect prioritization of this work.\n\n\n## Futures\n\nOnce this script has a little bit of exposure to real world usage, it will replace the existing flyctl scanners, much in the way that dockerfile-rails is the basis for the Dockerfiles produced for Rails applications with Fly.io. At which point, usage will be as simple as fly launch.\n\nIntegration with fly launch will also enable thing like setting of secrets, defining volumes, launching of databases, and defining health checks as part of the workflow.\n\nThis package will also be designed to be re-run and accept arguments which will customize the Dockerfile produced. Peruse the usage for dockerfile-rails to see examples of the types of customizations possible. Some highlights:\n\nThe scanner will also be able to do things like detect the inclusion of puppeteer and automatically install and configure Chrome/Chromium. This is already being done for Rails applications today.\n\nAnother thing already being done for Rails applications is to run the web server as a non-root user for security reasons. Repeating this for Node.js will require knowledge of what files the application is expected to write to and which are expected to be read-only. This knowledge is necessarily framework specific, and may not be possible for minimal and general purpose frameworks like express.\n\n\n## Got Feedback?\n\nIf you have questions, comments, or concerns, let us know!\n\nIf they are even vaguely Fly.io related, feel free to use our community forum. Otherwise, start a discussion on GitHub.\n\nAnd to those that wish to contribute, perhaps to make support for their favorite framework(s) better‚Ä¶. let‚Äôs do this!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/gossip-glomers/",
    "content": "We‚Äôre Fly.io. We run apps for our users on hardware we host around the world. This post isn‚Äôt about our platform. Rather, it‚Äôs an elaborate plot to get you to write some code just for the hell of it.\n\nIn the field of computer science, the industry is represented by two separate yet equally important groups: the software developers who build Rails applications and mobile games, and the academics who write theory papers about why the problems those apps try to solve are NP-hard. This is a story about both.\n\nDistributed systems span the practical-academic divide. Reading a stack of MIT PhD dissertations may be a good Friday night, but it won‚Äôt equip you for debugging a multi-service outage at 2am. That requires real-world experience.\n\nLikewise, building a fleet of microservices won‚Äôt give you the conceptual tools to gracefully \u0026 safely handle failure. Many failure scenarios are rare. They don‚Äôt show up in unit tests. But they‚Äôre devastating when they do show up. Nailing down the theory gives you a fighting chance at designing a correct system in the first place.\n\nThe practical and academic tracks seldom converge. To fix this, we teamed up with Kyle Kingsbury, author of Jepsen, to develop a series of distributed systems challenges that combine real code with the academic rigor of Jepsen‚Äôs verification system.\n\nWe call these challenges the Gossip Glomers.\n\nWhat the f$#* is a Glomer? It‚Äôs an elaborate pun about the CAP theorem.\n\n\n## How it works\n\nYou know Kyle Kingsbury from his ‚ÄúCall Me Maybe‚Äù blog posts that eviscerate distributed databases. You may also have known about Jepsen, the Clojure-based open-source tooling Kyle uses to conduct these analyses. Well, Kyle also wrote another tool on top of Jepsen called Maelstrom.\n\nMaelstrom runs toy distributed systems on a simulated network. It easily runs on a laptop. Kyle uses it to teach distributed systems. We all thought it‚Äôd be neat to build a series of challenges that would teach people around the Internet Maelstrom, and, in turn, some distributed systems theory.\n\nEach challenge is composed of several parts:\n\n\n## Pathway to distributed systems enlightenment\n\nOur challenges start off easy and get more difficult as you move along. They‚Äôre organized into six high-level challenges with many of those having several smaller challenges within them.\n\nFirst, you‚Äôll start with the Echo challenge. This is the ‚Äúhello world‚Äù of distributed systems challenges. It gets you up and running and helps you understand how these challenges work.\n\nNext, you‚Äôll build a totally-available, distributed unique ID generator. In this challenge, nodes will need to be coordination-free and independently generate a unique identifier for any number of clients.\n\nAfter that, the difficulty starts to ramp up with the broadcast challenge. In this challenge, you‚Äôll need to propagate messages out to all the nodes in the cluster. You‚Äôll need to ensure fault tolerance in the face of network partitions and then work to optimize your message delivery to minimize the number of messages sent within your system.\n\nOnce you‚Äôve made it past broadcast, you‚Äôll implement a grow-only counter, or g-counter. The tricky part with this challenge is that you‚Äôll need to build on top of Maelstrom‚Äôs sequentially consistent key/value store.\n\nThen you‚Äôll dive into the world of replicated logs by building a Kafka-like system. This challenge will build on the linearizable key/value store provided by Maelstrom but you‚Äôll need to figure out how to not only make it correct but also efficient.\n\nFinally, you‚Äôll wrap up with the totally-available transactions challenge where you‚Äôll build a transactional database on various consistency levels.\n\n\n## A bit of history\n\nOver the past year, we‚Äôve been growing like gangbusters. That‚Äôs great. But it also means we‚Äôve been hiring, and hiring is hard.\n\nWe hire resume-blind, based on work-sample tests: we have people write code and design systems, and then score those submissions based on a rubric. We‚Äôve got criteria set up for early-career, mid-level, and team-lead developers. But we didn‚Äôt have strong criteria for hiring staff engineers.\n\nSo we began tossing around ideas. In a previous life, some of us had success with a series of cryptography challenges called Cryptopals, so we figured we‚Äôd try something similar, but with a distributed systems flavor.\n\nThat sounded great but how do you actually test distributed systems to know if someone passed or failed? For weeks, we wrote up one iteration after another but none of them felt right.\n\nFinally, we had a brilliant idea. Let‚Äôs find someone who lives and breathes distributed system validation! That someone is Kyle Kingsbury.\n\nAfter working on these challenges with Kyle, we realized that they are too much fun to keep to ourselves as an internal evaluation tool. So we‚Äôre releasing them for anyone to play with.\n\n\n## But wait‚Ä¶ there‚Äôs more!\n\nIf you scoff in the face of cascading failures, if you bend consistency levels to your will, and if you read k8s.af post-mortems as bedtime stories to your kids, you may be interested in trying our hardest challenge.\n\nWe reserved this last challenge for evaluating our staff engineers at Fly.io. So if you think you‚Äôd be up to the challenge, we‚Äôd love to talk to you."
  },
  {
    "title": "Try it out yourself!",
    "url": "https://fly.io/blog/shipping-logs/",
    "content": "Fly.io runs apps (globally) in just few commands. That means a lot of log output! Centralizing logs is important. Fire up an app and follow along as we see just how easy it can be.\n\nNearly all of our apps are puking output. Sometimes, it‚Äôs intentional. Often this output is in the form of structured logs.\n\nLogs are helpful for a variety of use cases - debugging, tracking, collating, correlating, coalescing, and condensing the happenings of your code into useful bits of human-parsable information.\n\nThere can be a lot of logs, from a lot of apps. Aggregating logs to a central place is useful for many reasons, but here are my top 2 favorite:\n\n\n## The Logging River\n\nSince we grab stdout from the processes run in your apps, whatever an app outputs becomes a log. Logs are constantly flowing through Fly.io‚Äôs infrastructure.\n\nHere‚Äôs how that works.\n\nYour apps run in a VM via Firecracker. Inside the VM, we inject an init process (pid 1) that runs and monitors your app. Since we build VM‚Äôs from Docker images, init is taking ENTRYPOINT + CMD and running that. The init program (really just a bit of Rust that we named init) is, among other things, gathering process output from stdout and shooting it into a socket.\n\nOutside of the VM, on the host, a bit of Golang takes that output and sends it to Vector via yet-another socket.\n\nVector‚Äôs job is to ship logs to other places. In this case, those logs (your app‚Äôs output) are shipped to an internal NATS cluster. For the sake of simplicity, let‚Äôs call NATS a ‚Äúfancy, clustered pubsub service‚Äù. Clients can subscribe to specific topics, and NATS sends the requested data to those subscribers.\n\nIn true Fly.io fashion, a proxy sits in front of NATS. We call this proxy ‚ÄúFlaps‚Äù (Fly Log Access Pipeline Server‚Ñ¢, as one does). Flaps ensures you only see your own logs.\n\nNATS is the fun part! You can hook into NATS (via Flaps) to get your logs.\n\nTo get your logs, all you need is an app that acts as a NATS client, reads the logs, and ships them somewhere. Vector can do just that! It‚Äôs fairly simple - in fact, we‚Äôve done the work for you:\n\n\n## The Fly Log Shipper‚Ñ¢\n\nTo ship your logs, you can run an instance of the Fly Log Shipper.\n\nThis app configures a Vector sink of your choosing, and runs Vector. A sink is a ‚Äúdriver‚Äù that Vector will ship logs to, for example Loki, Datadog, or (bless your heart) Cloudwatch.\n\nI liked the look of Logtail, so I tried out its free tier.\n\nLogtail actually lets you set Fly.io as a source of logs, but as we can see, we‚Äôre actually just telling Vector to send logs somewhere. If your log aggregator doesn‚Äôt know Fly.io exists, that‚Äôs fine. It just needs a Vector sink to exist. The process is the same no matter what log aggregator you use.\n\nIf you sign up for Logtail, it helpfully gives you instructions on setting that up with Fly.io.\n\nLet‚Äôs go ahead and follow those instructions (they‚Äôre similar to what you see on the Log Shipper repo).\n\n\n## Using the Log Shipper\n\nThe NATS log stream is scoped to your organization. This means that the Fly Log Shipper collects logs from all your applications.\n\nHere‚Äôs how to set it up with Logtail:\n\nYou can configure as many providers as you‚Äôd like by adding more secrets. The secrets needed are determined by which provider(s) you want to use.\n\nBefore launching your application, you should edit the generated fly.toml file and delete the entire [[services]] section. Replace it with this:\n\nThen you can deploy it:\n\nYou‚Äôll soon start to see logs appear from all of your apps.\n\nThat wasn‚Äôt too bad!\n\nYou have apps. Apps have logs! Run your apps and ship your logs in just a few commands.\n\n\n## Shipping Specific Logs\n\nSo far we‚Äôve seen how to ship logs from every application in your organization.\n\nYou can, however, narrow that down by setting a SUBJECT environment variable. That can be set in the fly.toml‚Äòs [env] section, or as an application secret.\n\nI opted to add it to my fly.toml, which looked like this:\n\nThe subject is in format logs.\u003capp_name\u003e.\u003cregion\u003e.\u003cinstance_id\u003e. An example SUBJECT to only log an application named sandwich (no matter what region it‚Äôs in) is:\n\nSee that greater-than symbol \u003e? That‚Äôs a NATS wildcard. There are also regular wildcards *, but the special wildcard \u003e is used at the end of the string to say ‚Äúand anything to the right of this‚Äù.\n\nSo, our use of \"logs.sandwich.\u003e\" says to ship any logs that are from application sandwich, no matter what region or instance they come from. You can (ab)use this to get the logs you‚Äôre interested in.\n\nGo forth and ship logs!"
  },
  {
    "title": "You can play with this stuff right now.",
    "url": "https://fly.io/blog/carving-the-scheduler-out-of-our-orchestrator/",
    "content": "We‚Äôre Fly.io, a global sandwich-rating company with a hosting problem. Even if you don‚Äôt have a sandwich to rate, you might benefit from the hosting platform we built. Check it out: with a working Docker image, you can be up and running on Fly.io in just minutes.\n\nSo, you want to build an app to rate sandwiches. Well, the world has a lot of different sandwiches. Pit beefs in Baltimore, Tonkatsu sandos in Shinjuku, and Cemitas in Puebla. You want real-time sandwich telemetry, no matter the longitude of the sandwich. So you need to run it all over the world, without a lot of ceremony.\n\nWe built one of those at Fly.io. We‚Äôve written a bunch about one important piece of the puzzle: how we take Docker images from our users and efficiently run them as virtual machines. You can run a Docker image as VM. You‚Äôre almost done! Time to draw the rest of the owl.\n\nTo turn our Docker transmogrification into a platform, we need to go from running a single job to running hundreds of thousands. That‚Äôs an engineering problem with a name:\n\n\n## Orchestration\n\nOrchestrators link clusters of worker servers together and offer up an API to run jobs on them. Kubernetes is an orchestrator; the Kleenex of orchestrators. Then, HashiCorp has Nomad, which we use, and about which more in a bit.\n\nFind a serverside developer complaining about how much harder it is to deliver an app in 2023 than it was in 2005, and odds are, they‚Äôre complaining about orchestration. They‚Äôre not wrong: Kubernetes is fractally complicated. But the idea isn‚Äôt.\n\nLet‚Äôs write an orchestrator. Start by writing a supervisor.\n\nI believe this design is so powerful it does not need to be discussed.\n\nA build-your-own-light-saber tool if ever there was one.\n\nThere are, like, 100 different supervisors. You can write a program to run a shell command, you can write a supervisor. Come on. You‚Äôve already written a supervisor. Let‚Äôs stop kidding each other.\n\nLet‚Äôs turn ours into an orchestrator.\n\nFor illustrative purposes, our supervisor takes a JSON configuration:\n\nInstead of reading this configuration from a file, like a dumb old supervisor, read it from an HTTP API, like a majestic orchestrator. ‚ÄúWorkers‚Äù run our simple supervisor code, and a server doles out tasks. Here‚Äôs an API:\n\nThe server implementing this is an exercise for the reader. Don‚Äôt overthink it .\n\nWorkers poll /jobs. They /claim them by name. The server decides which claim wins, awarding it a 200 HTTP response. The worker runs the job, until it stops, and posts /release.\n\nEnd-users drive the orchestrator with the same API; they post JSON tasks to /submit, check to see where they‚Äôre running, kill them by name with /cancel. Workers poll /cancellations to see what to stop running.\n\nThere. That‚Äôs an orchestrator. It‚Äôs just a client-server process supervisor.\n\nI see a lot of hands raised in the audience. I‚Äôll take questions at the end. But let‚Äôs see if I can head some of them off:\n\nYou there in the back hollering‚Ä¶ this isn‚Äôt a real orchestrator, why? Oh, because we‚Äôre not\n\n\n## Scheduling\n\nScheduling means deciding which worker to run each task on.\n\nScheduling is to an orchestrator what a routing protocol is to a router: the dilithium crystal, the contents of Marcellus Wallace‚Äôs briefcase, the thing that, ostensibly, makes the system Difficult.\n\nIt doesn‚Äôt have to be hard. Assume our cluster is an undifferentiated mass of identical workers on the same network. Decide how many jobs a worker can run. Then: just tell a worker not to bid on jobs when it‚Äôs at its limit.\n\nBut no mainstream orchestrator works this way. All of them share some notion of centralized scheduling: an all-seeing eye that allocates space on workers the way a memory allocator doles out memory.\n\nEven centralized scheduling doesn‚Äôt complicate our API that much.\n\nInstead of rattling off all the available jobs and having workers stampede to claim them, our new API assigns them directly. Easier for the workers, harder for the server, which is now obligated to make decisions.\n\nHere‚Äôs the rough outline of a centralized scheduler:\n\nThe textbook way to rank viable workers is ‚Äúbin packing‚Äù. Bin packing is a classic computer science problem: given a series of variably-sized objects and fixed-size containers, fit all the objects in the smallest number of containers. The conventional wisdom about allocating jobs in a cluster is indeed that of the clown car: try to make servers as utilized as possible, so you can minimize the number of servers you need to buy.\n\nSo far, the mechanics of what I‚Äôm describing are barely an afternoon coding project. But real clusters tend to run Kubernetes. Even small clusters: people run K8s for apps like ratemysandwich.com all the time. But K8s was designed to host things like all of Google. So K8s has fussy scheduling system.\n\nTo qualify as ‚Äúfussy‚Äù, a scheduler needs at least 2 of the following 3 properties:\n\nThese tenets of fussiness hold true not just for K8s, but for all mainstream orchestrators, including the one we use.\n\nThe Mickensian aspect.\n\n\n## Nomad\n\nLet‚Äôs start by reckoning with what‚Äôs going on with Kubernetes.\n\nThe legends speak of a mighty orchestrator lurking within the halls of Google called ‚ÄúBorg‚Äù. Those of us who‚Äôve never worked at Google have to take the word of those who have that Borg actually exists, and the word of other people that K8s is based on the design of Borg.\n\nThe thing about Borg is that, if it exists, it exists within an ecosystem of other internal Google services. This makes sense for Google the same way having S3, SQS, ECS, Lambda, EBS, ALBs, CloudWatch, Cognito, EFS, RedShift, Route53, Glacier, SNS, VPC, Certificate Manager, QFA, IAM, KMS, CodeCommit, OpsWorks, Cloudformation, Snowball, X-Ray, Price List Marketplace Metering Service Entitlement Modernization, and EC2 does for AWS. Like, somewhere within Google there‚Äôs a team that‚Äôs using each of these kinds of service.\n\nYou can‚Äôt argue with the success Kubernetes has had. I get it.\n\nIt makes less sense for a single piece of software to try to wrap up all those services. But Kubernetes seems to be trying. Here‚Äôs some perspective: K8s is, some people say, essentially Borg but with Docker Containers instead of Midas packages. Midas is neat, but it in turn relies on BigTable and Colossus, two huge Google services. And that‚Äôs just packages, the lowest level primitive in the system. It‚Äôs an, uh, ambitious starting point for a global open source standard.\n\nAt any rate, our customers want to run Linux apps, not Kubernetes apps. So Kubernetes is out.\n\nSometime later, a team inside Google took it upon themselves to redesign Borg. Their system was called Omega. I don‚Äôt know if it was ever widely used, but it‚Äôs influential. Omega has these properties:\n\nHashicorp took Google‚Äôs Omega paper and turned it into an open source project, called Nomad.\n\nOmega‚Äôs architecture is nice. But the real win is that Nomad is lightweight. It‚Äôs conceptually not all that far from the API we designed earlier, plus Raft.\n\nNomad can run Unix programs directly, or in Docker containers. We do neither. Not a problem: Nomad will orchestrate jobs for anything that conforms to this interface:\n\nFor the year following our launch, Fly.io‚Äôs platform was a Rust proxy and a Golang Nomad driver. The driver could check out a Docker image, convert it to a block device, and start Firecracker on it. In return for coding to the driver interface, we got:\n\nAbout Nomad itself, we have nothing but nice things to say. Nomad is like Flask to K8s‚Äôs Django, Sinatra to K8s‚Äôs Rails. It‚Äôs unopinionated, easy to set up, and straightforward to extend. Use Nomad.\n\nAnother very cool system to look at in this space is Flynn. Flynn was an open source project that started before Docker was stable and grew up alongside it. They set out to build a platform-as-a-service in a box, one that bootstraps itself from a single-binary install. It does so much stuff! If you‚Äôve ever wondered what all the backend code for something like Fly.io must be like (multiple generations of schedulers and all), check out what they did.\n\nBut we‚Äôve outgrown it, because:\n\nBin packing is wrong for platforms like Fly.io. Fussy schedulers are premised on minimizing deployed servers by making every server do more. That makes a lot of sense if you‚Äôre Pixar. We rent out server space. So we buy enough of them to have headroom in every region. As long as they‚Äôre running, we‚Äôd want to use them.\n\nHere‚Äôs a Google presentation on the logic behind Nomad‚Äôs first-fit bin packing scheduler. It was designed for a cluster where 0% utilization was better, for power consumption reasons, than \u003c 40% utilization. Makes sense for Google. Not so much for us.\n\nWith strict bin packing, we end up with Katamari Damacy scheduling, where a couple overworked servers in our fleet suck up all the random jobs they come into contact with. Resource tracking is imperfect and neighbors are noisy, so this is a pretty bad customer experience.\n\nNomad added a ‚Äúspread scheduling‚Äù option, which just inverts the bin pack scoring they use by default. But that‚Äôs not necessarily what we want. What we want is complicated! We‚Äôre high-maintenance! In a geographically diverse fleet with predictable usage patterns, the best scheduling plans are intricate, and we don‚Äôt want to fight with a scheduler to implement them.\n\nWe Run One Global Cluster. This isn‚Äôt what Nomad expects. Nomad wants us to run a bunch of federated clusters (one in Dallas, one in Newark, and so on).\n\nThere are two big reasons we don‚Äôt federate Nomad:\n\nWe Outgrew The Orchestration Model. Nomad scheduling is asynchronous. You submit a job to a server. All the servers convene a trustees meeting, solicit public comment, agree on the previous meeting‚Äôs minutes, and reach consensus about the nature of the job requested. A plan is put into motion, and the implicated workers are informed. Probably, everything works fine; if not, the process starts over again, and again, until seconds, minutes, hours, or days later, it does work.\n\nThis is not a bad way to handle a flyctl deploy request. But it‚Äôs no way to handle an HTTP request, and that‚Äôs what we want: for a request to land at our network edge in S√£o Paulo, and then we scale from zero to handle it in our GRU region, starting a Fly Machine on a particular server, synchronously.\n\nThe Fly.io step in there that costs the most is pulling containers from registries. People‚Äôs containers are huge! That makes the win from caching large ‚Äì¬†and just not captured by the Nomad scheduler. Nomad autoscaling is elegant, and just not well matched to our platform. How the autoscaler works is, it takes external metrics and uses them to adjust the count constraint on jobs. We scrape metrics every 15 seconds, and then Nomad‚Äôs scheduling work adds a bunch of time on top of that, so it never really worked effectively.\n\nAt this point, what we‚Äôre asking our scheduler to do is to consider Docker images themselves to be a resource, like disk space and memory. The set of images cached and ready to deploy on any given server is changing every second, and so are the scheduling demands being submitted to the orchestrator. Crazy producers. Crazy consumers. It‚Äôs a lot to ask from a centralized scheduler.\n\nSo we built our own, called\n\n\n## N√ºmad\n\nI would also accept ‚Äúnonomad‚Äù, ‚Äúyesmad‚Äù, ‚Äúno, mad‚Äù, and ‚Äúfauxmad‚Äù for this dad joke.\n\nJust kidding, we call it flyd.\n\nThere is a long and distinguished literature of cluster scheduling, going back into the 1980s. We decided not to consult it, and just built something instead.\n\nflyd has a radically different model from Kubernetes and Nomad. Mainstream orchestrators are like sophisticated memory allocators, operating from a reliable global picture of all capacity everywhere in the cluster. Not flyd.\n\nInstead, flyd operates like a market. Requests to schedule jobs are bids for resources; workers are suppliers. Our orchestrator sits in the middle like an exchange. ratemysandwich.com asks for a Fly Machine with 4 dedicated CPU cores in Chennai (sandwich: bun kebab?). Some worker in MAA offers room; a match is made, the order is filled.\n\nOr, critically: the order is not filled. That‚Äôs fine too! What‚Äôs important is that the decision be made quickly, so that it can be done synchronously. What we don‚Äôt want is a pending state waiting for the weather to clear up.\n\nOur system has a cast of three characters:\n\nThe engine of this system is flyd.\n\nIn Nomad-land, our Firecracker driver doesn‚Äôt keep much state. That‚Äôs the job of huge scheduling servers, operating in unlighted chambers beyond time amidst the maddening beating and monotonous whine of the Raft consensus protocol.\n\nUnlike Nomad, which goes through some effort to keep the entire map of available resources in the cluster in memory, nothing in flyd is cached; everything is just materialized on-demand from disk.\n\nIn flyd-land, state-keeping is very much the worker‚Äôs problem. Every worker is its own source of truth. Every flyd keeps a boltdb database of its current state, which is an append-only log of all the operations applied to the worker.\n\nflyd is rigidly structured as a collection of state machines, like ‚Äúcreate a machine‚Äù or ‚Äúdelete a volume‚Äù. Each has a concrete representation both in the code (using Go generics) and in boltdb. Everything happening in flyd (in logs, traces, metrics or whatever) happens at a particular state for a particular resource ID. Easy to reason about. And, of course, if we bounce flyd, it picks up right where it left off.\n\nflyd operates off of a local boltdb database, but our platform also has an SQLite view of all the resources allocated systemwide. We built it by caching Consul, but, in keeping with our ethos of ‚Äúif you see Raft anywhere, something went wrong‚Äù, we‚Äôve replaced it with something simpler. We call it Corrosion. Corrosion is what would happen if you looked at Consul, realized every server is its own source of truth and thus distributed state wasn‚Äôt a consensus problem at all but rather just a replication problem, built a SWIM gossip system, and made it spit out SQLite. Also you decided it should be written in Rust. Corrosion is neat, and we‚Äôll eventually write more about it.\n\nAll the flyd instances in (say) Madrid form a MAD cluster. But it‚Äôs not a cluster in the same sense Nomad or K8s uses: no state is shared between the flyd instances, and no consensus protocol runs.\n\nTo get jobs running on a flyd in MAD, you talk to flaps. flaps is running wherever you are (in my case, ORD).\n\nflaps uses Corrosion to find all the workers in a particular region. It has direct connectivity to every flyd, because our network is meshed up with WireGuard. flyd exposes an internal HTTP API to flaps, and flaps in turn exposes this API:\n\n‚ÄúCreating‚Äù a Fly Machine reserves space on a worker in some region.\n\nTo reserve space in Sydney, flaps collects capacity information from all the flyds in SYD, and then runs a quick best-fit ranking over the workers with space, which is just a simple linear interpolation rankings workers as more or less desirable at different utilizations of different resources.\n\n‚Ä† (and, in the future, the cartesian of regions cross new hardware products, like space modulator coprocessors)\n\nRather than forming distributed consensus clusters, Fly.io regions like MAD and SYD‚Ä† are like products listed on an exchange. There are multiple suppliers of MAD VMs (each of our workers in Madrid) and you don‚Äôt care which one you get. flaps act like a broker. Orders come in, and we attempt to match them. flaps does some lookups in the process, but it doesn‚Äôt hold on to any state; the different flaps instances around the world don‚Äôt agree on a picture of the world. The whole process can fail, the same way an immediate-or-cancel order does with a financial market order. That‚Äôs OK!\n\nHere‚Äôs what doesn‚Äôt happen in this design: jobs don‚Äôt arrive and then sit on the book in a ‚Äúpending‚Äù state while the orchestrator does its best to find some place, any place to run it. If you ask for VMs in MAD, you‚Äôre going to get VMs in MAD, or you‚Äôre going to get nothing. You won‚Äôt get VMs in FRA because the orchestrator has decided ‚Äúthat‚Äôs close enough‚Äù. That kind of thing happened to us all the time with Nomad.\n\n\n## Scheduling, Reconsidered\n\nIf you‚Äôre a certain kind of reader, you‚Äôve noticed that this design doesn‚Äôt do everything Fly Apps do. What happens when an app crashes? How do we deploy across a bunch of regions? How does a rollback work? These are problems Nomad solved. It doesn‚Äôt look like flaps and flyd solve them.\n\nThat‚Äôs because they don‚Äôt! Other parts of the platform ‚Äî most notably, flyctl, our beloved CLI ‚Äî take over those responsibilities.\n\nFor example: how do we handle a crashed worker? Now, flyd will restart a crashed VM, of course; that‚Äôs an easy decision to make locally. But some problems can‚Äôt be fixed by a single worker. Well, one thing we do is: when you do a deploy, flyctl creates multiple machines for each instance. Only one is started, but others are prepped on different workers. If a worker goes down, fly-proxy notices, and sends a signal to start a spare.\n\nWhat we‚Äôre doing more generally is carving complex, policy-heavy functionality out of our platform, and moving it out to the client. Aficionados of classic papers will recognize this as an old strategy.\n\nNetworks, boy I tell ya.\n\nWhat we had with Nomad was a system that would make a lot of sense if we were scheduling a relatively small number of huge apps. But we schedule a huge number of relatively small apps, and the intelligent decisions our platform made in response to stimuli were often a Mad Hatter‚Äôs tea party. For instance: many times when Europe lost connectivity to us-east-1 S3, apps would flake, and Nomad would in response cry ‚Äúchange places!‚Äù and reschedule them onto different machines.\n\nWhat we‚Äôve concluded is that these kinds of scheduling decisions are actually the nuts and bolts of how our platform works. They‚Äôre things we should have very strong opinions about, and we shouldn‚Äôt be debating a bin packer or a constraint system to implement them. In the new design, the basic primitives are directly exposed, and we just write code to configure them the way we want.\n\nInternally, we call this new system ‚ÄúAppsV2‚Äù, because we‚Äôre good at naming things. If you‚Äôre deploying an app in January of 2023, you‚Äôre still using Nomad; if you‚Äôre deploying one in December of 2023, you‚Äôll probably be interacting with flyd. If we do it right, you mostly won‚Äôt have to care.\n\nThe Fly Machines API runs on flyd and reserves, starts, and stops individual VMs.\n\n\n## Drawing Most Of The Owl\n\nOver the last couple years, we‚Äôve written about most of the guts of Fly.io:\n\nIt took us awhile, but we‚Äôre glad to have finally written down our thoughts about one of the last remaining big pieces. With an execution engine, a control plane, and an orchestrator, you‚Äôve got most of our platform! The only huge piece left is fly-proxy, which we have not yet done justice.\n\nWe hope this is interesting stuff even if you never plan on running an app here (or building a platform of your own on top of ours). We‚Äôre not the first team to come up with a bidding-style orchestrator ‚Äî they‚Äôre documented in that 1988 paper above! But given an entire industry of orchestrators that look like Borg, it‚Äôs good to get a reminder of how many degrees of freedom we really have."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-in-johannesburg/",
    "content": "Fly.io is busily adding servers in new regions, but here‚Äôs one we prepared earlier. Launch a full-stack app in Johannesburg, South Africa! It‚Äôs easy to get started.\n\nDid you know that we‚Äôre in Johannesburg? There‚Äôs rugby and cricket. Hearty kota and Gatsby sandwiches. Braii under sunny skies and low-latency full-stack apps. Front end, Postgres, Redis, the works: if your users support the Springboks and Banyana Banyana, you should put your whole app in JNB."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/wal-mode-in-litefs/",
    "content": "LiteFS is a distributed file system that magically replicates your SQLite databases. Make an update on one server and, voil√†, your change is instantly available to your other servers on the edge. Take a look at our Getting Started guide to see how to add LiteFS to your application.\n\nBy and large, SQLite is configuration-free. You can get pretty far by just using the default settings. As your application grows and you start tweaking settings, one of the first knobs you‚Äôll come across is the journal mode. This setting determines how SQLite performs transactions on disk and there are essentially two modes: the rollback journal \u0026 the write-ahead log, or WAL.\n\nThe rollback journal was the original transaction mechanism and it‚Äôs still the default. The WAL mode is the shiny new transaction mode. If you start reading blog posts \u0026 forums about SQLite, one tip you will repeatedly hear is, ‚Äúuse WAL mode!‚Äù\n\nIf your database is slow, you should use the WAL mode.\n\nIf you have concurrent users, you should use the WAL mode.\n\nWAL mode. WAL mode. WAL mode.\n\nIn the SQLite world, the write-ahead log is as close to a silver bullet as you can find. It‚Äôs basically magic fairy dust that makes your database better and you should almost always use it.\n\nHowever, LiteFS, our distributed SQLite file system, only supported the rollback journal mode. Until now! With the release of LiteFS v0.3.0, we now support all journaling modes.\n\n\n## Quick primer on journal modes\n\nWe‚Äôve written about the internals of the rollback journal and the WAL mode in previous posts, but here‚Äôs a refresher.\n\nWith the rollback journal, SQLite:\n\nBecause the pages in the database file are moving around and being deleted, this mode does not allow read transactions \u0026 write transactions to occur at the same time.\n\nThe WAL works the opposite way:\n\nSince the original data is never changed during the transaction, readers can continue running in parallel while another process is writing to the database. In addition to improved concurrency, the WAL also tends to have better write performance.\n\n\n## Databases as a history of change sets\n\nMost developers think of databases as just a collection of tables \u0026 rows. And that‚Äôs how you should view it when you‚Äôre building an application. However, when designing database tooling like LiteFS, it‚Äôs better to think in terms of change sets.\n\nA good analogy is baseball card collections. You might start off buying a pack of cards to start your collection. Over time, you may buy more packs or you might trade cards with friends. Each of these actions is a ‚Äúchange set‚Äù, adding and/or removing a set of cards from your collection.\n\nEventually, word gets out about your sweet baseball card collection and your friends want to have the same set. So each time you make a change, you send each friend a list of which cards were added and removed so they can update their collections. Now everyone has the same collection just by communicating change sets.\n\nThat, in a nutshell, is how LiteFS nodes keep distributed copies of your database in sync. However, instead of baseball cards, these LiteFS nodes communicate change sets of fixed-sized blocks called pages.\n\nSQLite applies these change sets of pages safely \u0026 atomically by using either a rollback journal or the write-ahead log. These two methods have a different approach but, at the end of they day, they both transactionally update a set of pages in a SQLite database.\n\nIn LiteFS, we track the beginning and end of these transactions through the file system API. We can see which pages have changed and bundle them up in an internal file format called LTX.\n\n\n## Detecting page sets with the rollback journal\n\nThe rollback journal is a simple mechanism, which makes it easy for LiteFS to determine when write transactions start \u0026 end. From a high-level, SQLite implements transactions like this:\n\nLiteFS acts as a passthrough file system so it can see all these file system calls. On the initial journal creation, it begins watching for page changes. On write(2), it marks a page as changed. And finally, on unlink(2) it will copy the page change set to an LTX file and then delete the journal.\n\n\n## Detecting page sets with the WAL\n\nSQLite‚Äôs operations when it uses the WAL mode are a bit more complicated but it still has similar start \u0026 end triggers.\n\nLiteFS can read the list of changed pages from the WAL and copy them out to an LTX file when the final WAL write for the transaction comes in. Again, both the rollback journal and WAL are implementation details so we end up with the same LTX format with either one.\n\nIn the WAL mode, SQLite will also maintain a shared-memory file (aka SHM) and uses it as an index to look up pages in the WAL. This piece is managed by SQLite so LiteFS doesn‚Äôt touch it during a write.\n\n\n## Applying transactions to the replica\n\nOnce an LTX file is created on the primary LiteFS node, it will send it to all connected replica LiteFS nodes. These replicas will validate the file, perform some consistency checks, and then apply the change set to the SQLite database.\n\nThe LiteFS replica imitates a SQLite client and takes the same locks in order to apply the transaction. That means it looks like just another SQLite client doing an update so it‚Äôs safe across other processes using the database.\n\n\n## Bootstrapping made easy\n\nPreviously, it was tough to convert an existing application to use LiteFS. You‚Äôd need to create a SQL dump of your database and import in using the sqlite3 command line. That was a pain.\n\nWe‚Äôve improved this workflow with the new litefs import command. This command lets you remotely send a SQLite database to your LiteFS cluster and it will transactionally replace it. That means you can start a cluster with an existing database or you can even revert to an old snapshot on a live application.\n\n\n## Reworking checksumming\n\nLiteFS uses a fast, incremental checksum for ensuring the state of the entire database is consistent across all nodes at every transaction. The method is simple: we XOR the CRC64 checksums of every page in the database together. This approach let us incrementally update individual pages by XOR'ing out the old checksum for a page and XOR'ing in the new checksum for the page. That‚Äôs pretty cool.\n\nHowever, in practice, it was difficult to ensure we were calculating the correct previous checksum for a page every time we performed an update as page data is spread across the database file, journal file, \u0026 WAL file. The edge cases for determining the previous page data were too easy to get wrong.\n\nSo in v0.3.0, we decided to rework the database checksum. It still uses the same algorithm of XOR'ing page checksums but now we maintain a map of the current checksum of every page in the database so they can be XOR‚Äôd together on commit. We no longer need to track the previous checksum and this change made a lot of edge cases disappear.\n\nThis approach is not without its trade-offs though. First, it requires additional memory. The map keys are 4-byte unsigned integers and the values are 8-byte hash values so we need about 12 bytes per page. SQLite uses 4KB pages by default so that‚Äôs 262,144 pages per gigabyte. Our total memory overhead for our map of page hashes ends up being about 3MB of RAM per gigabyte of on-disk SQLite database data. LiteFS targets database sizes between 1 to 10 GB so that seemed like a reasonable trade-off.\n\nSecond, this approach adds CPU overhead after each commit. Map iteration and XOR computation are quite fast but these do begin to show up in performance profiles as the database grows. In our tests, we‚Äôve found it adds about 5ms per gigabyte of SQLite data. That‚Äôs pretty high. Fortunately, much of this iteration can be cached since XORs are associative. We‚Äôll be implementing this cache in the next version of LiteFS.\n\n\n## Improving debugging with the trace log\n\nOne benefit to having checksum bugs in v0.2.0 was that it gave us plenty of time to get our hands dirty with debugging. The best tools come out of necessity and the LiteFS trace log is one of those tools.\n\nDebugging a failed database or distributed system is a bit like a murder mystery in that you know how it ended but you need to put the pieces together to figure out how it happened.\n\nIn the previous version of LiteFS, we didn‚Äôt have many clues when one of these failures happened so it required a Sherlock Holmes level of deductive reasoning to figure out the mystery. The trace log simplifies this process by writing out every internal event to a log file so we can see where things went awry after the fact.\n\nSQLite uses the POSIX file system API so debugging with a normal strace would look like a series of seemingly opaque system calls. LiteFS translates these system calls back into SQLite related actions such as WriteDatabase() or LockSHM(). When we write those events to the trace log, we can decorate the log lines with additional information such as page numbers and checksums. All this makes reading the trace much more straightforward.\n\nThe trace log is not without its costs though. It will increase I/O to your disk as there are a lot of events that are written. It‚Äôs typical to see your disk I/O double when you enable the trace log. However, it does cap the total size of the trace log by using a rolling log so you don‚Äôt need much space available. By default, it will roll over to a new log file every 64MB and it will retain the last 10 logs in a gzipped format.\n\nThe trace log is disabled by default, however, you review the trace log documentation if you need it to debug any LiteFS issues.\n\n\n## Upcoming work\n\nThe WAL support \u0026 stability improvements have been huge steps in moving LiteFS to be production ready but there‚Äôs still more work to come. In the next release, we‚Äôll be focused on making LiteFS easier to integrate into your application by adding support for write forwarding. That will let you write to your database from any node and have LiteFS automatically forward those writes to the primary instead of having your application redirect writes.\n\nWe‚Äôll also be making performance improvements by adding LZ4 compression to the LTX files. This will reduce latency between nodes and it will significantly cut down on bandwidth costs.\n\n\n## Thank you!\n\nFinally, we‚Äôd like to give a huge shoutout for everyone who has tried LiteFS and given feedback. It makes a world of difference! Kent C. Dodds even live streamed his experience with LiteFS and it gave us incredible, detailed feedback. Thank you!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/launching-redis-by-upstash/",
    "content": "We‚Äôre Fly.io. We put your code into lightweight microVMs on our own hardware around the world, close to your users. Redis by Upstash is managed Redis living right next door to your Fly.io apps. Check us out‚Äîyour app and database can be running close to your users within minutes.\n\nWe love databases that scale globally. As an ambivalent database provider, we built a global, automated Postgres, and we tinkered with global Redis on scrappy startup weekends. But the Fly.io forecast called for integration over invention. So we partnered up on launching a simple, global, low-latency Redis service built by the intrepid crew at Upstash.\n\nRedis by Upstash sounds good enough to launch a cologne. We think it‚Äôs as big a deal. Oh, and there‚Äôs a generous free tier.\n\nKeep reading to learn how our first integration came to life. Or, just sign up for Fly and give it a try:\n\n\n## A Better Redis for Global Deployments\n\nSo what‚Äôs special here? I assure you: this isn‚Äôt stock Redis with a price tag slapped on.\n\nComplex features like global read replicas demand good DX to get noticed. But in the managed Redis market, read replicas are elusive, hidden behind sales calls, enterprise pricing plans and confusing UI.\n\nWith flyctl redis update and a few keystrokes, you can spin up global Redis replicas in seconds, with write forwarding switched on. Reads and writes make their way to the geographically-nearest replica, which happily forwards writes along to its primary, ensuring read-your-write consistency along the way. So, with a single Redis URI, you can safely experiment with global deployment without changing your app configuration.\n\nVM-to-Redis requests are reliably fast, in every region, because your apps run on the same bare metal hardware as your databases, one network hop away at most. Check out Upstash‚Äôs live latency measurements to compare Fly.io with serverless platforms like Vercel or AWS. This comparison is not entirely fair, as we run apps on real VMs; not in JavaScript isolates. But we love the colors.\n\nFinally, it‚Äôs worth mentioning these databases are secure: only reachable through your Fly.io encrypted, private IPv6 network.\n\n\n## Like a Surgeon\n\nWhen this integration was on the cards, we had two clear goals: don‚Äôt expose Redis to the internet, and give Upstash full control of their service without compromising customer app security. Serendipity struck as we pondered this.\n\nWe were knee-deep in fresh platform plumbing ‚Äî the Machines API and Flycast private load balancing. The API grants precise control over where and how VMs launch. And Flycast yields anycast-like powers to apps on the private, global WireGuard mesh.\n\nSo Upstash Redis is a standard Fly.io app ‚Äî a multitenant megalith running on beefy VMs in all Fly.io regions. These VMs gossip amongst themselves over their private IPv6 network. Upstash uses our API to deploy. We support Upstash like any other customer. Awesome.\n\nBut Redis runs in its own Fly.io organization, and therefore, in its own isolated network. And customer apps, each in their own. We needed a way to securely connect two Fly applications. Enter Flycast, stage left.\n\nFlycast is a beautiful, complex cocktail of BPF, iptables and tproxy rules: fodder for another post! Flycast offers public proxy features ‚Äî geo-aware load balancing, concurrency control and TLS termination ‚Äî between apps that share a private network. With a small tweak, Flycast could now surgically join services with customer networks.\n\nCustomer apps can connect to their provisioned Redis, but not to anything else in the Upstash private network. Upstash can‚Äôt access the customer‚Äôs network at all. Mission accomplished.\n\n\n## A Tale of Provisioning\n\nYou might be curious how provisioning Redis works, end-to-end.\n\nYour flyctl redis create hits the Fly.io API. We mint a fresh Flycast IP address on your network and pass that IP along to Upstash‚Äôs API with the desired database configuration.\n\nIn the same request, Upstash informs their Fly.io mega-deployment about your database, and we (Fly.io) point the Flycast address at Upstash‚Äôs app. We blast this info to our global proxies. They‚Äôll now proxy connections on this IP to the nearest healthy mega-Redis instance. This all happens in a matter of seconds.\n\nAlright, so now you have a Redis connection URL to chuck requests at.\n\nRemember that Upstash‚Äôs Redis deployment is multitenant. Upstash hosts scores of customer databases within a single OS process. With a clever shuffling of data from RAM to persistent disks, many, many more databases can fit in this instance than your average Redis running on its own VM.\n\nBut multitenancy poses a problem. How can mega-Redis identify the target database for a given request?\n\nYour Redis URL includes a unique database password (remember this is all private, encrypted traffic). Could we use this password to identify your database? Technically, yes, but if you leak your Redis password on a live coding stream, anyone else with a Redis database could hijack yours! Yeah, let‚Äôs not.\n\nBefore, we passed your Flycast IP address to Upstash, so they have it on record. Could they match that against the source address of the incoming Redis TCP connection? Not quite! Connections to Redis pass through our proxy. So, traffic will appear to arrive from the proxy itself; not from your Flycast IP.\n\nNo worries! We‚Äôve got another trick up our sleeve.\n\n\n## A Protocol for Proxies\n\nBonus: our proxy supports prepending proxy procotol headers to TCP requests.\n\nThis curious 10-year-old internet resident is understood by most web servers and programming languages. At the top of the protocol spec, we spot our problem:\n\nRelaying TCP connections through proxies generally involves a loss of the original TCP connection parameters such as source and destination addresses, ports, and so on.\n\nRedis runs on port 6379, just because. Here‚Äôs a typical header for Redis connection initiation:\n\nPROXY TCP6 fdaa:0:47fb:0:1::19 fdaa:0:47fb:0:1::16 6379 6379\n\nHere we have two IPs ‚Äî source and destination ‚Äî on the same lovingly-named network, fdaa:0:47fb. The source IP belongs to the application VM, which is assigned randomly and is of little use here. But the destination address is the Flycast IP assigned to our particular database. Ace.\n\nNow we‚Äôre in the home stretch. Redis parses this header, plucks out that Flycast IP, finds the associated customer database, and forwards traffic to it. In wafts the sweet aroma of victory.\n\n\n## A Need for Speed\n\nLet‚Äôs talk about a clear-cut use case for global Redis: caching HTML at the edge.\n\nLast year we turbo-boosted our Paris-based, recipe finder Rails app by deploying Postgres replicas around the globe. But our database has grown. We don‚Äôt need to replicate all of its contents, and we‚Äôre too busy to spend time optimizing our queries. Let‚Äôs just lean on a lightweight HTML cache, which Rails is good at.\n\nWe know we can get similar or better performance by caching HTML in Redis alongside our deployed VMs. And we can do this in a few minutes, really. First, let‚Äôs add a few read replicas in distant, exotic lands.\n\nThen, with a sprinkle of Rails magic, our naive HTML cache is on the scene. Metrics can be boring, so, trust us that our Time To First Byte is still in the low milliseconds, globally, for GET requests on cached recipe pages.\n\n\n## RYOW\n\nNow and then, one must write. And read-your-own-write consistency is a thing you need to care about when hitting speed-of-light latency in global deployments. That‚Äôs life, kids.\n\nReaders hitting database replicas may not be served the very freshest of writes. We‚Äôre OK with that. Except in one case: when that replica is serving the author of the write. Good UX demands that a writer feel confident about the changes they‚Äôve made, even if they have to wait a few hundred milliseconds.\n\nTo that end, Upstash Redis replicas take one of two paths to ensure a consistent read-your-own-write experience, with some trade-offs. Let‚Äôs talk it out.\n\nIsa ‚Äî one our recipe editors in Santiago ‚Äî is worried that the recipe for Humitas Chilenas mentions New Mexico Green Chiles. While they may be the first chiles grown in outer space, they‚Äôre generally not tossed into humitas. So she makes corrections and proudly smashes that ENVIAR button.\n\nMeanwhile, Santiago Redis has been diligently keeping track of the unique IDs of the writes that pass through Isa‚Äôs Redis connection.\n\nSo, that write is forwarded on to Paris, securely, over the WireGuard mesh. Santiago Redis holds blocks on the write command, waiting for replication to catch up to this specific write. On a clear internet day, we might wait 150ms, and Isa is redirected to the recipe page and sees her updated recipe sans chiles.\n\nBut under poor network conditions, we may need to wait longer, and we don‚Äôt want to wait forever. Editing must go on. This kind of thing can happen, and we need to be prepared for it.\n\nSo, the less happy path: Santiago Redis waits up to 500ms for the written value to return via replication. After that, Redis client connection is released, suggesting to the Redis client that the write completed. Now, this is risky business. If we redirect Isa to her recipe before her write makes that round trip, she gets spicy Humitas once again. New Mexican space chiles haunt her confused mind.\n\nNo fear - Santiago Redis has our back. Remember that it was tracking writes? When Isa‚Äôs recipe read is attempted, Santiago grabs the ID of the most recently tracked write on her connection. It checks to see if that ID exists in the replicated database contents. If so, Isa gets a fast, correct read of her updated recipe.\n\nBut if her change didn‚Äôt arrive yet, Santiago forwards the read operation to our our source of truth ‚Äî Paris Redis ‚Äî at the cost of another full round trip to Europe. Such is the price of consistency."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-november-14-to-december-5-2022/",
    "content": "Build real-time applications on any backend with Replicache and Fly, run Cron on Fly, two LiveWire how-tos, and learn how Fly reluctantly built its Postgres database service.\n\nOk, it‚Äôs been longer than a week since the last update because a lot of us at Fly were enjoying some time with ü¶É, ü•ß, and üë®‚Äçüë©‚Äçüëß‚Äçüë¶. Let‚Äôs get to it!\n\n\n## Real-Time Collaboration With Replicache and Fly-Replay\n\nDov Alperin wires up Replicache to WebSockets to show how any framework and Fly can be used to build realtime web applications. Check it out if you aspire to build the next Figma.\n\nRead Real-Time Collaboration With Replicache and Fly-Replay\n\n\n## Laravel LiveWire\n\nThe Laravel team at Fly continues to crank out some pretty great tutorials.\n\n\n## Streaming to the Browser With LiveWire\n\nUse WebSockets to stream content from the server to a persons web browser. Chris Fidao walks through an example that shows how a log file could be streamed from a server to anybody watching it from a browser.\n\nRead Streaming to the Browser with LiveWire\n\n\n## Offloading Data Baggage with LiveWire\n\nWhen paginating large datasets between the server and browser, you don‚Äôt want to load so much data that the users browser slows down and becomes unresponsive. Kathryn Anne Tan shows how this data can be unloaded so that the people using your website don‚Äôt have to deal with all your baggage.\n\nRead Offloading Data Baggage with LiveWire\n\n\n## Supercronic on Fly\n\nIn September, Fly Machines got a way to run tasks periodically either ‚Äúmonthly‚Äù, ‚Äúweekly‚Äù, ‚Äúdaily‚Äù, and ‚Äúhourly; however, there is no way to control the precise time those jobs run because it‚Äôs a hard problem to solve at scale.\n\nNow there‚Äôs a guide for those who need more precise control over cron that runs on both versions of the Fly Apps platform. Brad Gessler runs through how to wire up Superchronic in your Fly Dockerfiles and deploy cron to production.\n\nRead Supercronic on Fly\n\n\n## Postgres on Fly\n\nDid you know that Fly actually wants you to use off-platform database services, like RDS, CrunchyData, or PlanetScale? Well then how the heck did we end up building Fly Postgres!? Chris Nicoll and Shaun Davis walk down memory lane and chronicle how it all happened.\n\nRead How We Built Fly Postgres\n\nP.S. If you‚Äôre CrunchyData, PlanetScale, or a cloud infrastructure provider you should run your infrastructure on Fly.\n\nIf you‚Äôre in the northern hemisphere, stay warm! See you on the next edition of The Logbook."
  },
  {
    "title": "Fly Postgres",
    "url": "https://fly.io/blog/how-we-built-fly-postgres/",
    "content": "Like many public cloud platforms, Fly.io has a database offering. Where AWS has RDS, and Heroku has Heroku Postgres, Fly.io has Fly Postgres. You can spin up a Postgres database, or a whole cluster, with just a couple of commands. Sign up for Fly.io and launch a full-stack app in minutes!\n\nFly.io is an ambivalent database provider‚Äîone might even use the word ‚Äúreluctant‚Äù. The reasons for that are interesting, as is the way Fly Postgres works. When we relate this in conversations online, people are often surprised. So we thought we‚Äôd take a few minutes to lay out where we‚Äôre coming from with databases.\n\nWe started Fly.io without durable storage. We were a platform for ‚Äúedge apps‚Äù, which is the very 2019 notion of carving slices off of big applications, leaving the bulk running in Northern Virginia, and running the slices on small machines all around the world. In an ‚Äúedge app‚Äù world, not having durable storage makes some sense: the real data store is in us-east-1, and the slices are chosen carefully to speed the whole app up (by caching, running an ML model, caching, serving images, and caching).\n\nOf course, people asked for databases from day one. But, on days one through three hundred thirty-one, we held the line.\n\nSomewhere around day fifteen, we grew out of the idea of building a platform exclusively for edge apps, and started looking for ways to get whole big crazy things running on Fly.io. We flirted with the idea of investing in a platform built-in database. We rolled out an (ultimately cursed) shared Redis. We even toyed with the idea of offering a managed CockroachDB; like us, Cockroach is designed to run globally distributed.\n\nAnd then we snapped out of it. Databases! Feh!\n\nHere‚Äôs our 2020 reasoning, for posterity: just because we didn‚Äôt offer durable storage on the platform didn‚Äôt mean that apps running on Fly.io needed to be stateless. Rather, they just needed to use off-platform database services, like RDS, CrunchyData, or PlanetScale. Hooking globally distributed applications up to RDS was (and remains) something ordinary teams do all the time. What did we want to spend our time building? Another RDS, or the best platform ever for you to run stuff close to your users?\n\nBy day two hundred and ninety or so, the appeal of articulating and re-articulating the logic of a stateless global platform for stateful global apps began to wear off. RDS! Feh! Somewhere around then, Jerome and Steve figured out LVM2, gave all our apps attached disk storage, and killed off the stateless platform talking point.\n\nNow, disk storage is just one of the puzzle pieces for giving apps a reliable backing store. Storage capabilities or not, we still didn‚Äôt want to be in the business of replicating all of RDS. So we devised a cunning plan: Build the platform out so it can run a database app, build a friendly database app for customers to deploy on it, and add some convenience commands to deploy and manage the app.\n\nWe wouldn‚Äôt have a managed database.\n\nNo, we have an automated database.\n\nPostgres is a good database for this. It‚Äôs familiar and just works with the migration tools baked into full-stack frameworks.\n\nIn January 2021, we soft-launched a fly pg create command that would deploy an automagically configured two-node Postgres cluster complete with metrics, health checks, and alerts. (The alerts were as cursed as our shared Redis.) This was a big-deal effort for us. Back in 2020, we were really small. Almost everyone here had a hand in it.\n\nWhen Shaun arrived at Fly.io later that year, he took over the job of making Fly Postgres more reliable and more convenient to manage‚Äîstill in hard mode: developing and shipping features that make the platform better for apps like Fly Postgres, and making Fly Postgres plug into those.\n\nThis post is mostly ancient history! Shaun‚Äôs no longer a team of one, and lots has happened since this post should have been written and shipped. Everything still holds; it‚Äôs just more and better now.\n\n\n## Postgres is really cool all by itself\n\nHere‚Äôs a way you can run Postgres on Fly.io:¬†point fly launch at the latest official Postgres Docker image. Remove the default services in fly.toml, since this isn‚Äôt a public app. Provision and mount a volume. Store POSTGRES_PASSWORD as a Fly Secret. Deploy.\n\n(Then fly ssh in and create a database and user for your app.)\n\nIf you‚Äôll only ever want this one instance, this is pretty good. If anything happens to your lonely node, though, your Postgres service‚Äîand so, your app‚Äîis down (and you may have lost data).\n\nHere‚Äôs a better setup: one primary, or leader, instance that deals with all the requests, and one replica instance nearby (but preferably on different hardware!) that stays quietly up to date with the latest transactions. And if the leader goes down, you want that replica to take over automatically. Then you have what you can call a high-availability (HA) cluster.\n\nPostgres has a lot of levers and buttons built right in. You can deploy two Postgres VMs configured so one‚Äôs a writable leader and the other is a standby replica staying up to date by asynchronous streaming replication.\n\nWhat Postgres itself doesn‚Äôt have is a way to adapt cluster configuration on the fly. It can‚Äôt notify a replica that the primary has failed and it should take over, and it certainly can‚Äôt independently elect a new leader if there‚Äôs more than one eligible replica that could take over. Something else has to manipulate the Postgres controls to get HA clustering behaviour.\n\nThat‚Äôs where Stolon comes in.\n\n\n## Postgres, WAL, and Streaming Replication\n\nWrite-Ahead Logging (WAL): Before a transaction is applied to tables and indexes on the primary (or only) instance, it‚Äôs written to nonvolatile storage, in the Write-Ahead Log. This means you can afford not to write changes to every affected data file on disk after every single transaction; if data pages in memory are lost, they can be reconstructed by replaying transactions from the WAL.\n\nPostgres streaming replication sends each WAL record along to the replica right after the transaction is committed on the leader. As the record is received, it‚Äôs replayed to bring the replica up to date.\n\nWe have some heartier, SQLite-flavoured WAL content around here somewhere.\n\n\n## Clustering with Stolon\n\nStolon is a Golang Postgres manager. We chose it for a few reasons: it‚Äôs open source, it‚Äôs easy to build and embed in a Docker image, and it can use Consul as its backend KV store (we‚Äôre good at Consul).\n\nWe spun up a Consul cluster for Fly Postgres to use, and since it was there, we also made it available for any Fly app that wanted a locking service.\n\nStolon comes with three components that run alongside Postgres in each instance‚Äôs VM: a sentinel, a keeper, and a proxy.\n\nIf the leader instance fails, the proxies start dropping all connections and Stolon elects a new leader, using Consul to lock the database in the meantime. If both (all) your instances fail, the database is unavailable until one or the other recovers. New connections go to the new leader as soon as it‚Äôs ready, without rebooting clients or changing their config.\n\nIf you‚Äôve ever received a late-night email from Heroku saying your DB was replaced, you know why this is awesome.\n\n\n## Stolon + Consul Intensifies\n\nStolon is chatty as hell with Consul, and this can be a problem.\n\nKeepers, sentinels, and proxies do all their communication via the Consul leader. If a Stolon component can‚Äôt reach Consul, it repeats its request until it can. A single flapping Stolon cluster, early on, could saturate our Consul connections.\n\nMeanwhile, if a Stolon proxy can‚Äôt reach Consul, it throws its hands in the air and drops all client connections until it can. We had several Postgres outages traceable to either Consul falling over or faraway Postgres nodes not being able to connect to it.\n\nThe more Postgres clusters people spun up, the more of a problem this was.\n\n\n## Less Consul With HAProxy\n\nThe Stolon proxy relies on Consul to know which instance to route connections to.\n\nBut Consul isn‚Äôt the intrinsic authority on who the leader is: Postgres on every instance knows its own role. If we can replace the Stolon proxy with one that can just ask the nodes who‚Äôs leader, that‚Äôs less load on our shared Consul cluster, and if there‚Äôs trouble with Consul there‚Äôs one component fewer to freak out about it.\n\nIt‚Äôs not exactly supported, but it‚Äôs possible to use HAProxy with Stolon, and we did.\n\nHere‚Äôs how we‚Äôve got HAProxy set up:\n\nWe also added Consul clusters in a couple more regions. This spreads the burden on Consul, but crucially,¬†it puts Consul clusters close to people‚Äôs primary Postgres VMs. Network flakiness between Stolon and Consul breaks Stolon. The internet is flaky. The less internet we can span, the happier Stolon is.\n\nStolon and Consul are still intense: we‚Äôve been adding new Consul clusters ever since to keep up.\n\n\n## Here‚Äôs the Fly Postgres App\n\nWe‚Äôre running a few things on each Fly Postgres VM:\n\nThis is a pretty deluxe Postgres cluster app. You can shell into a running instance and add a database, restart the PG process, trigger a failover, run stolonctl commands directly, and more.\n\nOur Golang supervisor, flypg, glues the other processes together and does nice things like try to recover from individual process crashes before giving up and letting the whole VM get rescheduled.\n\nAll the parts are open source; you can fork it and add PgBouncer or whatever.\n\nYou can enable extensions for WAL-G, TimescaleDB, and PostGIS yourself, without forking.\n\nSo that‚Äôs the Fly Postgres app. You can deploy it with fly launch like any Fly app, straight from a clone of the postgres-ha repo. It is faster to deploy the built image straight from Docker Hub, and the image has version metadata you can use to upgrade later.\n\nThe following will create a 2-instance HA cluster that apps on your org‚Äôs internal WireGuard network can connect to:\n\nThen, to let an app use this Postgres:\n\nNow I don‚Äôt know if I made that look complicated or simple!\n\nIt‚Äôs simple for what you get. Every instance of your postgres-ha app is a magical cluster building block! Add an instance and it automatically becomes a member of the cluster and starts replicating from the leader. If it‚Äôs in the PRIMARY_REGION, it‚Äôs eligible to participate in leader elections. You can add nodes in other regions, too; they can‚Äôt become leader, but you can read from them directly on port 5433. It‚Äôs all inside the app. Get a bit fancier with the Fly-Replay header in your consuming app, and you can do your reads from the closest instance and send your writes to the primary region.\n\nBut yeah, this isn‚Äôt quite the Fly Postgres experience. Since we expect lots of people to deploy this exact app, it was reasonable to bundle up that mild cluster-creation rigamarole into a fly pg create command, which is much like fly launch with one of our more mature framework launchers. There are similar nuggets of flyctl convenience for managing your fly pg created database cluster.\n\nUse it in something awesome!\n\n\n## An Observation\n\nWe‚Äôve mentioned that continual reliance on Consul is something of an Achilles‚Äô heel for Stolon-managed clusters. It‚Äôs not unique to Stolon and Consul, but a matter of needing a separate backend store for cluster state: in return for high availability and Borg-like assimilation of new instances, we accept an additional failure mode.\n\nIf you‚Äôre running a single node, and you‚Äôre never going to add another one to make a cluster, there‚Äôs no upside to this high-availability machinery. A lone node is more reliable without any of it.\n\nWe did briefly deploy a leaner, standalone Postgres app for the ‚ÄúDevelopment‚Äù fly pg create configuration. This created a poor experience for users wanting to scale up to a HA cluster‚Äîthe plumbing wasn‚Äôt there to do it.\n\nBut quite a lot of people do run Fly Postgres on a single instance (just for development, right??). It‚Äôs still automated, and you still get the knowledge that you‚Äôre in good company and deploying a maintained app.\n\nThe great thing is: if you really want the simpler setup, you can just deploy your own Postgres app. It‚Äôs all apps on Fly.io!\n\n\n## Snapshots and Restores\n\nYou can, and should, make your own backups of data that‚Äôs important to you. That being said, a restore-your-database feature is guaranteed to make people‚Äôs lives easier.\n\nIf you‚Äôre shipping Postgres as a Service and don‚Äôt care about the underlying infrastructure, you‚Äôll do Postgres native backups, copy data files and the WAL to object storage somewhere, then restore from those. Stolon will manage this for you.\n\nBut if you‚Äôre building infrastructure that can run databases, this doesn‚Äôt move you forward: every database has its own mechanism for backing up individual files. Some require data dumps using specific tools, some let you copy files out of the file system, etc.\n\nVolumes, which hold users‚Äô persistent data‚Äîfor Postgres, SQLite, or whatever‚Äîare logical volumes on SSDs physically installed in our servers. We have low-level block device powers and the ability to take consistent, block-level snapshots of a disk.\n\nSo that‚Äôs how we back up a Postgres database: by periodically grabbing a point-in-time version of the raw block device it‚Äôs on. You recover a database by restoring this to an entirely new block device and deploying a Postgres instance to use it.\n\nConveniently, that approach works for pretty much anything that writes to a file system, solving backups for anything you want to run on Fly.io.\n\nOnce we got user-facing snapshot restores working for Postgres apps, we could generalize that to Volumes at large. Which is good, because people run every database you can think of on Fly.io.\n\nThis is a good example of ‚ÄúPostgres‚Äù work that was actually platform work with an elephant face taped on. Like persistent storage itself, shared Consul, our crap health-check alerts, image version updates, and countless ‚Äúhow should flyctl and the platform behave‚Äù minutiae.\n\n\n## Back to Fly Postgres vs. Managed Databases\n\nSo Fly Postgres is an app, not a database service. This is not a bummer: it‚Äôs fascinating, I tell you! Working on this one app helps us work through what we want the platform to offer to apps and how to implement that. It‚Äôs an intrinsic part of the process of building a platform you could run your fully managed database service on.\n\nMeanwhile, we don‚Äôt blame you if you‚Äôd actually prefer a boring managed database over our fascinating app. We love boring! Boring can be the best experience! We think the best solution to this is to partner with service providers to do integrations that really nail the Postgres, or MySQL, or Redis(!), or whatever, UX on Fly.io. After all, there‚Äôs no single best database for everyone.\n\nAnd for all that, heading for 2023, Fly Postgres is doing the job for lots of apps! Automated Postgres turned out more useful than we‚Äôd have predicted."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/replicache-machines-demo/",
    "content": "We‚Äôre Fly.io. React, Phoenix, Rails, whatever you use: we put your code into lightweight microVMs on our own hardware in 26 cities and counting. Check us out‚Äîyour app can be running close to your users within minutes.\n\nImagine this: you have invented the best design tool since Figma. But before you can compete with the design-industry heavyweight, you need to be able to compete on one of Figma‚Äôs main propositions: real-time collaboration. You do some research and find that this is an upsettingly hard problem.\n\nYou start with a single server that coordinates users‚Äô changes for each document, using something like Replicache (which we‚Äôll come back to in a sec). Clients connect over WebSockets. But your business grows, and that one server gets overloaded with connections.\n\nYou‚Äôll have to split your documents among servers, but that means routing connections for each document to the server that has it. What now? Write complex networking logic to correctly route connections? ‚ÄúThere has to be a better way,‚Äù you say to yourself.\n\n‚ÄúThere is!‚Äù I say, popping out of your computer!\n\nOK, I may have passed through ‚Äúconversational writing style‚Äù right into ‚Äúwitchcraft.‚Äù Let‚Äôs reel it back in.\n\nI‚Äôm going to demonstrate what I think is a good solution for the problem of a distributed real-time backend, using Replicache, Fly Machines and a Fly.io feature we‚Äôve talked about before: the Fly-Replay header.\n\nOur demo app for today is‚Äîdrum roll‚Äîa real-time collaborative todo app!\n\n\n## Replicache: an aside\n\nReplicache is a ‚ÄúJavaScript framework for building high-performance, offline-capable, collaborative web apps.‚Äù It handles the hard parts of collaboration tech for us: conflicts, schema migrations, tabs, all the good (and bad) stuff.\n\nUnfortunately for us, for all that Replicache is super good at, it does not use WebSockets. But Websockets are perfect for frequent bidirectional updates to a document! Thankfully, using the Power of Software‚Ñ¢ we can just make Replicache bend to our WebSockety will.\n\nThe protocol that Replicache uses can be described in three parts: a push endpoint that clients can use to push changes to the server, a pull endpoint the client can use to pull changes made since the last time it pulled, and a persistent connection (usually either WebSockets or Server Sent Events) to poke clients and let them know something has changed so they should pull. If the protocol requires a persistent connection anyway, we should make everything work over that connection.\n\nDon‚Äôt do what I am about to describe in production without some serious thought/tuning. While the way I do things totally works, I also have not tested it beyond the confines of this basic demo, so like, proceed with caution.\n\nBuilding on replicache-express which implements the push, pull, and poke endpoints, I added a new endpoint for opening a WebSocket connection to a given ‚Äúroom‚Äù; in this case a room is just a given document.\n\nHere is the (slightly simplified) flow that happens when a WebSocket connection is opened via /api/replicache/websocket/:spaceID:\n\nThe actual process is slightly more nuanced to prevent sending more over the wire than we really have to (like updates we know clients have already seen). Every time we send a message to a given client we remember that we sent it. This way, the next time we need to update the client we can just send it what has changed and not the whole document. If you are curious how the server sync logic works, the code is available here.\n\nThis whole process is implemented on the client basically in the inverse by overriding the Replicache client‚Äôs push and pull logic to use our persistent WebSocket connection. When the frontend receives a message it stores the changes from the message in a temporary array, then instructs the Replicache client to ‚Äúpull‚Äù. The trick is that we override the pull logic from the Replicache client to just read straight from the array we‚Äôve been storing websocket messages in.\n\nWe similarly override the push functionality to write the message over our websocket instead of HTTP.\n\n\n## Our architecture\n\nLet‚Äôs take a look at the architecture diagram for this demo.\n\nThe Replicache WebSocket server we just talked about is what we can see running on each Fly machine in the backend section. For the sake of the Figma-like app example, we can think of each machine running the backend for a given design document. You can see them named room(a-d).\n\nWe can see something cool happening, though: instead of our clients having to know where to find the specific backend server where the document they want lives, all the clients connect via the same router. The router is the super cool part of this app.\n\n\n## The router\n\nLet‚Äôs think for a second about what our router has to do:\n\nFor the sake of this demo, we will consider requirement 1. out of scope and have our router use hardcoded values.\n\nFor requirement 2., we just need to help the client find the right backend machine for its document when it makes a request for a WebSocket connection upgrade; the client and the backend can establish their connection and communicate directly from then on.\n\nWe‚Äôll be running the backend servers as Fly Machines on the same Fly.io private network as the router app, which means we can use a super-cool feature called fly-replay to send a connection request on to a given VM.\n\nfly-replay is a special header that fly-proxy, the aptly named Fly.io proxy, understands, that can be used to replay a request somewhere else inside your private network. For instance, to replay a request in region sjc your app‚Äôs response could include the header fly-replay: region=sjc. You can also do fly-replay: app=otherapp;instance=instanceid, to target a specific VM. This is what we‚Äôll use in our router.\n\n\n## Putting it all together\n\nI unfortunately am not going to make the Next Best Thing Since Figma in this post for the sake of example, but I built something almost as good: a collaborative real-time todo list based on this demo from Replicache.\n\nOur todo list has three layers (directions to actually run these yourself can be found here):\n\nFor the backend we just have a pool of Fly machines, each one responsible for a single todo list. Each todo list will be identified by the id of the machine it runs on.\n\nSo when a client connects to /api/replicache/websocket/machineID on the router, the router responds with fly-replay: app=replicache-backend;instance=machineID. This response is handled by fly-proxy, who replays the request against the appropriate backend machine, and voil√†: the client is now directly talking to the right backend machine.\n\nWe now have a way over-engineered todo list(s): each list runs on its own VM, and a simple router leveraging fly-replay sends WebSocket connections to the correct backend.\n\nYou can check out the demo for yourself at https://replicache-frontend.fly.dev. You‚Äôll see a list of clickable IDs. Each ID is a machine running an instance of the backend which corresponds to an individual todo list. If you are so inclined to set this demo up for yourself, the code and instructions are here."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-november-7-to-november-14-2022/",
    "content": "Automatically deploy Elixir apps to production with Github Actions CI and troubleshoot performance issues with OpenTelemetry. Help the Python community draft Django docs. Deploy S3-compatible object storage close to your Fly apps.\n\nLast week we got improved Elixir on Fly docs‚Äîthis week we get even more goodies that make it easier to deploy and monitor Elixir apps in production.\n\n\n## Github Actions for Elixir CI\n\nMark rolls up his sleeves, grabs a shovel, and digs into getting Github Actions working for Elixir CI. The best part? You don‚Äôt have to get as dirty to get it setup because Mark did all the hard work for you.\n\nRead Github Actions for Elixir CI\n\n\n## Elixir, OpenTelemetry, and the Infamous N+1\n\nAfter your Elixir CI Github Action automatically deploys your app to production, learn how to use OpenTelemetry to monitor and track down performance issues in your production deployments. Alexander Koutmos shows us how by tracking down an N+1 query issue.\n\nRead Elixir, OpenTelemetry, and the Infamous N+1\n\n\n## Django on Fly\n\nAmazing things happen on the Internet, like the Python community getting together and documenting how to deploying Django apps to Fly. While we don‚Äôt have an official 1.0 set of docs yet, it‚Äôs getting mighty close thanks to community contributors.\n\nWill Vincent has been pushing forward a more comprehensive guide for Django that‚Äôs way better than our current ‚ÄúRun a Python App‚Äù guide in this Github Pull Request. There‚Äôs also a thread in the Community Forum about an updated blogpost for deploying a ‚ÄúHello World‚Äù Django app.\n\nIf you‚Äôre a Django or Python developer, check out the PR or read the Community post.\n\n\n## Deploy your own MinIO S3-compatible object storage to Fly\n\nChris updates our docs on how to deploy your own self-hosted S3-compatible object storage server to Fly. We still think its easier to use an S3 host, like AWS S3, but sometimes that doesn‚Äôt make sense when you need object storage in the same datacenter as your application.\n\nRead the MinIO deployment guide\n\nI‚Äôll see you next week!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-october-29-to-november-6-2022/",
    "content": "This week Redis gets some power-ups, Elixir confesses its love for React, Fly gets real about Postgres, Livewire gifts us with a few tutorials, and a many of us in North America didn‚Äôt have to change the clocks around our house because we convinced our families last year to make the switch to UTC.\n\n\n## React inspires Elixir LiveView ‚ù§Ô∏è ü§ó\n\nChris McCord writes about how React inspired LiveView reminding us that seemingly very different frameworks actually have a lot to learn from each other.\n\nIt‚Äôs worth taking this moment to think about what you could learn from other frameworks that evoke strong emotions‚Äîthere‚Äôs lots to learn about what they did well, or even some of the mistakes they may have made.\n\nRead A Love Letter to React\n\n\n## Elixir docs improvements\n\nWell well well, our documentation just keeps getting better around here. Last week we saw improvements to Postgres docs. This week Elixir is taking a turn.\n\nRead the revamped Elixir Docs\n\n\n## Postgres: What Fly manages and what you manage\n\nFly tries to make it very clear that we provide tools that make it easy to provision and manage a Postgres database cluster, but the tool is so good at quickly getting Postgres databases up and running that it‚Äôs easy to forget that its not managed.\n\nThe Postgres docs were updated to lay out, in more detail, exactly what Fly manages and what we expect customers to manage.\n\nRead about what Fly manages, and what you manage with Postgres\n\nIf managing a Postgres database isn‚Äôt your thing, we even threw in a few links to some popular managed Postgres services.\n\n\n## Much to learn about Laravel Livewire\n\nThere‚Äôs lots to learn this week about Laravel Livewire. Our first teacher, Kathryn Anne, makes complex client-side pagination, grouping, and sorting in a table look easy with this tutorial.\n\nRead Hoarding Order with Livewire\n\nThen Chris walks us through how to send server-side notifications from a Livewire app to peoples‚Äô browsers who are currently using your app.\n\nRead Global Notifications with Livewire\n\n\n## Fly Redis gets a dashboard\n\nWhen you run fly redis dashboard \u003corg\u003e you‚Äôll be whisked into the Upstash Redis console where you‚Äôll see stats on your Redis instances and instructions on how to connect it to your application. Don‚Äôt forget to run fly version update to get the latest CLI before you run this spiffy new command.\n\n\n## Fly Machines multiple processes preview\n\nIt‚Äôs pretty slick how they work: define your processes and many of them will run inside of one container. Shhhh, don‚Äôt tell the container police that we‚Äôre running multiple processes inside of one container.\n\nRead the thread about multiple process in Fly Machines\n\nSee you next week!"
  },
  {
    "title": "Phoenix screams on Fly.io.",
    "url": "https://fly.io/blog/love-letter-react/",
    "content": "We‚Äôre Fly.io. React, Phoenix, Rails, whatever you use: we put your code into lightweight microVMs on our own hardware in 26 cities and counting. Check us out‚Äîyour app can be running close to your users within minutes.\n\nIt‚Äôs hard to overstate the impact React has had since its release in 2013. For me, React came with a few revelations. First was a reactive HTML-aware component model for building UIs. Second was colocated markup directly in the app code. Third, it focused on efficiency in a world where SPAs were increasingly heavy-handed.\n\nIt was also something that I could grok in a weekend.\n\nMy previous attempts at drive-by learning other reactive frameworks of the day were not so successful. Phoenix borrowed a lot from React when we first shipped LiveView in 2018, but only recently have we gone all-in with an HTML-aware component system in Phoenix 1.7.\n\nIt‚Äôs hard to imagine building applications any other way. What follows is a heartfelt homage to React‚Äôs impact on the frontend and backend world, and how Phoenix landed where it did thanks to the revelations React brought almost ten years ago.\n\n\n## Reactive component system\n\nWith LiveView, I was inspired by React components and their beautifully simple programming model. A component is an object that defines a render function, and returns some HTML (or in later versions, a function that renders HTML). That function makes use of component state, and whenever a state change occurs the render function is called again.\n\nThis was such a simple model to understand when coming into React for the first time. Here we have a React component with some state, and a render function. The function returns some HTML, and calls setState when a button is clicked. Any time setState is called, React will call render again, and the UI updates. Easy peasy.\n\nWe borrowed from this with LiveView by taking that model and slapping it on the server in a stateful process:\n\nI‚Äôve talked previously about what this kind of programming model on the server enables, like the fact we don‚Äôt write bespoke routes, controllers, and serializers, or JSON APIs or GraphQL endpoints. But here we‚Äôre just appreciating how easy this model is to understand. In a world of ever-increasing framework complexity, React‚Äôs take on interactive applications was a breath of fresh air that we were quick to borrow.\n\nAnother choice React made was also extremely contentious at the time: putting HTML right in with your app code. People hated it. But React was right.\n\n\n## JSX:¬†A Colocation Revelation\n\nLike many folks ten years ago, you might still be thinking ‚ÄúHTML in your app code?! Are we back to the 2000‚Äôs PHP days of mixing code and markup together in a file? And we call this progress?‚Äù\n\nThese kind of takes were common. They also missed the point. Unlike the PHP days of yore, React applications weren‚Äôt a string concatenation of app code, HTML, and business logic masquerading as a web application.\n\nReact‚Äôs JSX templates place the most coupled pieces of UI together: the markup and stateful code supporting that markup. Think about it: you have a bunch of variables (state) in your app code that are also needed in your template code for UI rendering or behavior. You also have a bunch of UI interactions in your templates that make it back into app code‚Äîlike button clicks. These two things are necessarily tightly coupled. Change either side of the contract and the other side breaks. So React made the wise step to put those tightly coupled things together.\n\nThis brings us to a lesson React taught me that I later carried over to Phoenix: if two pieces of code can only exist together, they should live together. Or to think about it another way, if two pieces of code must change together, they must live together.\n\nThere‚Äôs no guesswork on what happens if I change some LiveView state or LiveView template variables because they live in the same file. I also don‚Äôt have to search throughout the codebase to find which coupled-but-distant template file needs to be added or changed to accommodate the code I‚Äôm writing.\n\nNow, there are times where it‚Äôs not practical to write app code and markup in a single file. Sometimes template reuse or a large document means it makes more sense to have a separate template. In these cases, you want the next best thing: colocated files. In general, the tightly coupled parts of your application should be as close as practically possible. If not the same file, then the same directory, and so on.\n\n\n## HTML-aware components as extensible building blocks\n\nReact also popularized HTML-aware components with their JSX template system. On top of writing HTML in your component‚Äôs app code, you call components from markup in an HTML tag-like way.\n\nThis is more than a cute way to make function calls. It‚Äôs also not something I appreciated right away. The advantage of this approach is a natural composition of static HTML tags alongside dynamic components and logic. Large HTML structures quickly lose their shape when mixing dynamic code and reusable UI with tags‚Äîan issue with Ruby or Elixir-like templating engines.\n\nFor example, imagine you need to render a collection of items, then within that collection, conditionally call some other template code. With Rails or older Phoenix style \u003c%= %\u003e templates, the markup structure almost entirely gets lost in the mess of branches:\n\nThis has a few problems. First, the markup structure is completely lost when mixing code branches and comprehensions.\n\nThis makes template editing a brittle and frustrating experience. If our goal is to dynamically build markup, why does the markup structure get lost in the mix? It gets worse when we try to encapsulate this table into a reusable piece of UI. The best we could do prior to adopting React‚Äôs approach is bespoke functions or templates that hide the entire table from the caller:\n\nThen the caller can render the component:\n\nThis works, but extensible UI components are all but impossible. The moment we want to customize one aspect of the table, we need to write another template like user_table which slightly alters the cells or adds more actionable links to another cell, and so on. If we tried to make it extensible without an HTML-aware component primitive, we‚Äôd end up with something like:\n\nOur bespoke functions now mask the HTML structure, which makes it difficult to figure out what‚Äôs happening. We also can‚Äôt easily encapsulate table row and cell styling.\n\nWorse, we prevent the caller from passing their own arbitrary block content to our components.\n\nFor example, imagine instead of a string ‚ÄúUsers‚Äù as the table title, the caller wanted to render HTML within the \u003ch1\u003e, such as a subtitle, icon, or even another component? With template engines that only do string concatenation, passing strings around prohibits all of this. A caller may try passing a string of HTML instead, but it‚Äôs a nonstarter:\n\nPassing strings around for arbitrary content quickly breaks down. It‚Äôs not only terrible to write, but the user would have to forgo HTML escaping and carefully inject user-input into their dynamic strings. That‚Äôs a no-go.\n\nReact‚Äôs JSX showed us a better way. If we make our templating engine HTML-aware and component calls become tag-like expressions, we solve the readability issues. Next, we can allow the caller to provide their own arbitrary markup as arguments.\n\nReact allows passing markup as an inner component block, or as a regular argument (‚Äúprop‚Äù in React parlance) to the component. For example, in React, one could write:\n\nLater frameworks like Vue, and the Web Component spec itself standardized and expanded this concept with the ‚Äúslot‚Äù terminology.\n\nIn Phoenix, HTML syntax for components along with slots turns our mess of mixed HTML tags and strings into this beautifully extensible UI component:\n\nThe Phoenix HEEx template engine supports calling components external to the current scope in a similar React style, such as \u003cTable.simple\u003e...\u003c/Table.simple\u003e. Phoenix also allows calling imported function components directly with the \u003c.component_name /\u003e notation.\n\nIn the table example above, we call the table function with arguments passed in a tag-like attribute syntax, just like in React props. Next, the table accepts an internal block of arbitrary markup, and we here we can make use of slots to pass title and col information.\n\nThe neat thing about slots in Phoenix is the fact that they are collection based. The caller can provide an arbitrary number of entries, such as in our \u003c:col\u003e example. To render a table, internally the table component can simply iterate over the cols we passed for each row, and ‚Äúyield‚Äù back to us the individual user resources. You can see this in action via the :let={user} syntax in the col entries.\n\nThe internal table can also iterate over the cols to build the \u003cth\u003es for the table head. What results is far more pleasant to write than pure HTML and can be extended by the caller without bespoke functions. The function component and slot primitives allow us to encapsulate everything about building tables in our UI in a single place.\n\nLike React, you‚Äôll find that your Phoenix applications establish a surprisingly small set of core UI building blocks that you can use throughout your application.\n\nFly.io was practically born to run Phoenix. With super-clean built-in private networking for clustering and global edge deployment, LiveView apps feel like native apps anywhere in the world.\n\n\n## Efficient at its core\n\nMy SPA trials and tribulations began before React entered this world. I‚Äôve gone from jQuery spaghetti, Backbone.js, Angular 1, Angular 2, Ember, and finally React. React provided just the right amount of structure, while being quick to pick up and get going with. It was also super fast.\n\nReact really pushed the industry forward with their virtual DOM features. Instead of replacing large parts of the browser‚Äôs DOM with a freshly rendered template on any little change, React kept a ‚Äúvirtual‚Äù DOM as a datastructure that it was able to compute diffs against. This allowed React to compute the minimal set of concrete DOM operations required to update the browser when state changes occur.\n\nThis was groundbreaking at the time.\n\nOther SPA frameworks quickly followed suit with their own optimizations. Server-side frameworks are a different paradigm entirely, but they can learn a lot from React‚Äôs innovation. Phoenix certainly did.\n\nFor Phoenix, we borrowed these ideas, but we have this pesky layer between the app and the client, known as the network. Our problem set is quite different from React, but if you squint, you can see all the same inspirations and approaches we took in Phoenix LiveView‚Äôs optimizations.\n\nFor example, on the server we only want to execute the parts of the template that changed rather than the entire template. Otherwise we‚Äôre wasting CPU cycles. Likewise, we only want to send the dynamic parts of the template that changed down the wire instead of the entire thing to limit latency and bandwidth. While we don‚Äôt keep a virtual DOM on the server, we do keep track of the static and dynamic parts of the HEEx templates. This allows us to do efficient diff-based rendering on the server and send down minimal diffs to the client. Meanwhile, the client uses morphdom to apply only the minimal patches necessary on the client.\n\nThe end result is this: a state change occurs in the LiveView component tree, a diff of changes is computed on the server with noops where possible, and the minimal diff of changes is sent down the wire. On the client, we take those changes and apply them via a minimal set of DOM operations to efficiently update the UI. Sound familiar?\n\n\n## React‚Äôs influence on the backend\n\nReact changed the front-end game when it was released, and its ideas have trickled up to the backend world. And no, I don‚Äôt mean React Server Components (but React is also trickling up to the server too!). Outside of Phoenix, you‚Äôll find other backend frameworks now ship with their own HTML-aware component system, such as Laravel‚Äôs Blade templates in the PHP space.\n\nIf you‚Äôre a backend framework in 2022 and not shipping an HTML-aware engine, it‚Äôs time to follow React‚Äôs lead. I can‚Äôt imagine Phoenix not landing where we did, and my only regret is we didn‚Äôt follow React sooner. Thank you React for paving the way! ‚ù§Ô∏è"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-october-21-to-28-2022/",
    "content": "It‚Äôs been a minute since the last Logbook. It turn‚Äôs out tracking every single change monthly gets to be a lot, so we‚Äôre going to try something a little different‚Äîan over-generalized weekly tl;dr of changes at Fly.io that you‚Äôll hopefully find helpful for deploying or managing your Fly.io apps.\n\n\n## Postgres docs improvements\n\nFirst up, some new documentation was created to run people through how to fail over a Postgres database.\n\n‚ÄúGetting Started‚Äù was updated to show how to setup a Postgres database and attach it to an application. There‚Äôs also docs on how to bring over your Postgres database from Heroku if you‚Äôre moving your apps over from there.\n\nIf you have ideas for improving these docs, open the ‚ÄúEdit on Github‚Äù link at the bottom of each page to propose changes. Expect more improvements over the next few weeks.\n\n\n## PSA: Fly Postgres is not fully managed Postgres\n\nWant to know a secret that‚Äôs not a secret? Fly‚Äôs Postgres database is not a fully managed database. Fly does give you great tools to provision and upgrade Postgres instances, but if they run out of disk space and you don‚Äôt have monitoring hooked up, your customers will be telling you about it.\n\nRead the Anatomy of a Postgres Outage.\n\nThis isn‚Äôt to single anybody out‚Äîoutages happen to the best of us. I once sudo rm -rf / a production sever before containers made for easy recoveries, which is why I‚Äôm on a Frameworks team and don‚Äôt let myself near the Fly.io production servers. We just want folks to know what they‚Äôre getting into when they deploy their apps on Fly.io.\n\n\n## Integrating the Elastic Stack (ELK) Into a Laravel App on Fly.io\n\nNot to be confused with Cervus canadensis, ELK is a way to process streams of data and store them in a way that can be quickly retrieved later. In this example you‚Äôll learn how ELK can be used in a Laravel application to track user analytics and generate reports with them in a fancy pants dashboard.\n\nRead Integrating the Elastic Stack (ELK) Into a Laravel App on Fly.io.\n\n\n## Shut down an idle Phoenix app\n\nWhy pay for something you‚Äôre not using? Chris McCord shows how a Phoenix app can shut itself down on Fly Machines if it hasn‚Äôt received quests for a configurable period of time.\n\nRead Shut down an idle Phoenix app.\n\n\n## What‚Äôs up with Fly Apps v1 and v2?\n\nYou may have heard of Fly Machines, but did you know when you fly launch an app today, it doesn‚Äôt deploy to a Machine?\n\nChris F lays out the differences you can expect between the way Fly apps currently behave today, and how they‚Äôll behave when they‚Äôre deployed to Fly Machines.\n\nRead the Advantages/Disadvantages of Machine Apps post.\n\nfly pg create does use Fly Machines, which you can read more about at Fly‚Äôs Postgres Docs and the Postgres on Machines post.\n\n\n## Colorblind friendly dashboard improvements\n\nA little change in the Fly dashboard goes a long way!\n\nRead the Colorblind friendly dashboard improvements post.\n\nThat‚Äôs it for this week. Happy Halloween, stay safe out there trick-or-treating, and I‚Äôll see you next week!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/introducing-litefs/",
    "content": "Fly.io runs apps close to users by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. We‚Äôre also building an open-source distributed file system for SQLite called LiteFS which is pretty cool too. Give us a whirl and get up and running quickly.\n\nFull-stack developers are sleeping on SQLite, a database most devs think more suited to unit tests than production loads. That‚Äôs true enough for some apps. Most web apps are read-heavy, though, and we can use that to our advantage. With the right tooling, SQLite makes for faster, simpler web apps.\n\nTo understand why we won‚Äôt shut up about SQLite, think about latency. You have a budget of around 100ms to make an app feel snappy. Individual Postgres queries add milliseconds of latency. Apps often run multiple queries before responding to users. Database round trips can take a big bite out of a latency budget.\n\nThe same problem infects your full-stack code. Developing against a relational database requires devs to watch out for ‚ÄúN+1‚Äù query patterns, where a query leads to a loop that leads to more queries. N+1 queries against Postgres and MySQL can be lethal to performance. Not so much for SQLite.\n\nThe challenge of building full-stack on SQLite is that it isn‚Äôt client-server: it‚Äôs a library that runs inside of your app. In the past, that‚Äôs made it hard to get durability and replication. Most devs aren‚Äôt comfortable with a ‚Äúsingle-server‚Äù architecture, where any downtime in your app server takes your whole app down.\n\nBut you don‚Äôt need to make peace with single-server designs to take advantage of SQLite. Earlier this year, we wrote about why we‚Äôre all in on Litestream. Litestream is SQLite‚Äôs missing disaster recovery system: it‚Äôs a sidecar process that hooks into SQLite‚Äôs journaling and copies database pages to object stores such as S3. Like SQLite itself, it has the virtue of being easy to get your head around; we explained most of the design in a single blog post, and using it just takes a couple commands.\n\nWe want to see how far we can push this model, and so we‚Äôve been working on something new.\n\n\n## LiteFS: Where We‚Äôre Going We Don‚Äôt Need Database Servers\n\nAt least, not as such.\n\nLiteFS extends the idea of Litestream with fine-grained transactional control. Where Litestream simply copies the raw SQLite WAL file, LiteFS can inspect and ship individual transactions, which span pages, and are the true unit of change in a SQL database.\n\nSQLite imposes on us a constraint that makes this transactional control harder: SQLite is baked into the apps that use it. If you build something that changes the SQLite library itself, you‚Äôre not building tooling; you‚Äôre building a new database. And we‚Äôre not interested in getting people to switch to a new flavor of SQLite.\n\nThere‚Äôs two options for intercepting the file system API in SQLite:\n\nThe VFS option is easier so, naturally, we chose to build a FUSE file system. That‚Äôs how you‚Äôre supposed to do it, right?\n\nLiteFS works by interposing a very thin virtual filesystem between your app and your on-disk database file. It‚Äôs not a file system like ext4, but rather a pass-through. Think of it as a file system proxy. What that proxy does is track SQLite databases to spot transactions and then LiteFS copies out those transactions to be shipped to replicas.\n\nIn the default journaling mode, transactions are easy to identify: a write transaction starts when the -journal file is created, and ends when it‚Äôs deleted. The journal stores the page numbers and old page data and we can look up the new page data from the main database file.\n\nYou see where this is going. SQLite‚Äôs exquisitely documented file format makes it easy for LiteFS to replicate whole databases. Now we‚Äôve got transaction boundaries. So we roll those transactions up into a simple file format we call LTX. LiteFS replicas can replay those transactions back to recreate the current (or any previous) transaction state of a LiteFS-tracked SQLite database ‚Äî without touching app code. It seems like magic, but it‚Äôs a natural consequence of SQLite‚Äôs strong design.\n\n\n## Ok, so why didn‚Äôt you write a VFS instead?\n\nFirst off, we have nothing against the SQLite VFS system‚Äîit‚Äôs great! We‚Äôre planning on also releasing LiteFS as a VFS with a super catchy name like‚Ä¶ LiteVFS.\n\nIf you‚Äôre unfamiliar with VFSes, they serve as an abstracted file system API. In fact, you use them all the time since SQLite ships with two built-in VFS modules: one for Unix \u0026 one for Windows. You can also load a third-party VFS as an extension, however, therein lies the first problem. There‚Äôs an extra step to use it. Every time someone needs to use the database, they have to remember to load the VFS. That includes when your application runs but also when you just load up the sqlite3 CLI.\n\nLiteFS also needs to run an API server to replicate data between nodes. This gets complicated if you have multiple processes on a single machine trying to access the same local database. Which one runs the API server?\n\nThe FUSE file system solves many of these usability issues by being a single point that all database calls go through. Once you mount it, there‚Äôs no additional steps to remember and any number of processes can use it just like a regular file system.\n\n\n## What LiteFS Can Do Today\n\nLiteFS‚Äô roots are in Litestream which was built with a simple purpose: keep your data safe on S3. However, it still ran with a single-server architecture which poses two important limitations.\n\nFirst, if your one server goes down during a deploy, your application stops. That sucks.\n\nSecond, your application can only serve requests from that one server. If you fired up your server in Dallas then that‚Äôll be snappy for Texans. But your users in Chennai will be cursing your sluggish response times since there‚Äôs a 250ms ping time between Texas \u0026 India.\n\nLiteFS aims to fix these limitations.\n\nTo improve availability, it uses leases to determine the primary node in your cluster. By default, it uses Hashicorp‚Äôs Consul.\n\nWith Consul, any node marked as a candidate can become the primary node by obtaining a time-based lease and is the sole node that can write to the database during that time. This fits well in SQLite‚Äôs single-writer paradigm. When you deploy your application and need to shut down the primary, that node can release its lease and the ‚Äúprimary‚Äù status will instantly move to another node.\n\nTo improve latency, we‚Äôre aiming at a scale-out model that works similarly to Fly Postgres. That‚Äôs to say: writes get forwarded to the primary and all read requests get served from their local copies. Most app requests are reads, and those reads can be served lightning fast from in-core SQLite replicas anywhere in your deployment.\n\nBut wait, that‚Äôs not all! There are many ways to do replication and each application has its own needs around data access. LiteFS also lets you use a static primary node if you don‚Äôt want to use Consul.\n\nWe even have more topologies in the works. We‚Äôve had suggestions from the community to support other approaches like primary-initiated replication. That would allow folks to stream real-time database updates to customers outside their network instead of customers connecting in. Kinda niche, but cool.\n\n\n## Split Brain Detection\n\nLiteFS uses asynchronous replication between a loose membership of ephemeral nodes. It trades some durability guarantees for performance and operational simplicity that can make sense for many applications.\n\nIt‚Äôs able to do this because the primary election through Consul is dynamic and self-healing, which is again both the good and the bad news. Because dynamic topologies can have weird failure modes, LiteFS is designed defensively: we maintain a checksum for the entire state of the database and include it in each LTX file. This sounds expensive, but we can maintain it incrementally.\n\nWe‚Äôre able to maintain this checksum by calculating the checksum for each page and XOR'ing the results together:\n\nWhen a transaction changes pages in the database, we‚Äôll start with the checksum of the previous LTX file, remove the old checksums for the changed pages, and add in the new checksums for the changed pages:\n\nSince XOR operations are commutative, we can even checksum across compacted LTX files or checksum the current state of the database. We can do this in LiteFS because we have fine-grained control over the file system writes.\n\nThese database checksums ensure that an LTX file cannot be applied out of order and corrupt your database: they ensure byte-for-byte consistency for all the underlying data. We verify these on startup so that every database must be in a consistent state relative to its LTX checksum.\n\n\n## Where We‚Äôre Heading With This\n\nWe think LiteFS has a good shot at offering the best of both n-tier database designs like Postgres and in-core databases like SQLite. In a LiteFS deployment, the parts of your database that really want to be networked are networked, but heavy lifting of the data itself isn‚Äôt.\n\nIt‚Äôs not just about performance. If you‚Äôre using a database server like Postgres or MySQL today, chances are you‚Äôre using a ‚Äúmanaged‚Äù database service, where some other team is making sure your database is up and running. Everybody uses managed services because keeping database servers happy is annoying. With SQLite, there‚Äôs not as much stuff that can break.\n\nAnd we‚Äôll keep saying this: the reason we think LiteFS and full-stack SQLite is a good bet is that the design is simple. You can read a summary of the LiteFS design and understand what each of these components is doing. SQLite is one of of the most trusted libraries in the world; most of our job is just letting SQLite be SQLite. Your app doesn‚Äôt even need to know LiteFS is there.\n\nWe‚Äôre plowing ahead on LiteFS features. Here are a few big ones to look out for:\n\nWAL-mode Support: today, LiteFS works with SQLite‚Äôs default rollback journal mode. But WAL mode is where it‚Äôs at with modern SQLite. The FUSE proxy model works fine here too: transactions start with a write to the -wal file, and end with another write that marks a header with the commit field set.\n\nWrite Forwarding: SQLite works with a single-writer, multiple-reader model and our primary/replica replication mimics that. However, it adds friction to require developers to forward writes to the primary node. Instead, we‚Äôre making it so any node can perform a write and then forward that transaction data to the primary. The primary can then replicate it out to the rest of the cluster.\n\nS3 Replication: running a cluster of LiteFS nodes significantly improves your durability over a single-server deployment. However, nothing gives quite the same warm fuzzy feeling as tucking away a copy of your database in object storage. This will work similarly to Litestream, however, LiteFS‚Äô LTX files are built to be efficiently compacted so restoring a point-in-time copy of your database will be nearly instant.\n\nEncryption: we want developers to feel safe keeping SQLite replicas on services like S3. So we‚Äôve designed an AEAD encryption scheme that fits into LiteFS naturally and ensures that even if you manage to expose your LTX files to the Internet, you won‚Äôt have exposed any plaintext.\n\n\n## Try It Out\n\nAfter several months of work, we‚Äôre comfortable calling LiteFS beta-ready. We‚Äôd be happy if you played around with it.\n\nWe‚Äôve set up a documentation site for LiteFS so you can get going with it and understand how it works. The easiest way to get up and running is to walk through our Getting Started with LiteFS guide. It only takes about 10 minutes and you‚Äôll have a globally-distributed SQLite application running. Crazy, right!?\n\nLiteFS is completely open source, developed in the open, and in no way locked into Fly.io, which invests resources in this solely because we are nerds about SQLite and not in any way because LiteFS is part of a secret plan to take over the world. Pinky-swear!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/scale-to-zero-minecraft/",
    "content": "I‚Äôm Dov Alperin. I wrote, and currently maintain, the official Fly.io Terraform provider. You don‚Äôt need Terraform to run scale-to-zero Minecraft on Fly Machines, but it makes configuration and resource provisioning a breeze. I admit I‚Äôm totally biased. (It‚Äôs true, though.) Start at the beginning with Fly.io, or jump right in here.\n\nRunning a Minecraft server for friends has become an archetypal first foray into the workings of the Internet. For some it‚Äôs learning to expose the tender underbelly of a home network to outside connections. For others it‚Äôs exploring the world of VMs, SSH, and infinite VPS options.\n\nFor me, as for so many, a Minecraft server was an early experience of running a ‚Äúproduction‚Äù web service‚Äîone that others consumed and ‚Äúdepended‚Äù on. Mine was a DigitalOcean droplet held together with glue and duct tape.\n\nJust a few years of experience (and a gig at a cloud-compute company) later, here‚Äôs my new take on this: an over-engineered, scale-to-zero Minecraft server running on a Fly Machine.\n\n\n## Scale-to-Zero?\n\nImagine this: you‚Äôre middle-school me and your Minecraft server has picked up a few more Daily Active Users than you‚Äôd expected. RAM is running low and the mortification of disappointing your peers is fast approaching as VPS resource utilization creeps up.\n\nYou scale the VPS up: more vCPUs, more RAM, smoother gameplay. You are now munching hungrily through the free tier of your hosting provider, or worse, paying money to keep your friends in enchanted boots and rotten flesh. Wouldn‚Äôt it be awesome if the munching could stop when nobody‚Äôs actually playing? Even better if the VM could start up again and carry on automatically when someone attempts to connect again.\n\nThis is the fundamental idea of scale-to-zero on Fly Machines: shut them down when no one is using them, but start them back up again when the user needs it, fast enough that no one is ever the wiser.\n\n\n## The plan\n\nOur magic scale-to-zero Minecraft server takes a few ingredients:\n\n\n## Getting started\n\nIf you don‚Äôt have Terraform yet, now‚Äôs a good time to install it.\n\nSet the FLY_API_TOKEN environment variable. The Terraform provider uses this to authenticate us to the Fly.io API:\n\nIn a new terminal, open a proxy to give Terraform access to the internal APIs we‚Äôll be using. Leave it open:\n\nCreate a new directory to work in:\n\nThen let‚Äôs start our Terraform prep by creating a file called main.tf where we can import the Fly.io provider:\n\nWith this in place, run terraform init to set up your workspace.\n\nIt‚Äôs just a few steps to get started with Terraform and Fly Machines\n\n\n## Let‚Äôs Build!\n\nWe are going to create four different resources:\n\nWe will use the following assumptions for now:\n\nAdd the following blocks to the main.tf we created earlier:\n\nThe first block creates the Fly.io app, as you might guess. From there we have blocks that create a 15GB persistent storage volume and an IPv4 address.\n\nNow we get to the meat of it: the fly_machine block. We start off by defining some basics: the machine name, what app it belongs to, what region it should run in, and what image it should run. In this case we use the super awesome minecraft-server Docker image from itzg.\n\nThe env block sets environment variables used by minecraft-server for configuration; for example, we‚Äôre setting the Autostop feature to shut down the VM when no one‚Äôs been connected for 120 seconds.\n\nThe services block exposes port 25565 to the outside world via the IP we defined earlier, and the mounts block connects the previously defined volume to our machine.\n\nYou may have noticed the MEMORY environment variable that we set to \"7G\". A Minecraft server wants a fair amount of memory, and some CPU oomph to match. So we specify vCPUs and 8G of RAM for this VM.\n\nFinally, with depends_on we tell Terraform to make sure the app and the volume are in place before trying to start a VM.\n\n\n## Some Warnings\n\n\n## Access\n\nAs we configured it here, anyone can join the server. That‚Äôs probably not what you want! Check out the documentation to find out how to set up an allowlist using environment variables.\n\n\n## Costs\n\nMinecraft servers aren‚Äôt exactly lightweight. Or rather: Java isn‚Äôt exactly lightweight. The example code creates a machine with 4 shared vCPUs and 8GB of RAM, and a 15GB storage volume. Vanilla Minecraft should still work fine, if a bit slower, if you tweak down the resources a bit. But we‚Äôre well outside of ‚Äúfree allowances‚Äù territory.\n\nThis is why the ‚Äúscale to zero‚Äù aspect of this project is so useful, of course! However, any TCP traffic will wake up the machine, including things like port scans. It‚Äôll go back to sleep, but the surefire way to prevent it incurring any further costs is to destroy the app. If you‚Äôre not going to use the server again, you‚Äôll want to do this anyway, so you don‚Äôt have to pay for the storage volume.\n\n\n## Let‚Äôs Play!\n\nOnce you have finished tweaking anything you want to tweak in the Terraform file, go ahead and run terraform apply (and confirm when it prompts you) to create all the resources.\n\nOnce the command stops running, open up your Minecraft Java Edition installation, head to the multiplayer screen and connect to flymcapp.fly.dev (once again replacing it with the app name you chose earlier) and find a tree to cut down with your fist!\n\nWhile it defaults to the latest ‚ÄúVanilla‚Äù server, docker-minecraft-server can be configured to run any number of modded servers. Check out the README for configuration options.\n\nOnce you have played around for a few minutes, try quitting out of Minecraft and watch the logs on the Monitoring tab of the Fly.io dashboard. You‚Äôll see that once the configured timeout is hit, it will shut itself down. Try connecting again, you‚Äôll see the machine automatically start itself back up. Cool huh? :)\n\nIf you are done with this guide and don‚Äôt intend to use the server again, go ahead and destroy the app. We have a cornucopia of tools for destruction! Since you created this app with Terraform, you can use terraform destroy; for any Fly.io app, including this one, there‚Äôs also fly apps destroy; or you can hit the red ‚ÄúDelete app‚Äù button under your app‚Äôs Settings tab in the Fly.io dashboard. Check in your dashboard, or use fly apps list, to check that it‚Äôs gone."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/sqlite-virtual-machine/",
    "content": "Fly.io runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. Give us a whirl and get up and running quickly.\n\nSQL is a weird concept. You write your application in one language, say JavaScript, and then send commands in a completely different language, called SQL, to the database. The database then compiles and optimizes that SQL command, runs it, and returns your data. It seems terribly inefficient and, yet, your application might do this hundreds of times per second. It‚Äôs madness!\n\nBut it gets weirder.\n\nSQL was originally designed for non-technical users to interact with the database, however, it‚Äôs used almost exclusively by software developers peppering it throughout their applications.\n\nWhy would this language made for ‚Äúbusiness folks‚Äù become the industry standard for how applications are built?\n\nOne of the key benefits of SQL is that it is declarative. That means you tell the database what you want but not how to do it. Your database knows WAY more about your data than you do so it should be able to make better decisions about how to fetch it and update it. This lets you improve your data layer by adding indexes or even restructuring tables with minimal effects on your application code.\n\nSQLite is unique among embedded databases in that it not only has a transactional, b-tree storage layer but it also includes a robust SQL execution engine. Today, we‚Äôre diving into how SQLite parses, optimizes, \u0026 executes your SQL queries.\n\n\n## A sandwich-making machine\n\nIf you‚Äôve read our previous sandwich-themed SQLite blog posts on the SQLite file format, the rollback journal, \u0026 the WAL, then you‚Äôre probably feeling pretty hungry by now. You‚Äôre also probably tired of the tedium of making sandwiches by hand, so we‚Äôll use a sandwich-making machine as our analogy in this blog post.\n\nThis machine will do a few tasks:\n\nThe process for building and executing SQL queries is similar to this sandwich-building process, albeit less delicious. Let‚Äôs dive in.\n\n\n## Teaching our machine to read\n\nThe first step is to give our machine an order. We hand it an order slip that says:\n\nTo our computer, this order is just a string of individual characters: M, a, k, e, etc‚Ä¶ If we want to make sense of it, we first need to group these letters together into words, or more specifically, ‚Äútokens‚Äù. This process is called ‚Äútokenizing‚Äù or ‚Äúlexing‚Äù.\n\nAfter tokenizing, we see this list of tokens:\n\nFrom there, we start the parsing stage. The parser takes in a stream of tokens and tries to structure it some way that makes sense to a computer. This structure is called an Abstract Syntax Tree, or AST.\n\nThis AST for our sandwich command might look like this:\n\nFrom here, we can start to see how we might take this definition and begin building sandwiches from it. We‚Äôve added structure to an otherwise structure-less blob of text.\n\n\n## Lexing \u0026 Parsing SQL\n\nSQLite does this same process when it reads in SQL queries. First, it groups characters together into tokens such as SELECT or FROM. Then the parser builds a structure to represent it.\n\nThe SQLite documentation provides helpful ‚Äúrailroad diagrams‚Äù to represent the paths the parser can take when consuming the stream of tokens. The SELECT definition shows how it can start with the WITH keyword (for CTEs) and then move into the SELECT, FROM, and WHERE clauses.\n\nWhen the parser is done, it outputs the aptly named Select struct. If you had a SQL query like this:\n\nThen you‚Äôll end up with an AST that looks something like this:\n\n\n## Determining the best course of action\n\nSo now that we have our sandwich order AST, we have a plan to make our sandwich, right? Not quite.\n\nThe AST represents what you want‚Äîwhich is a couple of sandwiches. It doesn‚Äôt tell us how to make the sandwiches. Before we get to the plan, though, we need to determine the optimal way to make the sandwiches.\n\nOur sandwich-making machine can assemble a plethora of different sandwiches, so we stock all kinds of ingredients. If we were making a monster sandwich loaded with most of our available toppings it might make sense for the machine to visit each ingredient‚Äôs location, using it, or not, according to the AST.\n\nBut for our BLT, we need only bacon, lettuce \u0026 tomato. It‚Äôll be way faster if we can have the machine look up the locations of just these three toppings in an index and jump directly between them.\n\nSQLite has a similar decision to make when planning how to execute a query. For this, it uses statistics about its tables‚Äô contents.\n\n\n## Using statistics for faster queries\n\nWhen SQLite looks at an AST, there could be hundreds of ways to access the data to fulfill a query. The naive approach would be to simply read through the whole table and check every row to see if it matches. This what we in the biz call a full table scan and it is painfully slow if you only need a few rows from a large table.\n\nAnother option would be to use an index to help you quickly jump to the rows you need. An index is a list of row identifiers that are sorted by one or more columns, so if we have an index like this:\n\nThen all the row identifiers for people who love ‚Äúmauve‚Äù are all grouped together in our index. Using the index for our query means we have to first read from the index and then jump to a row in the table. This has a higher cost per row as it requires two lookups, however almost no one likes mauve so we don‚Äôt have too many matching rows.\n\nBut what happens if you search for a popular color like ‚Äúblue‚Äù? Searching the index first and then jumping to our table for so many rows would actually be slower than if we simply searched the entire table.\n\nSo SQLite does some statistical analysis on our data and uses this information to choose the (probably) optimal recipe for each query.\n\nSQLite‚Äôs statistics are stored in several ‚Äúsqlite_stat‚Äù tables. These tables have evolved over the years so there‚Äôre 4 different versions of stats but only two are still in use with recent versions of SQLite: sqlite_stat1 \u0026 sqlite_stat4.\n\nThe sqlite_stat1 table has a simple format. It stores the approximate number of rows for each index and it stores the number of duplicate values for the columns of the index. These coarse-grained stats are the equivalent of tracking basic averages for a data set‚Äîthey‚Äôre not super accurate but they‚Äôre quick to calculate and update.\n\nThe sqlite_stat4 table is a bit more advanced. It will store a few dozen samples of values that are spread across an index. These finer-grained samples mean that SQLite can understand how unique different values are across the key space.\n\n\n## Executing on our plan\n\nOnce we have an optimized plan for building a sandwich, we should have our machine write it down. That way if we get the same order again in the future, we can simply reuse the plan rather than having to parse \u0026 optimize the order each time.\n\nSo what does this sandwich plan look like?\n\nThe plan will be recorded as a list of commands that the machine can execute to build the BLT again in the future. We don‚Äôt want a command for each type of sandwich, as we may have a lot of different types. Better to have a set of common instructions that can be reused to compose any sandwich plan.\n\nFor example, we might have the following commands:\n\nWe also have one more requirement that‚Äôs not immediately obvious. We only have so much space to hold our finished sandwiches so we need to make one sandwich at a time and have the customer take it before making the next sandwich. That way we can handle any number of sandwiches in an order.\n\nThis process of handing off is called yielding so we‚Äôll have a YIELD command when where we wait for the customer to take the sandwich.\n\nWe‚Äôll also need some control flow so we can make multiple of the same kind of sandwich so we‚Äôll add a FOREACH command.\n\nSo putting our commands together, our plan might look like:\n\nThis set of domain-specific commands and the execution engine to run it is called a virtual machine. It gives us a level of abstraction that‚Äôs appropriate for the task we‚Äôre trying to complete (e.g. sandwich making) and it lets us reconfigure commands in different ways for different sandwiches.\n\n\n## Inspecting the SQLite Virtual Machine\n\nSQLite‚Äôs virtual machine is structured similarly. It has a set of database-related commands that can execute the steps needed to fetch the results of a query.\n\nFor example, let‚Äôs start with a table of people with a few rows added:\n\nWe can inspect this with two different SQLite commands. The first command is called EXPLAIN QUERY PLAN and it gives a very high level plan of the query. If we run it for a simple SELECT with a conditional then we‚Äôll see that it performs a table scan of the persons table:\n\nThis command can give more information as you do more complex queries. Now let‚Äôs look at the other command to further inspect the plan.\n\nConfusingly, it‚Äôs called the EXPLAIN command. Simply drop the ‚ÄúQUERY PLAN‚Äù part of the first command and it will show a much more detailed plan:\n\nThis is the ‚Äúplain English‚Äù representation of the byte code that your query is compiled down to. This may look confusing but we can walk through it step-by-step to break it down.\n\n\n## The SQLite virtual machine instruction set\n\nJust like how a computer has low-level CPU operations such MOV and JMP, SQLite has a similar instruction set but it‚Äôs just at a higher level. As of this writing, there are 186 commands, or opcodes, that the SQLite VM can understand. You can find the full specification on the SQLite web site but we‚Äôll walk through a couple of them here.\n\nThe first opcode is an Init which initializes our execution and then jumps to another instruction in our program. The parameters for the opcodes are listed as p1 through p5 and their definition is specific to each command. For the Init opcode, it jumps to the instruction listed in p2 which is 11.\n\nAt address 11 we arrive at the Transaction opcode which starts our transaction. For most opcodes, the VM will move to the next address after executing the instruction so we move to address 12. This String8 opcode stores string value \"blue\" into register r[2]. The registers act like a set of memory addresses and are used to store values during execution. We‚Äôll use this value later for our equality comparison.\n\nNext, we move to address 13 which is a Goto instruction which has us jump to the instruction listed in its p2 parameter, which is address 1.\n\nNow we get into the row processing. The OpenRead instruction opens a cursor on the persons table. A cursor is an object for iterating over or moving around in a table. The next instruction, Rewind, moves the cursor to the first entry of the database to begin our table scan.\n\nThe Column instruction reads the favorite_color column into register r[1] and the Ne instruction compares it with the \"blue\" value in register r[2]. If the values don‚Äôt match then we‚Äôll move to the Next instruction at address 9. If they do match, we‚Äôll fill in registers r[3] , r[4], \u0026 r[5] with the column id, name, \u0026 favorite_color for the row.\n\nFinally, we get to where we can yield the result back to the caller using the ResultRow instruction. This will let the calling application copy out the values in registers r[3‚Ä¶5]. When the calling application calls sqlite3_step(), the program will resume from where it left off by calling Next and jumping back to re-execute the row processing at instruction 3.\n\nWhen Next no longer produces any more rows, it‚Äôll jump to the Halt instruction and the program is done.\n\n\n## Wrapping up our sandwich processing engine\n\nThe query execution side of SQLite follows this simple parse-optimize-execute plan on every query that comes into the database. We can use this knowledge to improve our application performance. By using bind parameters in SQL statements (aka those ? placeholders), we can prepare a statement once and skip the parse \u0026 optimize phases every time we reuse it.\n\nSQLite uses a virtual machine approach to its query execution but that‚Äôs not the only approach available. Postgres, for example, uses a node-based execution plan which is structured quite differently.\n\nNow that you understand the basics of how an execution plan works, try running EXPLAIN on one of your more complex queries and see if you can understand the step-by-step execution of how your query materializes into a result set for your application."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/sqlite-internals-wal/",
    "content": "Fly.io runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. Give us a whirl and get up and running quickly.\n\nIf you scour Hacker News \u0026 Reddit for advice about databases, some common words of caution are that SQLite doesn‚Äôt scale or that it is a single-user database and it‚Äôs not appropriate for your web-scale application.\n\nLike any folklore, it has some historical truth. But it‚Äôs also so wildly out-of-date.\n\nIn our last post, we talked about the rollback journal. That was SQLite‚Äôs original transactional safety mechanism and it left much to be desired in terms of scaling.\n\nIn 2010, SQLite introduced a second method called the write-ahead log, or as it‚Äôs more commonly referred to: the WAL.\n\n\n## Refresher on the rollback journal\n\nThe rollback journal worked by copying the old version of changed pages to another file so that they can be copied back to the main database file if the transaction rolls back.\n\nThe WAL does the opposite. It writes the new version of a page to another file and leaves the original page in-place in the main database file.\n\nSo how does this simple change enable SQLite to scale? Let‚Äôs revisit our sandwich shop example from the last post to see how the WAL would make things run more smoothly.\n\n\n## Sandwiches at scale\n\nIn our sandwich shop example, we had to choose between a single sandwich maker making sandwiches and one or more inventory specialists inventorying ingredients. We couldn‚Äôt do both at the same time. This is how the rollback journal works; a writer can alter the database or readers can read from the database‚Äîbut not both at the same time.\n\nThis is a problem since making a complicated, time-consuming sandwich will prevent inventory from being taken. Also, a single slow inventory counter will prevent any sandwiches from being made.\n\nA simple solution would be to take a photo of every ingredient after a sandwich is made. That way inventory counters could look at the photos to take inventory. However, it would be slow and inefficient to take a photo of every ingredient after a sandwich since many ingredients wouldn‚Äôt change. For example, if you‚Äôre making a grilled cheese then you‚Äôre not going to touch the pickles, right? Right!?\n\nA better solution would be to only take photos of the ingredients you took from after each sandwich. You can add these photos to a binder and now the inventory folks can see a point-in-time snapshot of the ingredients without interfering with the sandwich maker.\n\nThis is how the WAL works, in concept.\n\n\n## Enabling the sandwich log\n\nLet‚Äôs create a sandwiches.db that will store our current count of ingredients:\n\nTo enable our write-ahead log journaling mode, we just need to use the journal_mode PRAGMA:\n\nInternally, this command performs a rollback journal transaction to update the database header so it‚Äôs safe to use like any other transaction. The database header has a read \u0026 write version at bytes 18 \u0026 19, respectively, that are used to determine the journal mode. They‚Äôre set to 0x01 for rollback journal and 0x02 for write-ahead log. They‚Äôre typically both set to the same value.\n\n\n## Altering the ingredient count\n\nNow that we are using the WAL, we can add our initial inventory counts:\n\nInstead of updating our sandwiches.db database file, our change is written to a sandwiches.db-wal file in the same directory. Let‚Äôs fire up our hexdump tool and see what‚Äôs going on.\n\n\n## Starting with the WAL header\n\nThe WAL file starts with a 32-byte header:\n\nMost SQLite files start with a magic number and the WAL is no exception. Every WAL file starts with either 0x377f0682 or 0x377f0683 which indicate whether checksums in the file are in little-endian or big-endian format, respectively. Nearly all modern processors are little-endian so you‚Äôll almost always see 0x377f0682 as the first 4 bytes of a SQLite WAL file.\n\nNext, the 0x002de218 is the WAL format version. This is the big-endian integer representation of 3007000 which means it‚Äôs the WAL version created in SQLite 3.7.0. There‚Äôs currently only one version number for WAL files.\n\nThe next four bytes, 0x00001000, are the page size. We‚Äôre using the default page size of 4,096 bytes. The next four after that are 0x00000000 which is the checkpoint sequence number. This is a number that gets incremented on each checkpoint. We‚Äôll discuss checkpointing later in this post.\n\nAfter that, we have an 8-byte ‚Äúsalt‚Äù value of 0x5a20ee38f926b5d3. The term ‚Äúsalt‚Äù is typically used in cryptography (and sandwiches, actually) but in this case it‚Äôs a little different. Sometimes the WAL needs to restart from the beginning but it doesn‚Äôt always delete the existing WAL data. Instead it just overwrites the previous WAL data.\n\nIn order for SQLite to know which WAL pages are new and which are old, it writes the salt to the WAL header and every subsequent WAL frame. If SQLite encounters a frame whose salt doesn‚Äôt match the header‚Äôs salt, then it knows that it‚Äôs a frame from an old version of the WAL and it ignores it.\n\nFinally, we have an 8-byte checksum of 0x0dd5236d9972220b which is meant to verify the integrity of the WAL header and prevent partial writes. If we accidentally overwrite half the header and then the computer shuts down, we can detect that by calculating and comparing the checksum.\n\n\n## Adding WAL entries\n\nThe WAL works by appending new data to the -wal file so we‚Äôll see an entry for a single page in our WAL file. It starts with a 24-byte header and then writes 4,096 bytes of page data.\n\nThe first 4 bytes, 0x00000002, are the page number for the entry. This is saying that our page data following the header is meant to overwrite page 2.\n\nThe next 4 bytes, also 0x00000002, indicate the database size, in pages, after the transaction. This field actually performs double duty. For transactions that alter multiple pages, this field is only set on the last page in the transaction. Earlier pages set it to zero. This means we can delineate sections of the WAL by transaction. It also means that a transaction isn‚Äôt considered ‚Äúcommitted‚Äù until the last page is written to the WAL file.\n\nAfter that we see our salt value 0x5a20ee38f926b5d3 copied from the header. This lets us know that it is a contiguous block of WAL entries starting from the beginning of the WAL. Finally, we have an 8-byte checksum of 0x5333783e42122b82 which is computed based on the WAL header checksum plus the data in the WAL entry and page data.\n\n\n## Overlaying our WAL pages\n\nEvery transaction that occurs will simply write the new version of changed pages to the end of the WAL file. This append-only approach gives us an interesting property. The state of the database can be reconstructed at any point in time simply by using the latest version of each page seen in the WAL starting from a given transaction.\n\nIn the diagram below, we have the logical view of a b-tree inside SQLite and an associated WAL file with 3 transactions. Before the WAL file exists, we have three pages in our database, represented in black.\n\nThe first transaction (in green) updates pages 1 and 2. A snapshot of the database after this transaction can be constructed by using WAL entries for pages 1 and 2 and the original page from the database for page 3.\n\nOur second transaction (in red) only updates page 2. If we want a snapshot after this transaction, we‚Äôll use page 1 from the first transaction, page 2 from the second transaction, and page 3 from the database file.\n\nThe last transaction (in orange) updates pages 2 \u0026 3 so the entire b-tree is now read from the WAL.\n\nThe beauty of this approach is that we are no longer overwriting our pages so every transaction can reconstruct its original state from when it started. It also means that write transactions can occur without interfering with in-progress read transactions.\n\n\n## Retiring our gastronomical photo album\n\nYou may have noticed one problem with our append-only album of ingredient photos‚Äîit keeps getting bigger. Eventually it will become too large to handle. Also, we really don‚Äôt care about the ingredient photos that we took 400 sandwiches ago. We only want to allow sandwich makers and inventory counters to do their work at the same time.\n\nIn SQLite, we resolve this issue with the ‚Äúcheckpointing‚Äù procedure. Checkpointing is when SQLite copies the latest version of each page in the WAL back into the main database file. In our diagram below, page 1 is copied from the first transaction but pages 2 \u0026 3 are copied from the third transaction. The prior versions of page 2 are ignored because they are not the latest.\n\nSQLite can perform this process incrementally when old transactions are not in use but eventually, it needs to wait until there are no active transactions if it wants to fully checkpoint and restart the WAL file.\n\nThis naive approach can be problematic on databases that constantly have open transactions as SQLite will not force a checkpoint on its own and the WAL file can continue to grow. You can force SQLite to block new read \u0026 write transactions so it can restart the WAL by issuing a wal_checkpoint PRAGMA:\n\nor\n\n\n## Building a photo index\n\nAs our photo album grows, it gets slower to find the latest version of each photo in order to reconstruct our sandwich shop state. You have to start from the beginning of the album and find the last version of a page every time you want to look up a photo.\n\nA better option would be to have an index in the photo album that lists all the locations of photos for each ingredient. Let‚Äôs say you have 20 photos of banana peppers. You can look up ‚Äúbanana peppers‚Äù in the index and find the location of the latest one in the album.\n\n\n## Writing our index\n\nSQLite builds a similar index and it‚Äôs stored in the ‚Äúshared memory‚Äù file, or SHM file, next to the database and WAL files.\n\nBut SQLite‚Äôs index is a bit funny looking at first glance: it‚Äôs a list of page numbers and a hash map. The goal of the index is to tell the SQLite client the latest version of a page in the WAL up to a given position in the WAL. Since each transaction starts from a different position in the WAL, they can have different answers to exactly which version of a page they see.\n\nThe SHM index is built out of 32KB blocks that each hold 4,096 page numbers and a hash map of 8,192 slots. When WAL entries are written, their page numbers are inserted into the SHM‚Äôs page number list in WAL order. Then a hash map position is calculated based on the page number and the index of the page number is stored in the hash map.\n\nClear as mud? Let‚Äôs walk through an example.\n\nIn the diagram above, our first transaction in the WAL (green) updates pages 1 \u0026 2. They get written to the WAL, but the page numbers are also added to the page numbers list in the SHM file. SQLite calculates a hash map slot position for each page using the formula: (pgno * 383)%8192. In the hash map slot, we‚Äôll write the index of the page in our page number list. This is also 1 \u0026 2, respectively. Don‚Äôt read too much into the exact hash map positions in the diagram. There‚Äôs some loss of fidelity in simplifying 8,192 slots down to 14!\n\nThis hash-based mapping will generally spread out our page numbers across our hash map and leave empty space between them. Also, we are guaranteed to have a lot of empty slots in the hash map since there are double the number of hash map slots as there are page number spots. This will be useful when looking up our pages later.\n\nOur next transaction (red) only updates page 2. We‚Äôll write that page to the third entry in our page number list. However, when we calculate our hash map position, we have a collision with the entry for page 2 in transaction 1. They both point at the same slot. So instead of writing to that slot, we‚Äôll write our page number list index, 3, to the next empty slot.\n\nFinally, our third transaction updates pages 2 \u0026 3. We‚Äôll write those to indexes 4 \u0026 5 in our page number list and then write those indexes to our hash map slots. Again, our page 2 collides with updates in the first two transactions so we‚Äôll write the index to the first empty slot after the hash map position.\n\n\n## Reading our index\n\nNow that we have our index built, we can quickly look up the latest version of any given page for a transaction. Let‚Äôs say we‚Äôve started a read transaction just after transaction #2 completed. We only want to consider versions of pages in the WAL up to entry #3 since WAL entries in index 4 \u0026 5 occurred after our read transaction started.\n\nIf we want to look up page 2, we‚Äôll first calculate the hash position and then read all the indexes until we reach an empty slot. For page 2, this is indexes 2, 3, \u0026 4. Since our transaction started at entry #3, we can ignore entries after index 3 so we can discard index 4. Out of this set, index 3 is the latest version so our SQLite transaction will read page 2 from WAL entry 3.\n\nAstute readers may notice that we can have collisions across multiple pages. What happens if page 2 \u0026 page 543 in a database compute the same slot? SQLite will double check each entry in the page numbers list to make sure it‚Äôs the actual page we‚Äôre looking for and it will discard any others automatically.\n\n\n## Choosing a journaling strategy\n\nWhile there are always trade-offs between design choices, the vast majority of applications will benefit from WAL mode. The SQLite web site helpfully lists some edge cases where the rollback journal would be a better choice such as when using multi-database transactions. However, those situations are rare for most applications.\n\nNow that you understand how data is stored and transactions are safely handled, we‚Äôll take a look at the query side of SQLite in our next post which will cover the SQLite Virtual Machine."
  },
  {
    "title": "Reverse CTA",
    "url": "https://fly.io/blog/remote-ide-machines/",
    "content": "Fly.io upgrades containers to full-fledged virtual machines running on our hardware around the world, connected with WireGuard to a global Anycast network. This post is about one fun thing you can run on a VM. Check us out: your app can be running in minutes.\n\n‚ÄúRemote development environment!‚Äù\n\nWhether you reacted with a thrill of enthusiasm, a surge of derision or a waft of indifference, we‚Äôre not really here to change your mind. That phrase means a lot of different things at this point in history. The meaning we pick today is ‚Äúnerd snipe.‚Äù\n\nLet‚Äôs set up a remote in-browser IDE, configured for Elixir / Phoenix development, the hard way‚Äîthat is: using the command line and a Dockerfile.\n\nBrowser-based IDEs are a thing, and we happen to have a rather convenient infrastructure at our fingertips for deploying web apps (plug! CTA!). When we say ‚Äúthe hard way‚Äù, we have to concede that there are harder ways! We‚Äôre not writing an IDE; we‚Äôre not forwarding X11; it‚Äôs all relative.\n\n\n## What is this?\n\nWe‚Äôve leaned away from blog posts with a lot of code blocks in them. Too many code blocks make our eyes glaze over. But we really wanted to show off a fun way to play with Fly Machines, which are VMs that you manage directly. And a personal remote development environment is just the ticket: for individual use, we don‚Äôt need load-balancing, or lots of instances, or always-on‚Äîin fact, it‚Äôs better if we can turn it on and off.\n\n2023 update: In this article we ‚Äúmanually‚Äù set up and deploy an app on a Fly Machine. The Fly Apps V2 platform runs on Fly Machines, so now you can use fly launch to deploy this same app, including the sleep-when-idle feature.\n\nSo here we are. Once we got those code blocks flowing, we didn‚Äôt stop until we‚Äôd deployed, step-by-step, not one, but two separate apps on Fly.io.\n\n\n## What you‚Äôre in for\n\nIf you perform the ritual to completion, you‚Äôll have deployed an Elixir / Phoenix / SQLite hello-world app to Fly.io, from your own personal Elixir / Phoenix development environment that you‚Äôve configured and deployed on Fly.io, complete with¬†code-server IDE. You‚Äôll use the power of Machines to get the VM to go to sleep when you‚Äôre not using it, making it cheaper to run.\n\nThe steps to get there look roughly like:\n\n\n## Costs\n\nBetween the remote development VM and the Phoenix app we‚Äôll deploy as part of the demo, we‚Äôre planning to provision two VMs. The Phoenix app fits into the Fly.io free allowance for compute. We recommend giving the remote development environment 1GB of RAM, which takes it past our free allowance: it would cost a few dollars a month to run it full-time. We‚Äôll provision two volumes with a total of 3GB capacity, which exactly matches the free storage allowance on our Hobby Plan.\n\nDestroying both new apps after finishing the tutorial (if you have no further use for them) will keep costs minimal.\n\n\n## The thing we want to make\n\nAs we‚Äôve mentioned, code-server, an in-browser VS Code, is our IDE of choice for today‚Äôs exercise. It‚Äôs far from the only possibility, looking at the range of VS Code-flavoured possibilities alone (starting with zero amount of VS Code, and stopping short of fully-managed services). Say we‚Äôre connecting over SSH. A perfectly good remote dev environment, according to some people, would be tmux and Vim over SSH. VS Code and an SSH tunnel is a more comfortable option for most of us. If we can‚Äôt, or don‚Äôt want to, install VS Code on our local device, code-server makes it into a web app. We can get at code-server over SSH too.\n\nWe can also get code-server in the browser over an HTTPS connection (with a Let‚Äôs Encrypt certificate and a TLS-terminating proxy), and put a password in front of it. Since Fly.io will provide us the cert and proxy almost without us noticing, that‚Äôs the instant-gratification route. We‚Äôll go that way today, and skip over questions of SSH and WireGuard. If you want to talk about SSH and Fly.io remote dev setup, go see Amos.\n\nWe‚Äôll build from a Dockerfile. Our Docker image will hold a clean-slate (but mostly-configured) development environment. If we ever get mired in dependency hell, or our SSD goes fizzle, we can deploy a fresh machine using that.\n\nWorking files will live on a persistent storage volume. Nothing stops us checking our work into a remote Git repository regularly too.\n\nWe want to shut it down when it‚Äôs not in use (because we don‚Äôt get charged for CPU or RAM while the Fly Machine is stopped). Machines can be started and stopped manually using their REST API or flyctl, but here we‚Äôll also run a proxy called Tired Proxy that shuts the server down if it doesn‚Äôt get any HTTP requests for some time. As for waking it up: Fly‚Äôs proxy itself tries to wake machines for HTTP requests (and TCP connections).\n\n\n## Begin construction\n\nLet‚Äôs get our hands dirty, starting by walking through the three files we use to build our image. If you don‚Äôt want to type them in, you can clone the repo: git clone https://github.com/fly-apps/code-server-dev-environment.git\n\nIf you really don‚Äôt want to bash out the commands, we do have a Code Server Launcher. It will immediately deploy the same remote dev environment we‚Äôre creating in this demo.\n\n\n## Dockerfile\n\nTo summarize that:\n\nIt‚Äôs a multi-stage build that gets the code for Tired Proxy from its public Docker image, and uses an official Elixir base image to save us the trouble of finding things like Elixir, Mix, etc., that every Elixir dev environment should have.\n\nIn case you‚Äôre interested, here‚Äôs the source for the Tired Proxy: https://github.com/superfly/tired-proxy. This is one of my first Go experiences, so try not to be too traumatized.\n\nOver that, it installs some other things we know we want, including code-server (our IDE), flyctl (so we can deploy apps from the code-server terminal), and the Elixir Language Server extension for code-server (to make developing Elixir apps more comfy).\n\nIt copies in two other files from the local working directory: settings.json (just to get the dark theme in VS Code) and entrypoint.sh (a shell script which encapsulates all the things the VM should do every time it starts up). The Tired Proxy executable is copied from the first stage.\n\nFinally, it sets ENTRYPOINT to run /entrypoint.sh.\n\n\n## entrypoint.sh\n\nHere‚Äôs what that does:\n\nFirst, it sets the TIME_TO_SHUTDOWN environment variable to 3600 seconds (1 hour). This is used in the tired-proxy command later on.\n\nIt creates a folder for the Elixir project to live in, if one doesn‚Äôt already exist. The -p tag prevents errors in the case that /project already exists (as it should the second time you start the VM).\n\nIt initializes the environment, if that hasn‚Äôt already happened, by cloning project files from the repo indicated in the GIT_REPO environment variable (which we‚Äôll set when we run the VM), installing Hex and Rebar locally (non-interactively, with the --force flag), and getting project dependencies.\n\nFinally, the trick we have up our sleeve: it spawns a code-server, with the /project folder open, listening on port 9090‚Äîbut we don‚Äôt expose this port directly. Tired Proxy maps port 8080 to 9090, and if there‚Äôs no incoming HTTP connection for $TIME_TO_SHUTDOWN seconds, it exits. That‚Äôs it. That‚Äôs the whole trick.\n\n\n## settings.json\n\nWe provide a settings.json just to get the dark theme in our IDE.\n\nIf you‚Äôre a VS Code user, you can provide your own preferences in this file.\n\n\n## Over to flyctl\n\nIf you‚Äôre new to Fly.io, install flyctl, the Fly.io CLI tool, and run fly auth signup. If you already have flyctl installed, it‚Äôs worth making sure it‚Äôs up to date with fly version update.\n\n\n## flyctl vs fly\n\nThis is more of a secret than it should be, but flyctl uses fly as an alias for flyctl. So if I type fly secrets and the docs are for flyctl secrets, that‚Äôs all that‚Äôs about.\n\n\n## Prepare the Fly Machines App\n\n‚ÄúFly Machines App?‚Äù Let‚Äôs back up just a bit.\n\nMachines are basically just one level lower than apps. They‚Äôre VMs you can create, destroy, start, and stop directly through a REST API or with flyctl. The Fly.io platform still needs to keep track of these VMs‚Äîwho they belong to, where to route requests, all that. A machines app is where machine VMs store their important documents: their passports, Sears, Roebuck \u0026 Co. stock certificates, public IP addresses, etc. A machines app doesn‚Äôt run your code unless it has at least one Machine to provide the code‚Äînot to mention the CPU and RAM.\n\nThis is not so different from the kind of app we‚Äôre used to talking about: the kind that our orchestrator keeps running, to the best of its ability, until you scale to zero or destroy it. In both cases, you have VMs, and you have some centrally-stored information. Conceptually, it‚Äôs probably not helpful to hold onto a distinction between ‚Äúmachines apps‚Äù and ‚Äúapp apps‚Äù. But for the moment, there‚Äôs a practical difference in the way these two cases are implemented, so when it‚Äôs time to register a new app on Fly.io to shuffle paperwork for your code-server VM, you need to create a Machines App.\n\nRegister a new machines app on Fly.io with a name and an organization. The remote IDE URL will be \u003cyour-app-name\u003e.fly.dev, so choose well.\n\nMachines, and flyctl, are evolving so fast that gingerly configuring everything step-by-step like this is going to look positively quaint any minute now. We‚Äôre chasing a moving target here.\n\nCreate a new volume called storage, tied to this app, with size 2GB. You can choose to make it smaller. The VM will be tied to the hardware this volume is on.\n\nBy default, Fly.io apps have private IPV6 addresses for use within their organization‚Äôs WireGuard network. If you want to access this app without a WireGuard tunnel, it needs a public IP.\n\nSince you‚Äôre not running fly deploy on this app, you need to allocate this manually.\n\nUse fly secrets to pass in secret environment variables. Note the --stage flag, which is needed (at this time) because setting secrets on a machines app triggers a deploy by default, and we don‚Äôt want that in our case.\n\ncode-server asks for a password when you first open it. It will generate its own random password on installation if the PASSWORD environment variable isn‚Äôt set. You can still ssh into the VM later and extract it from a config file, but it‚Äôs easier to set it ahead.\n\nProviding your fly auth token allows you to deploy other apps to Fly.io from within this app. Trippy!\n\n\n## Run the machine for the first time\n\nHere‚Äôs the invocation to bring the code-server VM into being! Your app will be discoverable on the Internet as soon as the VM is up and listening for requests.\n\nThe above command should result in output close to this:\n\n\n## Time to (pretend to) code!\n\nNow we can visit your freshly-minted remote development environment at \u003cyour-app-name\u003e.fly.dev. It‚Äôll ask for the password you set earlier with fly secrets set. Fear not the light theme. Once settings.json is read in, it‚Äôll switch to dark.\n\nOnce you‚Äôre in, you‚Äôll see the /project folder open, complete with the cloned hello_elixir_sqlite project files. README.md contains instructions for building, previewing, and deploying that app. In case you‚Äôre just following along in your imagination, we‚Äôll repeat them here.\n\nOpen the integrated VS Code terminal with ^+`.\n\nRun mix phx.server to compile the app and start up the dev server. When that finishes, you can check it out in the browser at \u003cyour-app-name\u003e.fly.dev:4000. You‚Äôll recall you exposed port 4000 when you created the machine with fly machine run. This does mean everybody can see the dev server at that address, because this Phoenix app doesn‚Äôt have any password protection.\n\nAfter dutifully clicking the button to run migrations, you should see something like this:\n\nIt was tempting to link back to this post, but you‚Äôre already here. Read the Machines announcement to see more about why we‚Äôre stoked.\n\n\n## flyctl Round two: Deploy to Fly.io from Fly.io\n\nIn the VM‚Äôs Dockerfile, you installed flyctl, so you can run any fly command from the integrated terminal. You‚Äôre authed, because you set the FLY_API_TOKEN secret, which the CLI will read from the environment if it‚Äôs available.\n\nIt‚Äôs time for the second round of app-configuration, secret-setting, and volume-creation with flyctl, this time all for your Phoenix app.\n\nRegister the new app, but don‚Äôt deploy it. New app, new name. Use the fly.toml provided by the project repo. Deploy it wherever you like‚Äîit‚Äôs a whole independent app.\n\nGenerate a new secret for the Phoenix app.\n\nSet that as the secret SECRET_KEY_BASE that the app will have access to.\n\nThis sample app is configured to use an SQLite database, so you need some storage.\n\nProvision a 1GB volume in the same region as the Phoenix app.\n\nThat‚Äôs it! Deploy the Phoenix app.\n\nIt‚Äôll go live at https://your-phoenix-app-name.fly.dev.\n\nYou can stop the machine using fly machine stop, and revive it just by visiting the app in your browser (or with fly machine start). If you close the tab, it will just sleep after an hour without activity.\n\nDelete your Code Server app if you‚Äôre done with it. Also destroy the Phoenix app if you don‚Äôt want that!\n\n\n## Whew\n\nThis project was built to demonstrate our new Fly Machines feature, and how simple it can be to launch an app like code-server with it.\n\nThe Dockerfile serves as an example of how you can customize your setup ready to do some work, and it does some heavy lifting: cloning the repo, installing flyctl, and installing an entire Elixir developer environment. It wouldn‚Äôt be hard to swap out the Elixir bits for Ruby ones, if that‚Äôs your bag!\n\nFly machines are very keen to start themselves if someone reaches them over HTTP (thanks, fly-proxy), but they won‚Äôt stop by themselves with the code-server process running. You can reuse our Tired Proxy to send a VM to sleep, so you don‚Äôt get billed for it 24/7. One caveat: with this simple setup, if a random bot hits your port 8080, fly-proxy will treat that the same as you opening up the app in a tab‚Äîand try to wake the machine. Oh. Two caveats: Our experiments indicate that leaving a code-server tab open with the terminal pane active may keep it alive too.\n\n2023 update: Fly Proxy now has a built-in configurable feature for autostarting or autostopping Machines based on incoming traffic.\n\nYou can also start and stop and remove machines using flyctl or the REST API; you can write a little app with ‚Äústart‚Äù and ‚Äústop‚Äù buttons for your machines if you want to play. Obviously, the idea is that you could use the Machine API to write much bigger, more interesting apps than that. We leave that as an exercise for the reader!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/sqlite-internals-rollback-journal/",
    "content": "Fly.io runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. Give us a whirl and get up and running quickly.\n\nWhen database vendors recite their long list of features, they never enumerate ‚Äúdoesn‚Äôt lose your data‚Äù as one of those features. It‚Äôs just assumed. That‚Äôs what a database is supposed to do. However, in reality, the best database vendors tell you exactly how their database will lose your data.\n\nI‚Äôve written before about how SQLite stores your data. In order not to lose any of it when a transaction goes wrong, SQLite implements a journal. It has two different modes: the rollback journal \u0026 the write-ahead log. Today we‚Äôre diving into the rollback journal: what it is, how it works, and when to use it.\n\n\n## How to lose your data\n\nTo understand why you need a database journal, let‚Äôs look at what happens without one. In the last post, we talked about how SQLite is split up into 4KB chunks called ‚Äúpages‚Äù. Any time you make a change‚Äîeven a 1 byte change‚ÄîSQLite will write a full 4KB page.\n\nIf you tried to overwrite a page in your database file directly, it would work fine 99% of the time. However, that 1% of the time is catastrophic. If your server suddenly shut down halfway through a page write then you‚Äôll end up with a corrupted database.\n\nThe database needs to ensure that all page writes for a transaction either get written or don‚Äôt. No halfsies. This is called atomicity.\n\nBut that‚Äôs not all. If another process is querying the database, it‚Äôll have no consistent view of the data since you‚Äôre overwriting pages willy-nilly. The database needs to ensure each transaction has a snapshot view of the database for its entire duration. This is called isolation.\n\nFinally, we need to make sure bytes actually get flushed to disk. This part is called durability.\n\nThose make up 3 of the 4 letters of the ACID transactional guarantee that every database blog post is required to mention. The ‚ÄúC‚Äù stands for consistency but that doesn‚Äôt involve the rollback journal so we‚Äôll skip that.\n\n\n## All for one, or none at all\n\nEvery textbook definition of transactions involves a bank transfer where someone withdraws money from one account and deposits in another. Both actions must happen or neither must happen.\n\nThis example gets trotted out because atomicity is so unusual in the physical world that it‚Äôs hard to find anything else that‚Äôs as intuitive to understand.\n\nBut it turns out that atomicity doesn‚Äôt ‚Äújust happen‚Äù in databases either. It‚Äôs all smoke and mirrors. So let‚Äôs use a better example that involves our favorite topic: sandwiches.\n\n\n## Building a sandwich\n\nWhen you go to a sandwich shop, you walk up to the counter, announce your order, and you get a tasty sandwich in hand a short time after. To you, the consumer, this is atomic. If you order a ham-and-cheese sandwich, you won‚Äôt receive just a slice of ham or two pieces of dry bread. You either get a sandwich or you don‚Äôt.\n\nBut behind the counter, there are multiple steps involved: grab the bread, add the ham, add the cheese, hand it to the customer. If the sandwich maker gets to the cheese step and realizes they‚Äôre out of cheese, they can tell you they can‚Äôt make the sandwich and then put the ham and bread back where they found it. The internal state of the sandwich shop is restored to how it was before the order started.\n\nThe rollback journal is similar. It records the state of the database before any changes are made. If anything goes wrong before we get to the end, we can use the journal to put the database back in its previous state.\n\n\n## Our first transaction\n\nLet‚Äôs start our first transaction by creating a table in a sandwiches.db database:\n\nSQLite starts by creating a sandwiches.db-journal file next to our sandwiches.db database file and writing a journal header:\n\nThe first 12 bytes are filled with zeros but they‚Äôll be overwritten at the end of our transaction so let‚Äôs skip them for now.\n\nThe value 0xf65ddb21 is called a nonce and it‚Äôs a randomly generated number that we‚Äôll use to compute checksums for our entries in the journal. SQLite has some journal modes where it‚Äôll overwrite the journal instead of delete it so the checksums help SQLite know when its working with contiguous set of entries and not reading old entries left behind from previous transactions.\n\nNext, we have 0x00000000 which is the size of the database before the transaction started. Since this is the first transaction, our database was empty before the transaction.\n\nThen we specify the sector size of 0x00000200 (or 512). A disk sector is the smallest unit we typically work with for disk drives and SQLite keeps the journal header on its own sector. It does this because journal header is later rewritten and we don‚Äôt want to accidentally corrupt one of our pages if a sector write fails.\n\nFinally, we have 0x00001000 (or 4,096) which is the page size for our database, in bytes.\n\nSQLite can now freely write changes to the database file while knowing that it has written down the state of the database from before the transaction started.\n\nWhen you go to commit the changes, SQLite will rewrite the first 12 bytes of the journal header with two new fields: the magic number \u0026 the number of page entries in the journal.\n\nThe ‚Äúmagic number‚Äù is a ridiculous name for a constant value that is written to the beginning of a file to indicate its file type. For journal files, this magic number is d9d505f9 20a163d7. We don‚Äôt have any page entries since our database was empty so the page count stays as zeros.\n\nNext, we‚Äôll sync the journal to disk to make sure we don‚Äôt lose it.\n\nThe final step that ends the commit is when SQLite deletes the file. If any of the previous steps fail then SQLite can use the rollback journal to revert the state of the database. Just like with your ham-and-cheese, the transaction doesn‚Äôt happen until you have a sandwich in your hand.\n\n\n## Copying out to the journal\n\nNow let‚Äôs see how the journal works with an existing database. Our first transaction left us with a 2-page database. The first page holds our database header and some metadata about our schema. The second page is an empty leaf page for our sandwiches table.\n\nWe‚Äôll insert our sandwich into our table:\n\nThis will create a new journal with the following header:\n\nIt looks similar to before but we have a new randomly-generated nonce (0x0600399e) and our database size before the transaction is now 0x00000002 pages instead of zero.\n\nSince our transaction is updating the leaf page, SQLite needs to copy out the original version of the page to the journal as a page record. The journal page records are comprised of 3 fields.\n\nFirst, we have the page number to indicate that we‚Äôre updating page 2:\n\nThen it‚Äôs followed by a copy of the 4,096 bytes that were in the page before the transaction started. Finally, it computes a 32-bit checksum on the data in the page:\n\nInterestingly, the checksum is only calculated on a very sparse number of bytes in the page and is primarily meant to guard against incomplete writes. Since SQLite 3.0.0 dates back to 2004 and it works on minimal hardware, reducing any overhead can be critical. You can see the evolution of computing power as the WAL mode, which was introduced in 2010, checksums the entire page.\n\nWith our original copy of the page in the journal, we can update our copy in the main database file without having to re-copy the page. We can add a second sandwich to our transaction and SQLite will only update the main database file:\n\nThe database and journal end up looking like this:\n\n\n## When sandwiches go bad\n\nBack to our sandwich shop example, let‚Äôs say there is a catastrophic sandwich event that occurs in the middle of your order. Perhaps your sandwich artist couldn‚Äôt stand to make one more ham-and-cheese sandwich and abruptly quit.\n\nSo our shop owner subs out a new employee to replace the old one so the sandwich production can continue. But how do we deal with the in-process sandwich? The new employee could try to finish the sandwich but maybe the customer gave specific instructions to the old employee. When you‚Äôre dealing with something as critical as lunch, it‚Äôs best to start over and do it right.\n\nWhen SQLite encounters a failure scenario, such as an application dying or a server losing power, it needs to go through a process called ‚Äúrecovery‚Äù. For a rollback journal, this is simple. We can walk through our journal page records and copy each page back into the main database file. At the end, we truncate our main database file to the size specified in the journal header.\n\nRollback journals are even resistant to failures during their own recovery. If your server crashes midway through a recovery process, SQLite will simply start the recovery process from the beginning of the journal file.\n\nThe procedure is idempotent and is not considered complete until the pages copied back are synced to disk and the journal file is deleted. For example, let‚Äôs say we‚Äôd only copied half of page 2 in our diagram from the journal back to the database file and then our server crashed. When we restart, we still have our journal file in place and we can simply try copying that page again.\n\n\n## Keeping track of our ingredients\n\nOur sandwich shop owner begins to suspect that employees are skimming pickles off the line and decides to hire folks to inventory ingredients periodically. However, the owner quickly realizes that the inventory numbers are off because the inventory specialists are trying to count ingredients while sandwich makers are taking those same ingredients to put into sandwiches.\n\nTo fix this, the owner decides that the store must be locked while a sandwich is being made. However, when a sandwich isn‚Äôt being made, any number of inventory specialists can come in and count ingredients.\n\nThis is how it works in SQLite when using the rollback journal. Any number of read-only transactions can occur at the same time. However, when we start a write transaction then we need to wait for the readers to finish and block all new readers until the write is done.\n\nThis makes sense now that you know that we‚Äôre changing the main database file during a write transaction. We‚Äôd have no way to give read transactions a snapshot view of the database if we‚Äôre updating the same underlying data.\n\n\n## Read/write locks on the file system\n\nSince SQLite allows multiple processes to access it, it needs to perform locking at the file system level. There are 3 lock bytes that are used to implement the read/write lock at the file system level:\n\nWhen a read transaction starts, it checks the PENDING lock first to ensure a writer is not inside a write transaction or that a writer is not waiting to start a transaction. If the reader can obtain the PENDING lock then it obtains a shared lock on the SHARED lock byte and holds it until the end of the transaction.\n\nFor write transactions, it first obtains an exclusive lock on the PENDING lock byte to prevent new read transactions from starting. It then tries to obtain an exclusive lock on the SHARED lock byte to wait for in-process read transactions to finish. Finally, it obtains an exclusive lock on the RESERVED lock byte to indicate that a write transaction is in-process.\n\nThis series of steps ensure that only one write transaction is in effect at any time and that new readers won‚Äôt block it.\n\nLocks are located on a page at the 1GB position in the database file and this page is unusable by SQLite as some Windows versions use mandatory locks instead of advisory locks. If a database is smaller than 1GB, this page is never allocated and only exists within the operating system‚Äôs lock accounting system.\n\nWithin the lock page, a byte is used for the PENDING lock and another byte for the RESERVED lock. After that, 510 bytes are used for the SHARED lock. A byte range is used here to accommodate older Windows versions with mandatory locks. In those cases, a randomly chosen byte is locked by a client within that range. On Unix, the entire range is locked using fctnl() and F_RDLCK.\n\n\n## How to improve on journaling\n\nThe rollback journal is a simple trick to simulate atomicity and isolation and to provide durability to a database. Simple tricks are the best kind of tricks when you write a database so it‚Äôs a great place to start.\n\nBut it certainly has its trade-offs. Kicking out all other transactions whenever you need to write something can become a bottleneck for many applications that have concurrent users. When people say that SQLite doesn‚Äôt scale, it‚Äôs typically because they used the rollback journal.\n\nHowever, SQLite continued to improve and eventually introduced the write-ahead log (WAL) journaling mode and even the wal2 journaling mode. These provide significantly better support for concurrent readers.\n\nThis means that our inventory specialists in our example could each have a point-in-time view of all the ingredients‚Äîeven while the sandwich maker continues to make sandwiches! We‚Äôll get into how this works in our next post on WAL mode."
  },
  {
    "title": "Start simple",
    "url": "https://fly.io/blog/volumes-expand-restore/",
    "content": "Fly.io operates hardware around the world to run your apps close to your users. This post is about our storage volumes, which are handy for running full stack apps. Deploy, say, a Laravel app now‚Äîit takes mere minutes.\n\nFly Volumes are the persistent storage that makes it possible to run full stack apps entirely on the Fly.io platform, keeping your configuration, session or user data in place across deployments. Looking at them from another angle, volumes are space on NVMe drives on our servers that you can rent for your apps to use.\n\nWe‚Äôve recently made two major improvements to volumes: extending volume size, and self-service snapshot restores.\n\n\n## Extending a volume\n\nUntil recently, if you needed your volume to be bigger, you‚Äôd have to provision a new empty one and copy your data to it. This is not ideal, to put it mildly.\n\nBut you can now extend a volume on the CLI with fly volume extend \u003cvolume-id\u003e.\n\nThe app instance attached to the volume does have to restart to allow the file system to be resized. This will happen automatically for ‚Äúregular‚Äù apps, but Machines VMs will have to be restarted manually.\n\nMore details in the announcement on the Fly.io community forum and in the volumes docs.\n\nIdeally, we‚Äôd be able to alert you when you hit a usage threshold on a storage volume, or even better, give you the option to increase your volume size automatically when you hit a threshold. We‚Äôre not there yet!\n\n\n## Restoring data from a volume snapshot\n\nIt‚Äôs been possible for some time to restore a Fly.io HA Postgres database from a snapshot, but if you were using another database, you had to ask us to dig up and restore a snapshot for you.\n\nBut now you can restore regular volumes: as of flyctl v0.0.363, individual volume restores can now be performed by specifying¬†--snapshot-id at creation time.\n\nWhich means you can get back sandwiches.db by yourself (is there something other than sandwiches that you might store in an app‚Äôs database?), at your own convenience.\n\nVolume snapshots are taken daily (but not at the same time every day), and shunted off to object storage where they live for five days before they expire.\n\nIf you need to restore from a snapshot, you identify the volume you want, list its snapshots, get the ID of the one you want from the list, and create a new volume by pointing fly volume create at that ID.\n\nYou can restore to a bigger volume, if you like, but not a smaller one than the snapshot came from.\n\nForum announcement. Docs.\n\nIt‚Äôs just a couple commands to get an app deployed on Fly.io."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/sqlite-internals-btree/",
    "content": "Fly.io runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. Give us a whirl and get up and running quickly.\n\nOk, I‚Äôll admit it‚ÄîI‚Äôm a SQLite shill. There are few holes that I don‚Äôt try to put a SQLite-shaped peg into. It‚Äôs not that I dislike other databases, they‚Äôre great. But SQLite is so easy to use and, more importantly, it‚Äôs simple. Simplicity leads to reliability and I don‚Äôt know of a more reliable database than SQLite.\n\nThere‚Äôs a comfort with being able to read through a spec or a code repository and know that you‚Äôve covered the full breadth of a tool. That‚Äôs why I love Go‚Äôs simple 100-page language specification and that‚Äôs why I love SQLite‚Äôs 130KLOC core code base. It‚Äôs something that you can read through in a long weekend if, ya know, that‚Äôs what you do on weekends.\n\nThis constrained size means that SQLite doesn‚Äôt include every bell and whistle. It‚Äôs careful to include the 95% of what you need in a database‚Äîstrong SQL support, transactions, windowing functions, CTEs, etc‚Äîwithout cluttering the source with more esoteric features. This limited feature set also means the structure of the database can stay simple and makes it easy for anyone to understand.\n\nWe here at Fly.io have an unreasonable affinity for explanations involving sandwiches and this post will continue in that sacred tradition.\n\n\n## I have binders full of sandwiches\n\nRecently, I tried to remember a sandwich I ate last week but to no avail. Like any 10x engineer, I quickly over-engineered a solution by making a SQLite database to track every sandwich consumed.\n\nWe‚Äôll start off with our table definition:\n\nNow we‚Äôll have a record of every sandwich, its size in inches, and the number eaten.\n\nI live in Denver where we were known in the early 2000s for toasted sandwiches, and then in the 2010s for bad toasted sandwiches so we‚Äôll kick off with a toasted Italian sub.\n\nVoila! Our data is safe on disk. It‚Äôs easy to gloss over all the steps it takes to get from hitting the ‚Äúenter‚Äù key to bytes getting saved to disk. SQLite virtually guarantees that your database will never be corrupted or that your transaction will be half-written. But instead of glossing over, let‚Äôs dive in deep and see how our Italian sub looks on disk.\n\n\n## Efficient sandwich encoding\n\nOur row of sandwich data exists as an array of bytes inside SQLite that‚Äôs encoded using its record format. For our inserted row, we see the following bytes on disk:\n\nLet‚Äôs break these bytes down to see what‚Äôs going on.\n\n\n## The header \u0026 variable-length integers\n\nThe first byte of 0x15 is the size of our row‚Äôs payload, in bytes. After this is our rowid which is used as our PRIMARY KEY. Since this is the first row, its id has a value of 0x01.\n\nThese first two fields use what‚Äôs called a variable-length integer (‚Äúvarint‚Äù) encoding. This encoding is used so that we don‚Äôt use a huge 8-byte field for every integer and waste a bunch of space. It‚Äôd be like if a sandwich shop packaged every sandwich in enormous 6-foot party sub containers because they only could use one size of container. That‚Äôd make no sense! Instead, each size of sandwich gets its own container size.\n\nVarints use a simple trick. The high bit is used as a flag to indicate if there are more bytes to be read and the other 7 bits are our actual data. So if we wanted to represent 1,000, we start with its binary representation split into 7 bit chunks:\n\nThen we add a ‚Äú1‚Äù flag bit to the beginning of the first chunk to indicate we have more chunks, and a ‚Äú0‚Äù flag bit to the beginning of the second chunk to indicate we don‚Äôt have any more chunks:\n\nWith varints, we can now store our integer in 2 bytes instead of 8. This may seem small but many SQLite databases have a lot of integers so it‚Äôs a huge win!\n\nThe next two bytes after the rowid specify the data that is not spilled to overflow pages but the explanation is lengthy so we‚Äôre gonna wave our hands over that part.\n\n\n## Type encoding\n\nNext, we have a list of column types for our name, size, and count fields. Each data type has a different encoding that‚Äôs specified as a varint.\n\nFor our name column, the 0x1b value specifies that it is a TEXT type and has a length of 7 bytes. Type values that are odd and are greater or equal to 13 are TEXT fields and can be calculated with the formula (n*2) + 13. So our 7-byte string is (7*2) + 13 which is 27, or 0x1b in hex.\n\nBLOB fields are similar except they‚Äôre even numbers calculated as (n*2) + 12. SQLite alternates these TEXT and BLOB type values so small lengths of both types can be encoded efficiently as varints.\n\nNext, we have our ‚Äúlength‚Äù field which is a floating-point number. These are always encoded as a 0x07.\n\nAfter that, we have our ‚Äúcount‚Äù field which is an integer. These get packed down similar to varints but in a slightly different format. Integers that can fit in an 8-bit integer are represented with a type value of 0x01. 16-bit integers are 0x02, 24-bit integers are 0x03 and so on.\n\n\n## Value encoding\n\nOnce our types are all encoded, we just need to pack our data in. The text value of ‚ÄúItalian‚Äù is represented as UTF-8:\n\nThen our length of 7.5 is represented as an IEEE-754-2008 floating-point number. SQLite can optimize integer floating-point values by storing them as pure integer fields but since we have a decimal place it is stored with 8-bytes:\n\nAnd finally we use a single byte to hold our count of 2:\n\nCongrats! You‚Äôre now an expert on SQLite record formatting.\n\n\n## E_TOOMANYSANDWICHES\n\nAs my sandwich addiction continues unabated, I fill my SQLite database with more and more rows. I even make friends on the /r/eatsandwiches subreddit and start collecting their sandwiches. My sandwich database seems to grow without bound.\n\nSurprisingly though, adding or updating rows is still nearly as instantaneous as when I had a single row. So how does SQLite update a multi-gigabyte sandwich database in a fraction of a second? The answer is pages \u0026 b-trees.\n\nA naive approach to a database would just be to pack records in sequentially in a file. However, there‚Äôs no way to insert or update rows in the middle of the file without shifting and rewriting all the bytes after the new row.\n\nInstead, SQLite groups rows together into 4KB chunks called ‚Äúpages‚Äù. Why 4KB? That‚Äôs what file systems typically use as their page size so keeping everything aligned reduces page fetches from disk. Disks are usually the slowest part of a database so limiting page fetches can have a huge performance win.\n\n\n## Inspecting the page format\n\nIf we inspect our page, we can see its header:\n\nThere are several parts of this header but I masked out several bytes so we can focus on two particularly important fields. The first byte, 0x0d, indicates the page type. This page type is a table leaf. We‚Äôll talk about those more with b-trees.\n\nThe second important field is the cell count, which is 0x0003. This tells us that 3 records exist in this page.\n\nAfter the header, we have the cell pointer index:\n\nThis is a list of 2-byte values that represent offsets in the page for each record. The first record exists at offset 4,073 (0x0fe9), the second record exists at offset 4,050 (0x0fd2), etc. SQLite packs rows at the end of the page and then works backwards.\n\nAfter the index, we have a bunch of zeros until we reach the content area of the page which holds our row data in record format.\n\n\n## Structuring pages into trees\n\nNow that we‚Äôve chunked our data into pages, we can update a subset of our data without having to rewrite the whole file. That‚Äôs great for writing but now we have a list of pages to search through if we want to query our data and that won‚Äôt scale as-is.\n\nThe simplest approach would be to start at the first page and then search every page for a given row. This is called a ‚Äútable scan‚Äù and it can be really slow‚Äîespecially if your data is at the end of your table. If you‚Äôre into ‚Äúbig-O‚Äù notation, it‚Äôs also referred to as ‚Äúlinear time complexity‚Äù, or O(n). That means the amount of time it takes to search for a record has a linear relationship to the number of records you have, which is referred to as ‚Äún‚Äù.\n\n\n## Searching faster with binary search\n\nIf you are searching by your primary key, you could perform a binary search since the table is sorted by primary key. For a binary search, you search the page right in the middle of your table for your sandwich record. If the record exists, great! You‚Äôre done.\n\nIf the sandwich you‚Äôre searching for is before that page, then you find a new ‚Äúmiddle‚Äù page in the first half of your table. If it‚Äôs after the page, then you find a new middle page in the second half of your table. Keep slicing the search space in half and searching until you find your sandwich. If you squint a bit, this slicing and subdividing has the feel of a tree-like structure.\n\nA binary search has a logarithmic time complexity, or O(log n). That‚Äôs considered pretty good for data structures since it means you can scale up to a large number of records, n, while the cost grows at a much slower rate.\n\n\n## Improving binary search by persisting the tree\n\nWhile logarithmic time complexity is great, we still have a problem. Let‚Äôs run some numbers.\n\nIf we have a small 2MB database with 4KB pages then that means we have 512 pages. A binary search of that many pages would have a worst-case scenario of log‚ÇÇ(512), or 9 pages. That means we might have to read nine pages in our tiny database just to find one record! Page fetches are painfully slow in databases so we want to reduce that as much as possible.\n\nSQLite is structured as a b-tree, which is a data structure where each node can point to two or more child nodes and the values in these child nodes are all sorted. There are a ton of different variants of b-trees but the one that SQLite uses is called a b+tree. This type of tree stores all data in the leaf nodes, which is what our sorted list of pages represents, but also have an index of key ranges in the branch pages. SQLite refers to these branch pages as ‚Äúinterior pages‚Äù.\n\nTo illustrate this, let‚Äôs say our leaf pages hold sandwich records that are each 40 bytes. The record also contains an integer primary key that is 4 bytes on average. That means we can fit about 100 records into one 4KB page. If we have less than 100 records, we only need one page. Easy peasy.\n\nBut what happens when we add a 101st sandwich and it doesn‚Äôt fit anymore? SQLite will split the page into two leaf pages and add an interior page as the root of our b+tree that points to our child leaf pages. This interior page stores the key ranges for the leaf pages so that when you search, you can see what ranges each child page holds without having to actually read that child page.\n\nThis doesn‚Äôt seem like a big improvement over our binary search until we start adding more data. Interior pages are also 4KB and they store pairs of child primary keys and their page numbers so each entry in our interior page takes about 8 bytes. That means we can store about 500 entries in an interior page. For our 2MB database, that means we can hold the key range for all of our leaf pages in a single interior page. To find a given record, we only need to search the root interior page to find the correct leaf page. That means our worst case search is only 2 pages.\n\n\n## Growing a tree\n\nWhat happens when our root interior page fills up and we need a database bigger than 2MB? Similar to leaf pages, we split the interior page in two and add a new root parent interior page that points to the split interior pages. This means that we need to search the new root interior page to find the correct second-level interior page and then we search page that to find our leaf page. We now have a tree depth of 3.\n\nSince our new root can hold about 500 references to second-level interior pages and those second-level pages hold about 500 references to leaf pages, we can store about 250,000 pages, or about 1GB of data. If we continue adding rows, we‚Äôll need to split the root page again and increase our tree depth to 4.\n\nAt a tree depth of 4, we can hold about 500¬≥ leaf pages, or about a 476GB database. That means we only need to read 4 pages to find a given record‚Äîeven in a huge database!\n\n\n## OK, But Why?\n\nWhile it‚Äôs interesting to noodle around with internals, how does this actually apply to real-world scenarios?\n\nWell, knowing about record formatting tells us that storing integers instead of floating-point numbers is wildly more efficient as SQLite doesn‚Äôt compress floats.\n\nOr perhaps knowing that b-tree time cost grows logarithmically will let you feel more comfortable designing an application with a multi-gigabyte table.\n\nOr maybe, just maybe, discovering the /r/eatsandwiches subreddit will inspire your dinner tonight.\n\nLearning about the internals of our tools lets us feel comfortable with them and use them confidently. Hopefully low-level features like SQLite‚Äôs PRAGMAs seem a little less opaque now.\n\nI‚Äôll be writing more on SQLite internals in future posts‚Äîfrom rollback journals to write-ahead logs to the SQLite virtual machine. Want to know more about a specific topic? Hit me up on the Fly Community forum or ping us on Twitter."
  },
  {
    "title": "Launch a Laravel app, super quick",
    "url": "https://fly.io/blog/launching-laravel-bytes/",
    "content": "Fly runs apps close to users by taking Docker images and transmogrifying them into Firecracker micro-vms, running on our hardware around the world. If you want to ship a Laravel app, try it out on Fly.io. It takes just a couple of minutes.\n\nToday we‚Äôre launching Laravel Bytes - our new home for anything Laravel.\n\nWe‚Äôre excited to support deploying Laravel across the globe - and we have lots to talk about!\n\nWe‚Äôve already made it easy to run Laravel on Fly, but with the possibilities unlocked by global deployment, there‚Äôs still plenty more to consider. In future posts, we‚Äôll talk about the considerations that come with a global user base.\n\nFinally, we‚Äôll have lots of tips, tricks, strategies, how-to‚Äôs, and more (especially about Livewire - we love real-time apps)!\n\nKeep a lookout for new stuff in Laravel Bytes!\n\n\n## Getting Started with Laravel\n\nLaravel is supported in the fly launch command, so the easiest way to get a feel for running Laravel is in our docs for Deploying a Laravel Application.\n\nThe quickest version of that boils down to this (assuming the fly command is installed):\n\nFor something a bit more in depth, we‚Äôve also written up a more complete guide on running a Laravel apps on Fly with Redis, MySQL, queues workers, and cron tasks.\n\nIt‚Äôll take just a few minutes to get Laravel running on Fly!"
  },
  {
    "title": "Phoenix screams on Fly.io.",
    "url": "https://fly.io/blog/liveview-its-alive/",
    "content": "Phoenix LiveView lets you build interactive, real-time applications without dealing with client-side complexity. This is a post about the guts of LiveView. If you just want to ship a Phoenix app, the easiest way to learn more is to try it out on Fly.io; you can be up and running in just a couple minutes.\n\nLiveView started with a simple itch. I wanted to write dynamic server-rendered applications without writing JavaScript. Think realtime validations on forms, or updating the quantity in a shopping cart. The server would do the work, with the client relegated to a dumb terminal. It did feel like a heavy approach, in terms of resources and tradeoffs. Existing tools couldn‚Äôt do it‚Äîbut I was sure that this kind of application was at least possible. So I plowed ahead.\n\nThree years later, what‚Äôs fallen out of following this programming model often feels like cheating. ‚ÄúThis can‚Äôt possibly be working!‚Äù. But it does. Everything is super fast. Payloads are tiny. Latency is best-in-class. You write less code. More than that: there‚Äôs simply less to think about when writing features.\n\nThis blows my mind! It would be fun to say I‚Äôd envisioned ahead of time that it would work out like this. Or maybe it would be boring and pompous. Anyway, that‚Äôs not how it happened.\n\nTo understand how we arrived where we are, we‚Äôll break down the ideas in the same way we evolved LiveView to what it is today: we‚Äôll start with a humble example, then we‚Äôll establish the primitives LiveView needed to solve it. Then we‚Äôll see how, almost by accident, we unlocked a particularly powerful paradigm for building dynamic applications. Let‚Äôs go.\n\n\n## A Humble Start\n\nWe‚Äôll start small. Say you want to build an e-commerce app where you can update the quantity of an item like a ticket reservation. Ignoring business logic, the actual mechanics of the problem involve updating the value on a web page when a user clicks a thing. A counter.\n\nThis shouldn‚Äôt be difficult. But we‚Äôre used to navigating labyrinths of decisions, configuring build tools, and assembling nuts and bolts, to introduce even the simplest business logic. We go to build to a counter, and first we must invent the universe. Does it have to be this way?\n\nConceptually, what we really want is something like what we do in client-side libraries like React: render some dynamic values within a template string, and the UI updates with those changed values. But instead of a bit of UI running on the client, what if we ran it on the server? The live view might look like this:\n\nWe can render a template and assign some initial state when the live view mounts.\n\nNext, our interface to handle UI interactions could look something like traditional reactive client frameworks. Your code has some state, a handler changes some state, and the template gets re-rendered. It might look like this:\n\nUI components are necessarily stateful creatures, so we know we‚Äôll need a stateful WebSocket connection with Phoenix Channels which can delegate to our mount and handle_event callbacks on the server.\n\nWe want to be able to update our UI when something is clicked, so we wire up a click listener on phx-click attributes. The phx-click binding will act like an RPC from client to server. On click we can send a WebSocket message to the server and update the UI with an el.innerHTML = newHTML that we get as a response. Likewise, if the server wants to send an update to us, we can simply listen for it and replace the inner HTML in the same fashion. The first pass would look like this on the client:\n\nThis is where LiveView started‚Äîgo to the server for interactions, re-render the entire template on state change, send the entire thing down to the client, and replace the UI.\n\nIt works, but it‚Äôs not great.\n\nThere‚Äôs a lot of redundant work done on the server for partial state changes, and the network is saturated with large payloads to only update a number somewhere on a page.\n\nStill, we have our basic programming model in place. It only takes annotating the DOM with phx-click and a few lines of server code to dynamically update the page. Excited, we ignore the shortcomings of our plumbing and press on to try out what we‚Äôre really excited about ‚Äì¬†realtime applications.\n\n\n## Things get realtime\n\nPhoenix PubSub is distributed out of the box. We can have our LiveView processes subscribe to events and react to platform changes regardless of what server broadcasted the event. In our case, we want to level up our humble reservation counter by having the count update as other users claim tickets.\n\nWe get to work and crack open our Reservations module:\n\nWe add a subscription interface to our reservation system, then we modify our business logic to broadcast a message when reservations occur. Next, we can listen for events in our LiveView:\n\nFirst, we subscribe to the reservation system when our LiveView mounts, then we receive the event in a regular Elixir handle_info callback. To update the UI, we simply update our state as usual.\n\nHere‚Äôs what‚Äôs neat ‚Äì now whenever someone clicks reserve, all users have their LiveView re-render and send the update down the wire. It cost us 10 lines of code.\n\nWe test it out side-by-side in two browser tabs. It works! We start doubting the scalability of our naive approach, but we marvel at what we didn‚Äôt write.\n\nPhoenix is a win anywhere. But Fly.io was practically born to run it. With super-clean built-in private networking for clustering and global edge deployment, LiveView apps feel like native apps anywhere in the world.\n\n\n## Happy accident #1: HTTP disappears\n\nTo make the reservation count update on all browsers, we only wrote a handful of lines of server code. We didn‚Äôt even touch the client. The existing LiveView primitives of a dumb bidirectional pipe pushing RPC and HTML were all we needed to support cluster-wide UI updates.\n\nThink about that.\n\nThere was no library or user-land JavaScript to add. Our reservation LiveView didn‚Äôt even consider how the data makes it to the client. Our client code didn‚Äôt have to become aware of out-of-band server updates because we already send everything over WebSockets.\n\nAnd a revelation hits us. HTTP completely fell away from our thoughts while we implemented our reservation system. There were no routes to define for updates, or controllers to create. No serializers to define for payload contracts. New features are now keystrokes away from realization.\n\nUnfortunately, this comes at the cost of server resources, network bandwidth, and latency. Broadcasting updates means an arbitrary number of processes are going to recompute an entire template to effectively push an integer value change to the client. Likewise, the entire templates string goes down the pipe to change a tiny part of the page.\n\nWe know there‚Äôs something special here, but we need to optimize the server and network.\n\n\n## Making it fast\n\nOptimizing the server to compute minimal diffs is some of the most complex bits of the LiveView codebase, but conceptually it‚Äôs quite simple. Our optimizations have two goals. First, we only want to execute those dynamic parts of a template that actually changed from the previous render. Second, we only want to send the minimal data necessary to update the client.\n\nWe can achieve this in a remarkably simple way. Rather than doing some advanced virtual DOM on the server, we simplify the problem. An Elixir HTML template is nothing more than a bunch of HTML tags and attributes, with Elixir expressions mixed in the places we want dynamic data.\n\nLooking at it from that direction, we can optimize simply by splitting the template into static and dynamic parts. Considering the following LiveView template:\n\nAt compile time, we can compile the template into a datastructure like this:\n\nWe split the template into static and dynamic parts. We know the static parts never change, so they are split between the dynamic elixir expressions. For each expression, we compile the variable access and execution with change tracking. Since we have a stateful system, we can check the previous template values with the new, and only execute the template expression if the value has changed.\n\nInstead of sending the entire thing down on every change, we can send the client all the static and dynamic parts on mount, then only send the partial diff of dynamic segments for each update.\n\nWe can do this by sending the following payload to the client on mount:\n\nThe client receives a simple map of static values in the s key, and dynamic values keyed by the index of their location in the template. For the client to produce a full template string, it simply zips the static list with the dynamic segments, for example:\n\nWith the client holding the initial payload of static and dynamic values, optimizing the network on update becomes easy. The server now knows which dynamic keys have changed, so when a state change occurs, it renders the template, which lazily executes only the changed dynamic segments. On return, we receive a map of new dynamic values keyed by their position in the template. We then pass this payload to the client.\n\nFor example, if the LiveView performed assign(socket, :class, \"inactive\"), the following diff would fall out of the render/1 call and be sent down the wire:\n\nThats it! And to turn this little payload back into an updated UI for the client, we only need to merge this object with our static dynamic cache:\n\nThen we zip the merged data together and now our new HTML can be applied like before via an innerHTML update.\n\nReplacing the DOM container‚Äôs innerHTML works, but wiping out the entire UI on every little change is slow and problematic. To optimize the client, we can pull in morphdom, a DOM diffing library that can take two DOM trees, and produce the minimal amount of operations to make the source tree look like the target tree. In our case, this is all we need to close the client/server optimization loop.\n\nWe build a few prototypes and realize we‚Äôve created something really special. Somehow our naive heavy templates are more nimble than those React and GraphQL apps we used to sling. But How?\n\n\n## Happy accident #2: best in class payloads, free of charge\n\nOne of the most mind blowing moments that fell out of the optimizations was seeing how naive template code somehow produced payloads smaller than the best hand rolled JSON apis and even sent less data than our old GraphQL apps.\n\nOne of the neat things about GraphQL is the client can ask the server for only the data it wants. Put simply, the client can ask for a user‚Äôs username and birthday, and it won‚Äôt be sent any other keys of the canonical user model. This is super powerful, but it must be specified on the server via typed schemas to work.\n\nHow then, does our LiveView produce nearly keyless payloads with no real typed information?\n\nThe answer is it was mostly by accident. Going back to our static/dynamic representation in the Phoenix.LiveView.Rendered struct, our only goal initially was thinking about how to represent a template in a way that allowed us to avoid sending all the markup and HTML bits that don‚Äôt change. We weren‚Äôt thinking about solving the problem of client/server payload contracts at all.\n\nThere‚Äôs a lesson here that I still haven‚Äôt fully unpacked. In the same way as a user of LiveView I stopped concerning myself with HTTP client/server contracts, as an implementer of LiveView, I also had moved on from thinking about payload contracts. Yet somehow this dumb bidirectional pipe that sends RPC‚Äôs and diffs of HTML now allows users to achieve best in class payloads without actually spec'ing those payloads. This still tickles my mind.\n\nOne of the other really interesting parts of the LiveView journey is how the programming model never changed beyond our initial naive first-pass. The code we actually wanted to write in the beginning never needed to change to enable all these powerful optimizations and code savings. We simply took that naive first-pass and kept chipping away to make it better. This lead to other happy accidents.\n\n\n## Happy accident #3: lazy loading and bundle-splitting without the bundles\n\nAs we built apps, we‚Äôd examine payloads and find areas where more data would be sent than we‚Äôd like. We‚Äôd optimize that. Rinse and repeat. For example, we realized the client often receives the same shared fragments of static data for different parts of the page. We optimized by sending static shared fragments and components only a single time.\n\nImagine our surprise on the other side of finishing these optimizations when we realized we solved a few problems that all client-side frameworks must face, without the intention of doing so.\n\nBuild tools and client side frameworks have code splitting features where developers can manage how their various JavaScript payloads are loaded by the client. This is useful because bundle size is ever increasing as more templates and features are added. For example, if you have templates and associated logic for a subset of users, your bundle will include all supporting code even for users who never need it. Code splitting is a solution for this, but it comes at the cost of complexity:\n\nWith our optimizations, lazy-loading of templates comes free for free, and the client never gets a live component template it already has seen from the server. No bundling required, or build tools to configure.\n\nHere‚Äôs how it works. Imagine we want to update our reservation system to render a list of available events that can be reserved. It might look something like this:\n\nWhich we could render with a reusable reserve_item component:\n\nWe modified our initial reserve template to render a reservation button from a list of events. When LiveView performs its diffing, it recognizes the use of the shared statics and the following diff is sent down wire:\n\nNote the \"p\" key inside the diff. It contains a map of shared static template values that are referenced later for each dynamic item. When LiveView sees a static (\"s\") reference an integer, it knows that it points to a shared template value. LiveView also expands on this concept when using a live component, where static template values are cached on the client, and the template is never resent because the server knows which templates the client has or hasn‚Äôt seen.\n\nEven with our humble reservation counter, there are other bundling concerns we skipped without noticing. Our template used localization function calls, such as \u003c%= gettext(\"Sold out!\") %\u003e. We localized our app without even thinking about the details. For a dynamic app, you‚Äôd usually have to serialize a bag of localization data, provide some way to fetch it, and code split languages into different bundles.\n\nAs your LiveView application grows, you don‚Äôt concern yourself with bundle size because there is no growing bundle size. The client gets only the data it needs, when it needs it. We arrived here without every thinking about the problem of client bundles or carefully splitting assets because LiveView applications hardly have client assets.\n\n\n## Less code, fewer moving parts\n\nThe best code is the code you don‚Äôt have to write. In an effort to make a counter on a webpage driven by the server, we accidentally arrived at a programming model that sidesteps HTTP. We now have friction-free dynamic feature development with push updates from the server. Our apps have lower latency than those client apps we used to write. Our payloads are more compact than those GraphQL schemas we carefully constructed.\n\nLiveView is, at its root, just a way to generate all the HTML you need on the server, so you can write apps without touching JS. More dynamic apps than you might expect, thanks to Elixir‚Äôs concurrency model. But even though I wrote the internals of it, I‚Äôm still constantly blown away by how well things came together, and finding surprising new ways to apply it to application problems.\n\nAll this from a hack that started with 10 lines of JavaScript pushing HTML chunks down the wire."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-2022-07-18/",
    "content": "This is a Logbook post. It tells you what we‚Äôve been up to here at Fly.io, to make running your code close to users better in all the ways. The proof of the pudding is in the eating, though, so you should dig in now; you can have an app up and running in mere minutes.\n\nWe have some real gems in this edition. Have you ever wished you could grow the storage volume on a Fly.io app? Now you can!!\n\nWhat about this one: Ever wished that the $99 Pro Plan would include $99 of usage credits? OK, that one may have been a little bit specific. Conversely, have you ever wished that $99+ of monthly resource usage would come with support by email? Now it can!\n\nI don‚Äôt want to write spoilers for everything, so read on.\n\nThe changelog‚Äôs broken up into sections this time around, largely to corral the ongoing torrent of flyctl improvements. Keep your flyctl up to date to take advantage. If you‚Äôre playing (or working) with our new fast-booting machines VMs and flyctl, be sure to scan these changes! As always, if you‚Äôre interested in digging deeper into flyctl changes, dive into the releases page on GitHub.\n\n\n## Grab bag!\n\n\n## flyctl\n\n\n## flyctl \u0026 machines\n\n\n## Dashboard UI"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/soc2-the-screenshots-will-continue-until-security-improves/",
    "content": "Fly.io runs apps close to users by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. We‚Äôll come right to the point: if you were waiting for us to be SOC2-compliant before giving us all your money, well, we‚Äôre SOC2 now, so take us for a spin and make your checks payable to Kurt.\n\nIf you‚Äôre off getting your app up and running on Fly.io and finding your checkbook, great! I won‚Äôt get in your way. The rest of you, though, I want to talk to you about what SOC2 is and how it works.\n\nSpoiler: the SOC2 Starting Seven post held up pretty well.\n\nSOC2 is the best-known infosec certification, and the only one routinely demanded by customers. I have complicated feelings about SOC2, which you will soon share. But also, a few years ago, I wrote a blog post about what startups need to do to gear up for SOC2. Having now project-managed Fly.io‚Äôs SOC2, I‚Äôd like to true that post up, since I‚Äôm officially a leading authority on the process.\n\nSOC2 is worth talking about. It‚Äôs arcane in its particulars. Startups that would benefit from SOC2 are held back by the belief that it‚Äôs difficult and expensive to obtain. Consumers, meanwhile, split down the middle between cynics who‚Äôre certain it‚Äôs worthless and true-believers who think it sets the standard for how security should work.\n\nEverybody would be better off if they stopped believing what they believe about SOC2, and started believing what I believe about SOC2.\n\n\n## I‚Äôm a Customer. What Should I Know?\n\nBottom-line: SOC2 is a weak positive indicator of security maturity, in the same ballpark of significance as a penetration test report (but less significant than multiple pentest reports).\n\nI‚Äôm underselling SOC2. It assures some things pentests don‚Äôt:\n\nDepending on the kind of company you‚Äôre looking at, a SOC2 certification might be more or less meaningful. Intensely technical company? High-risk engineering? Look for the pentest. Huge number of employees? Get the SOC2 report.\n\nSo: if you‚Äôre clicking on SOC2 blog posts because you‚Äôre wondering how seriously you should take SOC2, there‚Äôs your answer. Go in peace.\n\nThe rest of you, buckle up.\n\n\n## What‚Äôs SOC2?\n\nYou might care about transaction integrity if you‚Äôre a Stripe, or confidentiality if you do e-Discovery for lawsuits, or privacy if you‚Äôre Equifax.\n\nThere‚Äôs a structure to the things you claim in SOC2, the AICPA‚Äôs ‚ÄúTrust Services Criteria‚Äù and something called ‚Äúthe COSO framework‚Äù. These are broken down into categories: security, availability, transaction integrity, confidentiality, and privacy. ‚ÄúSecurity‚Äù is mandatory, and is the only one that matters for most companies.\n\n‚ÄùDRL‚Äù is ‚ÄúDocument Request List‚Äù\n\nThe ground truth of SOC2 is something called the DRL, which is a giant spreadsheet that your auditor customizes to your company after a scoping call. You can try to reason back every line item on the DRL to first accounting principles, but that‚Äôd be approximately as productive as trying to reason about contract law from first principles after paying a lawyer to review an agreement.¬†Just take their word for it.\n\nWith me so far? SOC2. It‚Äôs a big spreadsheet an accounting firm gives you to fill out.\n\n\n## When Should You SOC2?\n\nWe waited as long as we felt we could.\n\nCareful, now. ‚ÄúGetting SOC2-certified‚Äù isn‚Äôt the same as ‚Äúdoing the engineering work to get SOC2-certified‚Äù. Do the engineering now. As early as you can. The work, and particularly its up-front costs, scale with the size of your team.\n\nThe audit itself though doesn‚Äôt matter, except to answer the customer request ‚Äúcan I have your SOC2 report?‚Äù\n\nSo, ‚Äúwhen should I SOC2?‚Äù is easy to answer. Do it when it‚Äôs more economical to suck it up and get the certification than it is to individually hand-hold customer prospects through your security process.\n\nThere‚Äôs a reason customers ask for SOC2 reports: it‚Äôs viral, like the GPL. The DRL strongly suggests you have a policy to ‚Äúcollect SOC2 reports from all your vendors‚Äù. Some SOC2 product vendors offer automated security questionnaires for companies to fill out, and they ask for SOC2 reports as well. Your customers ask because they themselves are SOC2, and the AICPA wants them to force you to join the coven.\n\nThat doesn‚Äôt mean you have to actually do it. If you can speak confidently about your security practice, you can probably get through anybody‚Äôs VendorSec process without a SOC2 report. Or you can pay an audit firm to make that problem go away.\n\nIt makes very little sense to get SOC2-certified before your customers demand it. You can get a long way without certification. If it helps, remember that you can probably make a big purchase order from that Fortune 500 customer contingent on getting your Type I in the next year.\n\n\n## What SOC2 Made Us Do\n\nWe started preparing for SOC2 more than a year before engaging an auditor, following the playbook from the ‚ÄúStarting 7‚Äù blog post. That worked, and I‚Äôm glad we did it that way. But to keep this simpler to read, I‚Äôm just going to write out all the steps we took as if they happened all at once.\n\nSingle Sign-On: Every newly-minted CSO I‚Äôve ever asked has told me that SSO was one of the first 3 things they got worked out when they took the position. Put compliance aside, and it‚Äôs just obvious why you want a single place to control credentials (forcing phishing-proof MFA, for instance), access to applications, and offboarding. We moved everything we could to our Google SSO.\n\nInside our network, we also use a certificate-based SSH access control system (I‚Äôll stop being coy, we use Teleport). To reach Teleport, you need to be on our VPN; to get to our VPN, you need Google SSO. Teleport, however, is authenticated separately, via Github‚Äôs SSO. So, for SSH, we have two authentication sources of truth, both of which need to work to grant access.\n\nIn addition to SSO, Teleport has the absolutely killer feature of transcript-level audit logs for every SSH session; with the right privilege level, you can watch other team members sessions, and you can go back in time and see what everyone did. This has the knock-on benefit of providing a transcript-level log of any REPL anyone has to drop into in prod.\n\nThere is, infamously, an ‚ÄúSSO tax‚Äù that companies pay to get access to the kinds of SAAS accounts that support SAML or OIDC. I have opinions about the SSO tax. It‚Äôs definitely a pain in our asses. If it‚Äôs early days for your company, for SAAS vendors that don‚Äôt deal in sensitive information, you can skip the SSO and just restrict who you give access to for the app. But mostly, you should just suck it up and pay for the account that does SSO.\n\nProtected Branches: I was surprised by how important this was to our auditors. If they had one clearly articulable concern about what might go wrong with our dev process, it was that some developer on our team might ‚Äúgo rogue‚Äù and install malware in our hypervisor build.\n\nIt‚Äôs easy to enable protected branches in Github. But all that does is make it hard for people to push commits directly to main, which people shouldn‚Äôt be doing anyways. To get the merit badge, we also had to document an approval process that ensured changes that hit production were reviewed by another developer.\n\nThis isn‚Äôt something we were doing prior to SOC2. We have components that are effectively teams-of-one; getting reviews prior to merging changes for those components would be a drag. But our auditors cared a lot about unsupervised PRs hitting production.\n\nWe asked peers who had done their own SOC2 and stole their answer: post-facto reviews. We do regular reviews on large components, like the Rust fly-proxy that powers our Anycast network and the Go flyd that drives Fly machines. But smaller projects like our private DNS server, and out-of-process changes like urgent bug fixes, can get merged unreviewed (by a subset of authorized developers). We run a Github bot that flags these PRs automatically and hold a weekly meeting where we review the PRs.\n\nCentralized Logging: A big chunk of the SOC2 DRL is about monitoring systems for problems. Your auditors will gently nudge you towards centralizing this monitoring, but you‚Äôll want to do that anyways, because every logging system you have is one you‚Äôll have to document, screenshot, and write policy about.\n\nWe got off easy here, because logging is a feature of our platform; we run very large ElasticSearch and VictoriaMetrics clusters, fed from Vector and Telegraf, and we‚Äôre generally a single ElasticSearch query away from any event happening anywhere in our infrastructure.\n\nOne thing SOC2 did force us to do was pick a ticketing system, which is something we‚Äôd done our best to avoid for several years. We send alerts to Slack channels and PagerDuty, and then have documented processes for ticketing them.\n\nAnother thing that surprised me was how much SOC2 mileage we got out of HelpScout. HelpScout is where our support mails (and security@ mails) go to, and while I‚Äôm not a HelpScout superfan, it is a perfectly cromulent system of record for a bunch of different kinds of events SOC2 cares about, like internal and external reports of security concerns.\n\nCloudTrail: We barely use anything in AWS other than storage. We compete with AWS! We run our own hardware! But SOC2 audit firms have spent the last 10 years certifying AWS-backed SAAS companies, and have added a whole bunch of AWS-specific line-items to their DRLs. We‚Äôre now much better at indexing and alerting on CloudTrail than we were before we did SOC2. It‚Äôs too bad that‚Äôs not more useful to our security practice.\n\nInfrastructure-as-Code: Your auditor will probably know what Terraform and CloudFormation are, and they will want you to be using it. Your job will be documenting how your own deployment system (the bring-up for new machines in your environment) is similar to Terraform. Sure, whatever.\n\nAn annoyance I did not see coming from previous experience was host inventory. Inventory is trivial if you‚Äôre an AWS company, because AWS gives you an API and a CLI tool to dump it. We run our own hardware, and while we have several different systems of record for our fleet of machines, they‚Äôre idiosyncratic and don‚Äôt document well; we ended up taking screenshots of SQL queries, which wasn‚Äôt as smooth as just taking a screenshot of our Tailscale ACLs or Google SSO settings.\n\nMDM and Endpoint Management: Here‚Äôs a surprise: in the year of our lord 2022, doing endpoint security in your SOC2 has fallen out of fashion. We were all geared up to document our endpoint strategy, but it turned out we didn‚Äôt have to.\n\nI should have some snarky bit of ‚Äúinsight‚Äù to share about this, but I don‚Äôt, and mostly all I can tell you is that you can probably cross this off your checklist of big projects you‚Äôll need to get done simply to get a SOC2 certification. You should do the work anyways, on its own merits.\n\nSOC2‚Äôs company control standards are firmly rooted in the accounting scandals of 2001, and it shows.\n\nBoring Company Stuff: I‚Äôd been mercifully insulated from this aspect of SOC2 in my former role as engineering support for SOC2‚Äôs, but had no such luck this time. I knew there was a lot of annoying company documentation involved in doing SOC2. I won‚Äôt put you to sleep with many of the details; if you‚Äôre the kind of company that should get a SOC2, the company and management stuff isn‚Äôt going to be an obstacle.\n\nWe had three ‚Äúboring company stuff‚Äù surprises that stick out:\n\nFirst, We needed a formal org chart posted where employees could find it. We‚Äôre not a ‚Äútitles and management‚Äù kind of company (we‚Äôre tiny), so this was a bother. But, whatever, now we have an org chart. Exciting!\n\nNext, our auditors wanted to see evidence of annual performance reviews. We don‚Äôt do annual performance reviews (we‚Äôre a continuous feedback, routine scheduled 1-on-1 culture). But if you‚Äôre not doing annual performance reviews, the AICPA can‚Äôt give assurances that an employee who exfiltrated our production database to Pastebin would be terminated. So now we have pro-forma annual reviews.\n\nThis kind of SOC2 thing falls under the category of ‚Äúthings you need to carefully explain to your team so they don‚Äôt think you‚Äôve suddenly decided to start stack ranking everyone‚Äù.\n\nFinally, background checks.\n\nBackground checks are performative and intrusive. Ask around for horror stories about how they flag candidates for not being able to source the right high school transcripts. Also, for us, they‚Äôre occasionally illegal: we have employees around the world, including several in European jurisdictions that won‚Äôt allow us to background check.\n\nThis is the only issue we ended up having to seriously back-and-forth with our auditors about. We held the line on refusing to do background checks, and ultimately got out of it after tracking down another company our auditors had worked with, finding out what they did instead of background checks, stealing their process, and telling our auditors ‚Äúdo what you did for them‚Äù. This worked fine.\n\nPolicies: You‚Äôre going to write a bunch of policies. It‚Äôs up to you how seriously you‚Äôre going to take this. I can tell you from firsthand experience that most companies phone this in and just Google [${POLICY} template], and adopt something from the first search engine result page.\n\n2019 Thomas would have done the same thing. But actually SOC2'ing a company I have a stake put me in a sort of CorpSec state of mind. You read a template incident response or data classification policy. You start thinking about why those policies exist. Then you think about would could go wrong if there was a major misunderstanding about those policies. Long story short, we wrote our own.\n\nThis part of the process was drastically simplified by the work Ryan McGeehan has published, for free, on his ‚ÄúStarting Up Security‚Äù site. We have, for instance, an Information Security Policy. It‚Äôs all in our own words (and ours quotes grugq). But it‚Äôs based almost heading-for-heading on Ryan‚Äôs startup policy, which is brilliant.\n\nOne thing about writing policies inspired by Ryan‚Äôs examples is that it liberates you to write things that make sense and read clearly and don‚Äôt contain the words ‚Äúwhereas‚Äù or ‚Äúdesignee‚Äù. Ryan hasn‚Äôt published a Data Classification policy. But our Data Classification policy was easy to write, in a single draft, just by using Ryan‚Äôs voice.\n\nIf you‚Äôre concerned about what you‚Äôre up against here, we ended up writing: (1) an Information Security Policy, which everyone on our team has to sign, (2) a Data Classification Policy that spells out how to decide which things can go in Slack or Email and which things you have to talk to management before transmitting at all, (3) a Document Retention policy (OK, this one I just sourced directly from our lawyers), (4) a Change Management policy, (5) a Risk Assessment policy, which says that sometime this year we will build a Risk Assessment spreadsheet explaining how we‚Äôll handle a zombie apocalypse (you think I‚Äôm joking), (6) a Vulnerability Management policy that roughly explains how to run nmap, (7) an Access Request policy that tells people which Slack channel to join to ask for access to stuff, (8) a Vendor Management policy that propagates the SOC2 virus to all our vendors, (9) an Incident Response policy, which we cribbed from Ryan, (10) a Business Continuity plan that says that Jerome is in charge if Kurt is ever arrested for robbing a bank, and (11) an employee handbook.\n\nI want to say more about our Access Management policy, which I shoplifted, at least in spirit, from a former client of mine that now works with us at Fly.io, but this is getting long, so you should just bug me about it online.\n\nWe use Slab as our company wiki / intranet. It‚Äôs great. Slab surprised me midway through the audit with a feature for ‚Äúverifying‚Äù pages, which might be the highest ROI feature/effort ratio I‚Äôve come across: I click a button and Slab adds a green banner to a page saying that it‚Äôs ‚Äúverified‚Äù for the next several months. That‚Äôs it, that‚Äôs the feature. Several DRL line items are about ‚Äúrecertifying‚Äù policies annually, and this gave us the screenshots we needed for that. Well done, Slab! Betcha didn‚Äôt even realize you implemented policy recertification.\n\nWhat We Didn‚Äôt Let SOC2 Make Us Do\n\nOrdered from most to least surprising (that is, there was no way we were going to do the stuff at the bottom of the list).\n\nThe Audit Itself\n\nIf you talk to people who‚Äôve done SOC2 before, you‚Äôll hear a lot of joking about screenshots. They‚Äôre not joking.\n\nThe whole SOC2 audit is essentially a series of screenshots. This is jarring to people who have had ‚Äúsecurity audits‚Äù done by consulting firms, in which teams of technical experts jump into your codebase and try to outguess your developers and uncover flaws in their reasoning. Nothing like that happens in a SOC2 audit, because a SOC2 audit is an actual audit.\n\nInstead, DRL line items are going to ask for evidence supporting claims you make, like ‚Äúour production repositories on Github have protected branches enabled so that random people can‚Äôt commit directly to main‚Äù . That evidence will almost always take the form of one or more screenshots of some management interface with some feature enabled.\n\nAnd that‚Äôs basically it? We divided the evidence collection stage of the audit up into a series of calls over the course of a week, each of which ate maybe twenty minutes of our time, most of which was us sharing a screen and saying ‚Äúthat checkbox over there, you should screenshot that‚Äù. I was keyed up for this before the calls started, prepared to be on my A-game for navigating tricky calls, and, nope, it was about as chill a process as you could imagine.\n\nSo, We‚Äôre Secure Now, And You Could Be Too\n\nThis was a lot of words, but SOC2 gives a lot of people a lot of feels, and I‚Äôd wished someone had written something like this down before I started doing SOC2 stuff.\n\nThe most important thing I can say about actually getting certified is to keep your goals modest. I‚Äôve confirmed this over and over again with friends at other companies: the claims you make in your Type I will bind on your Type II; claims you don‚Äôt make in your Type I won‚Äôt. It stands to reason then that one of your Type I goals should be helping your future self clear a Type II.\n\nI was a little concerned going into this that the quality of our SOC2 report (and our claims) would be an important factor for customers. And, maybe it will be. We got good auditors! I like them a lot! It wasn‚Äôt a paint-by-numbers exercise! But in talking to a couple dozen security people at other companies, my takeaway is that for the most part, having the SOC2 report is what matters, not so much what the SOC2 report says.\n\nI can‚Äôt not mention this, even though our auditors might see it and preemptively refuse to do this for us.\n\nAt least one peer, at a highly clueful, highly security-sensitive firm, described to us a vendor that had given them not one, not two, but five consecutive Type I reports. It is possible to synthesize excited bromide in an argon matrix! You can skip all the real work in SOC2!\n\nI‚Äôve spent the last several weeks trying to convince Kurt that we‚Äôre not going to do this. You‚Äôll know how I fared sometime next year.\n\nWe‚Äôll have more to say about pentesting soon.\n\nWe do a lot of security work that SOC2 doesn‚Äôt care about; in fact, SOC2 misses most of our security model. We build software in memory-safe languages, work to minimize trust between components, try to find simple access control models that are easy to reason about, and then get our code pentested by professionals.\n\nSOC2 doesn‚Äôt do a good job of communicating any of that stuff. And that‚Äôs fine; it doesn‚Äôt have to. We can write our own security report to explain what we do and how our security practice is structured, which is something I think more companies should do; I‚Äôd rather read one of those than a SOC2 report.\n\nAnd for all my cynicism, SOC2 dragged us into some process improvements that I‚Äôm glad we‚Äôve got nailed down. It helped to have clueful auditors, and a clear eye about what we were trying to accomplish with the process, if you get my drift.\n\nI expected ‚Äúentire new cloud provider‚Äù to be a complicated case for SOC2 audits. But the whole thing went pretty smoothly (and the un-smooth parts would have hit us no matter what we were building). If it was easy for us, it‚Äôll probably be easier for you. Just don‚Äôt do it until you have to."
  },
  {
    "title": "Roll your own FaaS",
    "url": "https://fly.io/blog/the-serverless-server/",
    "content": "I‚Äôm Will Jordan, and I work on SRE at Fly.io. We transmogrify Docker containers into lightweight micro-VMs and run them on our own hardware in racks around the world, so your apps can run close to your users. Check it out‚Äîyour app can be up and running in minutes. This is a post about how services like ours are structured, and, in particular, what the term ‚Äúserverless‚Äù has come to mean to me.\n\nFly.io isn‚Äôt a ‚ÄúGartner Magic Quadrant‚Äù kind of company. We use terms like ‚ÄúFaaS‚Äù and ‚ÄúPaaS‚Äù and ‚Äúserverless‚Äù, but mostly to dunk on them. It‚Äôs just not how we think about things. But the rest of the world absolutely does think this way, and I want to feel at home in that world.\n\nI think I understand what ‚Äúserverless‚Äù means, so much so that I‚Äôm going to stop putting quotes around the term. Serverless is a magic spell. Set a cauldron to boil. Throw in some servers, a bit of code, some eye of newt, and a credit card. Now behold, a bright line appears through your application, dividing servers from services‚Ä¶and, look again, now the servers have disappeared. Wonderful! Servers are annoying, and services are just code, the same kind of code that runs when we push a button in VS Code. Who can deny it: ‚ÄúNo server is easier to manage than no server.‚Äù\n\nBut, see, I work on servers. I‚Äôm a fan of magic, but I always have a guess at what‚Äôs going on behind the curtain. Skulking beneath our serverless services are servers. The work they‚Äôre doing isn‚Äôt smoke and mirrors.\n\nLet‚Äôs peek behind the curtain. I‚Äôd like to perform the exercise of designing the most popular serverless platform on the Internet. We‚Äôll see how close I can get. Then I want to talk about what the implications of that design are.\n\nClose your eyes, tap your keyboard three times and think to yourself, ‚ÄúThere‚Äôs no place like us-east-1‚Äù.\n\n\n## Let‚Äôs Start Building\n\nThe year is 2014, and the buzzword ‚Äúelastic‚Äù is still on-trend. Our goal: liberate innocent app developers from the horrors of server management, abstracting services written in Python or Javascript from the unruly runtimes they depend on. You‚Äôll give us a function, and we‚Äôll run it.\n\nOnce this is invented, you‚Äôll probably want to use it to optimize sandwich photos uploaded by users of your social sandwich side project.\n\nElasticSearch would shorten its name to Elastic in 2015, marking the peak of this fad.\n\nThe first tool in our toolbox is the virtual machine. VMs were arguably ‚Äúserverless‚Äù avant la lettre, and Lambda itself literally stood on the shoulders of EC2, so that‚Äôs where we‚Äôll begin.\n\nTake a big, bare-metal x86 server sitting in a datacenter with all the standard hookups. Like every server, it has an OS. But instead of running apps on that OS, install a Type 1 (bare-metal) hypervisor, like the open-source Xen.\n\nThe hypervisor is itself like a tiny operating system, but it runs guest OSs the way a normal OS would run apps. Each guest runs in a facsimile of a real machine; when the guest OS tries to interact with hardware, the hypervisor traps the execution and does a close-up magic trick to maintain the illusion. It seems complicated, but in fact the hypervisor code can be made a good deal simpler than the OSs it runs.\n\nNow give that hypervisor an HTTP API. Let it start and stop guests, leasing out small slices of the bare metal to different customers. To the untrained eye, it looks a lot like EC2.\n\nEven back in 2014, EC2 was boring. What we want is Lambda: we want to run functions, not a guest OS. We need a few more components. Let‚Äôs introduce some additional characters:\n\nThe Frontend reads an Invoke request for a function we want to run. (Someone‚Äôs just uploaded an image to your S3 sandwich bucket through your app.) Frontend asks a Manager to provide the network address of a Worker VM containing an instance of your function, where it can forward the request. The Manager either quickly returns an existing idle function instance, or if none are currently available, asks a Placement service to create a new one.\n\nThis is all easier said than done. For instance, we don‚Äôt want to send multiple requests racing toward a single idle instance, and so we need to know when it‚Äôs safe to forward the next request. At the same time, we need Manager to be highly available; our Manager can‚Äôt just be a Postgres instance. Maybe we‚Äôll use Paxos or Raft for strongly-consistent distributed consensus, or maybe gossiping load and health hints will be more resilient.\n\nWe can straightforwardly run a function instance on a Worker VM. But we can‚Äôt just use any old VM; we can‚Äôt trust a shared kernel with multitenant workloads. So: give each customer its own collection of dedicated EC2-instance Workers. Have Placement bin-pack function instances onto them. Boot up new Workers as needed.\n\nAnother catch: it takes seconds or even minutes to boot a new Worker. This means some of our requested functions have unacceptably (and unpredictably) high ‚Äúcold start‚Äù time. (Imagine, in 2022, holding on to your excitement for over a minute waiting for your image of the local sandwicherie‚Äôs scorpion-pepper grilled cheese to insert itself into your chat.) Have Placement manage a ‚Äúwarm pool‚Äù of running VMs, shared across all customers. Now functions can scale up quickly. To scale down, Manager periodically vacuums idle VMs, returning them to the warm pool for reuse.\n\nScale is our friend. We have lots of customers, so the warm pool smooths out unpredictable workloads, reducing the total number of EC2 instances we need. But we‚Äôre not out of the woods yet. We can get huge spikes of consumption: say, an accidentally-recursive function. One broken customer brings everyone else back to cold-start latency. The easiest fix: soft limits (‚Äúcontact us if you need more than 100 concurrent executions‚Äù). Beyond that, the service could adopt a token bucket rate-limiting mechanism to allow a controlled amount of sustained/burst scaling per customer or function.\n\nWe‚Äôve sketched most of orchestration, but hand-waved the actual function invocation. It‚Äôs not all that complicated, though.\n\nOnce Placement allocates enough resources on a Worker, it can load up the function instance there. Remember, it‚Äôs still 2014, and Docker only just became production-ready, so we‚Äôll roll our own container the old-fashioned way. A daemon on the Worker VM:\n\nGoogle Cloud Functions originally didn‚Äôt freeze its function instances and only billed the function execution- so you could run a Bitcoin miner in a background process on an idle function without paying a dime.\n\n\n## Iterating On The Design\n\nWe‚Äôve come up with a relatively naive design for Lambda. That‚Äôs OK! We‚Äôre Amazon and we can paper over the gaps with money and still have enough left over to make hats. More importantly, we‚Äôre out in front of customers, and we can start learning.\n\nFast forward to 2018. We made it. ‚ÄúServerless‚Äù is the new ‚Äúelastic‚Äù and it‚Äôs all the rage. Now let‚Äôs make it fast.\n\nWhat‚Äôs killing us in our naive design is Xen. Xen is a bare-metal hypervisor designed to run arbitrary operating systems in arbitrary hardware configurations. But our customers don‚Äôt want that. They‚Äôre perfectly happy running arbitrary Linux applications on a specific, simplified Linux configuration.\n\nEnter Firecracker.\n\nFirecracker is modern hypervisor built on KVM and exploits paravirtualization: the guest and the hypervisor are aware of each other, and cooperate. Unlike Xen, we don‚Äôt emulate arbitrary devices, but rather virtio devices designed to be efficient to implement in software. With no wacky device support, we lose hundreds of milliseconds of boot-time probing. We can be up and running in under 125ms.\n\nFirecracker can fit thousands of micro-VMs on a single server, paying less than 5MB per instance in memory.\n\nThis has profound implications. Before, we were carefully stage-managing how function instances made their way onto EC2 VMs, and the lifecycle of those EC2 VMs. But now, function instances can potentially just be VMs. It‚Äôs safe for us to mix up tenants on the same hardware.\n\nWe can oversubscribe.\n\nOversubscription is a way of selling the same hardware to many people at once. We just bet they won‚Äôt all actually ask to use it at the same time. And, at scale, this works surprisingly well. The trick: get really good at spreading around the load across machines to keep resource usage as uncorrelated as possible. We want to maximize server usage, but minimize contention.\n\nFirecracker lets us spread load more evenly, because we can run thousands of different customers on the same server.\n\nOur Workers are now bare-metal servers, not EC2 VMs. We need a warm pool of them, too. It‚Äôs a lot of extra micro-management. And it‚Äôs worth it. The resource-sharing shell game is way more profitable. Reportedly, Lambda runs in production with CPU and memory oversubscription ratios as high as 10x. Not too shabby!\n\nThere‚Äôs a tradeoff to this. We‚Äôve aggressively decorrelated our server workloads, shuffling customers onto machines like suits in a deck of cards. But now we can‚Äôt share memory across functions, like the classic pre-forking web server model.\n\nOn a single server, a function with n concurrent executions might consume only slightly more memory than a single function. Shuffled onto n machines, those executions cost n times more. Plus, on the single server, instances can fork instantly from a parent, effectively eliminating cold-start latency.\n\nAnd now we have a network-sized hole in performance. Functions are related; they‚Äôre intrinsically correlated. Think about serverless databases, or map-reduce functions, or long chains of functions in a microservice ensemble. What we want is network locality, but we also want related loads spread across different hardware to minimize contention. Our goals are in tension.\n\nSo some functions might perform best packed tightly to optimize performance, while others are best spread thin for more distributed resource usage across servers. Some kind of hinting along the lines of EC2 placement groups could help thread the needle, but it‚Äôs still a hard problem.\n\nAt any rate, we have a design, and it works. Now let‚Äôs start thinking about the ramifications of the decisions we‚Äôve made so far, and the decisions that we have yet to make.\n\nFly Machines are Firecracker VMs with a fast REST API that can boot instances in about 300ms, in any region supported by Fly.io. Care to craft your own twist on serverless?\n\n\n## Ramifications for Concurrency\n\nLambda‚Äôs one-request-per-instance concurrency model is simple and flexible: each function instance can handle one single request at a time. More load, more instances.\n\nThis works like Common Gateway Interface (CGI) of yore, or more precisely, like implementations of its successor FastCGI which reuse instances across requests.\n\nScaling is simple and straightforward. Each request is handled in its own instance, separate from all other concurrent requests. No locks, thread-safety or any other parallel programming concepts.\n\nBut handling concurrent requests in a single instance can be more efficient, especially for high-performance web application servers that can leverage asynchronous I/O event loops and user-space threads to minimize context-switching overheads. Google‚Äôs Cloud Run product supports configurable per-instance concurrency. Lambda‚Äôs design makes it harder for us to pull off tricks like that.\n\n\n## Ramifications for Pricing\n\nIf we‚Äôre Lambda, we bill per-second duration based on memory use, with a per-request surcharge; like a taxi meter, we have a base fee, and then the meter ticks up as long as we‚Äôre working.\n\nTwo ways of looking at the request fee. First, it‚Äôs a fudge factor representing the aggregate marginal costs of the various backends involved in handling the request.\n\nBut if you‚Äôre an MBA, it‚Äôs also a way to shift to ‚Äúpay-for-value‚Äù or value-based¬†pricing, a¬†founding tenet¬†of Lambda. Value pricing says that you pay based on how useful the service is; if we figure out ways to deliver the service more cheaply, that‚Äôs gravy for us. Without the surcharge, we‚Äôre doing cost-plus pricing. You‚Äôd just pay for the resources we allocated to you.\n\n(Remember, we‚Äôre up to 10x oversubscribed. Customers are, on average, utilizing only 10% of the resources they pay for.)\n\nWe combine CPU and memory pricing to simplify duration-based pricing. Simple is good, but costs our users flexibility if they have lopsided CPU or memory-heavy functions. For that problem, there‚Äôs Fargate, Lambda‚Äôs evil twin.\n\nThis pricing seems simple! But it‚Äôs actually a little bit complicated, if you are sensitive to cost.\n\nYour image-cruncher function might be making good use of its resources for most of its running time. But what if a function process is actually really fast? It might actually skew cheap in resources and expensive in requests.\n\nAnd now, you‚Äôve added a function to periodically scrape the major socials for new pictures tagged with any sandwich, artisanal sandwich stockist, or vending machine known to your database. Or, better, say you‚Äôre Max Rozen, doing uptime checks on every endpoint in your database. Now you‚Äôre paying full whack for CPU and RAM usage the whole time you wait (up to 10s) for a response from each one, to, you know, see if it‚Äôs online.\n\nThe value-based pricing here hits the sweet spot for functions that a) run long enough per request to amortize the request cost, and b) make enough use of the provisioned resources, while they run, to justify paying for them that long.\n\nPrioritizing nimble scaling, combined with instance-per-request and per-request billing, does set up a potential footgun for our customers. Don‚Äôt DDoS yourself.\n\nWe‚Äôre counting on the product as a whole to add enough value to keep less price-sensitive customers coming back, even far from the sweet spot.\n\n\n## Ramifications for APIs\n\nThe public runtime API to a Lambda function is the Invoke REST API, which accepts a POST method specifying the function name and request ‚Äúpayload‚Äù, and requires a signature with appropriate AWS credentials. This conforms to Amazon‚Äôs monolithic, internally-mandated API structure, but practically unusable outside the API-wrangling comfort of the AWS SDK.\n\nA cottage industry has sprung up around frameworks just to help you hook Lambda up to the web. Amazon built one of them into CloudFormation. Problem: too much YAML. Solution: more YAML.\n\nThe way out is embarrassingly simple: the runtime API can just pass HTTP requests directly to the function instance. Most of what ‚ÄúAPI gateways‚Äù do can be built into HTTP proxy layers. For the common case of web applications, an HTTP-based API eliminates a layer of indirection and plugs in nicely with the mature ecosystem of web utilities.\n\n\n## Ramifications for Resilience\n\nLambda‚Äôs execution environment sets strict limits:\n\nThis tightly-scoped lifecycle is great for the platform provider. It helps workloads quickly migrate away from overloaded or unhealthy instances, and makes it easy to shuffle functions around during server maintenance and upgrades without impacting services. And what‚Äôs good for the platform is probably good for most customers, too!\n\nBut it‚Äôs not ideal for apps\n\nOne alternative is for the platform to try to keep servers up and running forever, but sometimes you just have to reboot servers to patch stuff. Another option to recycling VMs is live migration, sending a snapshot of the running VM over the network to the new server with as little downtime as possible. Google Compute Engine supports live migration for its instances and uses the feature to seamlessly conduct maintenance on its servers every few weeks.\n\nDespite the simple runtime interface, Lambda functions run in a full Linux runtime environment that let you run your own x86 executables on the platform, which gives you all of POSIX for your application to play with.\n\nIf your apps can do with less, a ‚Äúlanguage sandbox‚Äù can offer some isolation without the overhead of virtualization. Google App Engine adopted this with tuned language runtimes that disabled networking and writing to the filesystem by disabling/customizing Python modules and restricting Java class usage. CloudFlare Workers adopt a similar approach with the v8 runtime wrapping JavaScript code in ‚Äòisolates‚Äô, offering a restricted set of runtime APIs modeled loosely after JavaScript browser APIs.\n\nWebAssembly extends the language-sandbox approach with a virtual instruction set architecture, either embedded within v8 isolates or run by a dedicated server-side runtime like wasmtime.\n\nFastly built its Compute@Edge product around WebAssembly/WASI. However, WASI is still young and evolving quickly. On the serverside, WASM‚Äôs overhead doesn‚Äôt pay its freight: there‚Äôs as much as a 50% performance gap between WASM and native code, which makes virtualization look cheap by comparison.\n\n\n## How did I do?\n\nI just designed a shameless knockoff of Lambda, the most popular specimen of the most serverless of serverless services: a fleeting scrap of compute you can will into being, that scales freely (not in the monetary sense) and fades into oblivion when it‚Äôs no longer needed.\n\nThis article contains no small degree of bias! There‚Äôs also no small degree of appreciation for the craft that goes on behind the curtain at AWS and other purveyors of ‚Äúserverless‚Äù services."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-2022-06-23/",
    "content": "This is a Logbook post. It tells you what we‚Äôve been up to here at Fly.io, to make running your code close to users better in all the ways. The proof of the pudding is in the eating, though, so you should dig in now; it‚Äôll take you just a minute or two to get up and running.\n\nWe‚Äôve had a lot of changelogs about our Phoenix/LiveView-based web UI in recent weeks. It‚Äôs pretty rad; we‚Äôve been vocal about being CLI-first, but we love a first-class dashboard. Our dashboard has sprouted a lot of new capabilities, and at this point you‚Äôre missing out if you never use it!\n\nThat doesn‚Äôt change the power and adaptability of the CLI‚Äîwe‚Äôll never not love flyctl (alias fly). Speaking of which: we have a few flyctl‚ú® entries in this here collection. Because flyctl is open-source, you can peek in and see what that means in terms of commits, any updates we may have missed, and, notably, lots of fun activity around Fly Machines.\n\nAnd now the changelog:"
  },
  {
    "title": "Come for the Rust esoterica, stay for the hosting",
    "url": "https://fly.io/blog/rust-memory-leak/",
    "content": "This is a post about fixing a memory leak in our Rust-based proxy, fly-proxy. That‚Äôs the code that gets your users‚Äô requests to the nearest VM that can fulfill them, on one of our servers in one of 21 regions worldwide. Take it for a spin by deploying an app in mere minutes.\n\nWe have a Rust-based proxy. It was leaking memory. We fixed it, and we‚Äôll talk about that, but to be really thorough, we‚Äôll look at how loading a web page works. Starting with hardware interrupts.\n\nThe downside of Thomas writing job postings is that they can turn out to have a great little primer on fly-proxy that we just have to link to, even though we are NOT hiring Rust devs at the moment.\n\n\n## Loading a web page ‚Äî a journey\n\nYou type https://fly.io in your browser address bar and hit enter. What happens next?\n\nFirst off, are you even using a keyboard? Not everyone can use a keyboard: voice input may be a suitable alternative there. Soft keyboards like the ones that pop up on mobile devices when you focus an input also don‚Äôt count ‚Äî they‚Äôre software, like the name implies.\n\nAs keys get pressed, electrical contact is made between two conductive bits, which closes a circuit, and the microcontroller inside the keyboard (it‚Äôs computers all the way down) takes note and stuffs it in a buffer.\n\nBack when you plugged in said keyboard, or, more likely, when your computer woke up and started enumerating USB devices, they entered a negotiation: the keyboard announced that it was HID class (for human interface device), and needed an ‚Äúinterrupt transfer‚Äù at a certain rate, and that‚Äôs the rate at which the computer will poll that device for‚Ä¶data.\n\n(What about Bluetooth, proprietary wireless dongles, or even laptop keyboards? Chances are, these all end up being USB anyway, as far as your computer is concerned. Yes, even for internal devices. It‚Äôs just easier that way)\n\nAnd then well, your computer does poll the device at the negotiated rate, and processes events in-order. So really, there‚Äôs no hardware interrupts involved.\n\n(To be pedantic, because it‚Äôs that kind of article, your USB keyboard can actually interrupt the CPU, but that‚Äôs only so the BIOS can be driven by your keyboard. By the time a real operating system has been started up, that behavior has been overriden.)\n\nYour operating system then does translation between scan codes (that depend on where the keys are located on the keyboard) and key codes, like ‚Äúthe letter A‚Äù. Then that goes through a bunch of event loops in the OS and your browser, and finally, we have https://fly.io somewhere in RAM.\n\nSo far so good.\n\nAnd then, well, browsers are complicated beasts. If your browser is Chrome, then it checks some probabilistic data structure for safe browsing‚Äîif the domain is on the Bad List, then you get a scary page that tells you to click away! Now!\n\nAfter that, or maybe in parallel, a DNS lookup happens, which translates the domain fly.io into an IPv4 and/or IPv6 address. This may happen over UDP, or it may happen over HTTPS! Or maybe it doesn‚Äôt happen, because it‚Äôs in the browser‚Äôs DNS cache, or the OS‚Äôs DNS cache, or your local router‚Äôs DNS cache. It‚Äôs caches all the way down, really.\n\nIf that succeeds, your browser tries to establish a TCP connection to that IP address. Because it‚Äôs an anycast IP address, packets get routed to an edge node nearby. For me that‚Äôs Paris, France. For my colleagues, it‚Äôs Toronto, Canada. Or South Africa, Brazil, the UK etc. It‚Äôs really quite the list.\n\n(That‚Äôs assuming BGP routes are behaving that day. BGP is like DNS in that it‚Äôs always to blame somehow. It‚Äôs how different AS (autonomous systems), or peers inside the same AS, know where to send a packet next, so that it eventually reaches its destination.\n\nWhen it works, it sends packets on‚Ä¶maybe not the best path, but a decent path. When the routes are wrong, it can send packets halfway around the world. And when it gets hijacked, well, it makes the headlines. Take notes, TV writers!)\n\nIt‚Äôs not like your browser crafts packets itself‚Äîthat would let it spoof IP addresses, which is generally frowned upon. No, it asks the underlying operating system to please establish a TCP connection, and that sends a SYN packet.\n\n(Technically it‚Äôs ‚Äúa TCP segment with the SYN flag set, wrapped in an IP datagram, wrapped in an Ethernet frame‚Äù, but that doesn‚Äôt exactly roll off the tongue).\n\nThat packet goes on quite a journey.\n\nIt goes from userland to kernel space, out a NIC, shooting through the air, or through copper, then almost definitely fiber, crossing the land, maybe speeding through the ocean deep if you live too far from us‚Äîfinally it enters a datacenter, a NIC, and the Linux kernel networking stack.\n\nTherein lies our first bit of Rust: an eBPF program, built with aya.\n\nBecause our edge nodes have to listen on entire ranges of IPv4 and IPv6 addresses, and also all ports at the same time, we have a small program, loaded in the kernel, that decides whether the connection you‚Äôre trying to establish is allowed or not.\n\nThat‚Äôs all determined by the app configuration. In our fictional scenario, your browser is connecting to fly.io on port 443, and that‚Äôs a-ok. The TCP handshake completes, our second bit of Rust is ready to take over.\n\nAt first your connection sits in an accept queue, unless someone is flooding us with SYN packets, in which case there‚Äôs cookies involved, no, not that kind, and unfortunately not the tasty kind either.\n\nWe try to process that accept queue as fast as the CPU will let us, asynchronously with tokio, which really actually uses mio, which really ‚Äújust‚Äù uses a kernel interface, in this case, epoll.\n\n(In practice, this just means we can handle a lot of connections concurrently, with only a spoonful of OS-level threads. It‚Äôs all event-based and all the functions are state machines in a trenchcoat. It‚Äôs a whole thing.)\n\nBecause the Fly.io app has a TLS handler, your browser and fly-proxy engage in a multi-stage dance to establish a secure tunnel. Through that dance, we are able to prove that you are visiting the real Fly.io by presenting a valid certificate for it, signed by third parties that your OS trusts, and your browser does too.\n\nNow that we‚Äôve negotiated a secure channel, we‚Äôre ready to move on to more substantial exchanges.\n\nBecause you‚Äôre using a modern browser, it supports HTTP/2, and so do we, thanks to hyper. That means the exchange is all binary, and I can‚Äôt show what it would look like without whipping out some sort of protocol analyzer like Wireshark.\n\nAny HTTP requests you send over that connection are handled by a tower service created especially for you, and of course there‚Äôs a bunch of concurrency limits involved: some set in the app configuration, and some global, just so everything stays nice and fast for everyone.\n\nBecause we occasionally need to look into how fly-proxy operates, or how a certain request was handled, a lot of internals are instrumented with tracing, which generates spans and log events that we funnel to a large store we can later query.\n\n(We‚Äôre able to turn up the verbosity for single requests, which is what we do when you report a problem and we‚Äôre trying to reproduce it! There‚Äôs no special ‚Äúdebug‚Äù build of fly-proxy, it‚Äôs all dynamically configured.)\n\nSo we get your request, and if there‚Äôs an instance of the app running on the same node, we‚Äôre able to serve it directly. And if not, we proxy it to a nearby node that does have a running instance: taking into account things like the round-trip time to that node, and how busy it is.\n\nEventually, just as the request was, the response is proxied all the way back to your computer, your browser gets a bunch of HTML (maybe compressed, maybe not), and before it‚Äôs even done parsing it, it immediately fires up a half-dozen new requests, re-using the same connection.\n\nAnd then we leak a bunch of memory.\n\n\n## What do you mean you leak a bunch of memory\n\nWell, we used to!\n\nIt wasn‚Äôt, like, a lot. But it added up. That‚Äôs the thing with leaks: when request volume goes up, you expect resource utilization to go up, too. But when request volume goes down, and utilization doesn‚Äôt go down too, well‚Ä¶I mean that is an incentive to deploy often.\n\nAnd it‚Äôs surprising! Because in languages with manual memory management, you can malloc and forget to free. You can new, and forget to delete.\n\nBut in Go for example, you can‚Äôt! Because there‚Äôs a garbage collector, which periodically runs, looks at all the stuff, and frees the stuff no one remembers. It‚Äôs just like in Coco (2017), except less sad.\n\nOf course that‚Äôs a lie. Because it doesn‚Äôt look at all the things, it‚Äôs generational. And you can totally leak memory with a GC. All you need to do is stuff references to a bunch of big structs in a big map, and never take them out. That way they always remain reachable, are never collected, and memory usage goes weeeeeeeee.\n\nAs for Rust, it‚Äôs RAII-ish, so when you hold a value, it‚Äôs fully initialized, and when it falls out of scope, the associated memory (and any other resources) gets freed. And if that‚Äôs not enough, you can do reference-counting via Rc, or Arc if you need to share stuff across threads, and then you can have multiple things pointing to a single thing, which only gets freed when no things point to it any longer.\n\nThat scheme has different performance characteristics than a GC: the cost is more ‚Äúspread out‚Äù (since stuff gets freed as soon as it‚Äôs no longer needed), there‚Äôs no pauses to worry about (even micro ones), people like it for real-time processing, high-performance network applications, and all sorts of other stuff.\n\nExcept there too, same deal: if you really try, you can leak memory in Rust. Don‚Äôt believe us? Lily says so too.\n\nBut we weren‚Äôt trying. And we didn‚Äôt really have an explanation we liked.\n\nAnd historically, things haven‚Äôt been so good on that front: there‚Äôs two problems you really didn‚Äôt want to have when you had a big Rust async application in production:\n\nThe first got a lot better with the advent of tracing. You‚Äôre still not able to dump ‚Äúall async stack traces‚Äù the way Go lets you, for example. Instead you instrument some code: here we‚Äôre establishing a connection, here we‚Äôre sending HTTP headers, and your downstream crates do too (like hyper), and then you‚Äôre looking at something a litlte more semantic than a stack trace.\n\nBut you also, still, can‚Äôt really dump it all. Something like tokio-console solves this and more, BUT it‚Äôs still early days, and not really something you can run in production today.\n\nAs for the second (leaking resources), I remember struggling with it just a year ago. Because allocations and deallocations are very fast and it‚Äôs all very nice, but there‚Äôs absolutely no one keeping track of where or why anything was allocated.\n\nThat wouldn‚Äôt be very zero-cost.\n\nAnd yet, sometimes RAII fails you. Sometimes you just keep stuffing items into a Vec or a HashMap and never ever clean it up. The question is: where? And how are you going to find that out without exporting metrics for every last container type in your codebase?\n\nLeak detectors have existed as long as we‚Äôve had leaks. The remarkable Valgrind tool suite comes with MemCheck, which has a --leak-check option. But it checks for a lot of errors that simply can‚Äôt happen in safe Rust code, and makes your program run 20-30x slower, also using ‚Äúa lot more memory‚Äù.\n\nSo, not an option for production web services.\n\nIn fact, if you‚Äôre writing a production network service, chances are you‚Äôve switched away from the system memory allocator (glibc) to something like jemallocator, which, on top of being (sometimes) faster, is also less prone to fragmentation, and comes with a wealth of tools to monitor memory usage.\n\nIncluding a heap profiler, which you can use to check for leaks. It feels like Go‚Äôs pprof, not a surprise since they‚Äôre both based on gperftools, which builds on the ideas in GNU gprof, in turn an extended version of the standard Unix prof tool.\n\nBut at the end of the day, you‚Äôre either looking at extremely long text output that doesn‚Äôt really have any temporal information, or a gigantic file PDF that makes you wish you had a trackball mouse handy.\n\nIt‚Äôll take less than 10 minutes to get almost any container you‚Äôve got running globally on our Rust-powered anycast proxy network.\n\n\n## Enter bytehound\n\nBytehound is a memory profiler written in Rust, which works extremely well for Rust codebases (but, I‚Äôm assuming, C \u0026 C++ codebases too!). I had no hand in it ‚Äî I just think it‚Äôs extremely cool.\n\nFor me, bytehound is the poster child for ‚ÄúNIH good? sometimes?‚Äù. Its custom stack unwinding implementation makes it orders of magnitude faster than competing tools. It‚Äôs designed to stream data to disk or over the network to some ‚Äúgatherer‚Äù process, which means its impact in production is minimal.\n\n(The non-NIH alternatives would be‚Ä¶something like heaptrack, Valgrind‚Äôs memcheck tool, one of the ‚Äúchecking‚Äù malloc implementations like Dmalloc, or even swapping malloc with an actual GC like Boehm.)\n\nIt‚Äôs all runtime instrumentation, too ‚Äî I was able to inject it straight into my veins our production binary with LD_PRELOAD: no recompilation needed here. And it supports jemallocator!\n\nIn fact, of the whole hour I spent on this, 80% were spent fighting with the dynamic linker/loader itself. Normally, it lets you inject just about any library into any executable, as long as the bitness matches up and all the dependencies are satisfied.\n\nBut, in secure-execution mode:\n\nSo, long story short, moving libbytehound.so to a standard search path like /lib/x86_64-linux-gnu and invoking chmod u+s on it did the trick.\n\nThe last 20% was a breeze: by default, bytehound starts profiling immediately, writing to a .dat file on disk as it goes. I applied a touch of load with oha, another of my Rust favs, monitored memory usage in htop, it goes up, it goes up, it don‚Äôt go down, at least it‚Äôs easy to reproduce.\n\nI then restarted fly-proxy (this is zero-downtime, never fear) opened the profile, and‚Ä¶no leaks.\n\nWell, none that matter:\n\nRestarting fly-proxy involves spinning up another instance, waiting until it‚Äôs ready to handle connections, then asking the previous instance to stop listening (then leaving it some time to handle established connections).\n\nAnd the drop you can see right before 10:33:00 is when we stop listening: memory usage drops back to the ~40MB we use for internal state. According to bytehound, our inventory of nodes and services (and SQLite‚Äôs cache) are the only things we leak:\n\n(Internal state on the left, SQLite mem cache on the right)\n\nSo RAII is working. And to be clear, I‚Äôm not really talking about the ‚Äúby the time you hold a value of a given type, the associated resource is fully initialized‚Äù part, I‚Äôm talking about the ‚Äúas soon as you let go of that value, associated resources get freed, too‚Äù part.\n\nFor my second try, I did‚Ä¶exactly the same thing, except I stopped profiling by sending SIGUSR1 before restarting fly-proxy, so that bytehound would consider anything that hadn‚Äôt been freed at that point leaked.\n\nThe ‚Äúonly leaked‚Äù flamegraph looks very different this time around:\n\nZooming in a little, we can see that almost all of it is allocated in fly_proxy::trace::honeycomb::HoneycombLayer:\n\nBut that‚Äôs not even the best view of it: the bytehound UI lets you filter/sort allocations any which way you like. For example, we can ask for ‚Äúonly leaked‚Äù, grouped by backtraces, sorted by largest leak first, with graphs:\n\nAnd seeing this, there‚Äôs no question. And there‚Äôs no distractions like in the flamegraph: the internal state we leak (because of background tasks we don‚Äôt cancel properly before shutting down ‚Äî that one‚Äôs on me) looks very different:\n\nThat‚Äôs not the last of bytehound‚Äôs goodies: using just a little bit of rhai scripting:\n\nWe can plot exactly the graph we want to see‚Äîhere: does our HoneycombLayer account for most of the leak?\n\nYes. Yes it does.\n\n\n## Why though? A tracing primer\n\nLet me first say: the leak wasn‚Äôt in any code provided by Honeycomb itself, or even any publicly available crate.\n\nA long time ago, Fly.io‚Äôs traffic was low enough that we could afford to send some traces to Honeycomb.\n\n(Or at least we thought we could, until Honeycomb emailed saying ‚Äúhey, wtf!‚Äù. They have burst protection now; it was a different time.)\n\nThe tracing / OpenTelemetry ecosystem wasn‚Äôt as fleshed-out as it is now. So we had to write custom code. And I say we, but I wasn‚Äôt there.\n\nWhen I started getting comfortable with fly-proxy‚Äòs codebase, and after I was done making CI builds faster (switched linkers, split crates, adopted cargo nextest, set up incremental compilation caching, etc.), I noticed that custom HoneycombLayer code, and wanted to remove it, but after all, it wasn‚Äôt even exporting traces any more, so what harm could it do?\n\nThe way tracing works is that you can instrument functions: this opens and enters a span when the function is called, exits that span when the function returns, and, if nothing else refers to that span, the span is closed, and eventually exported somewhere nice.\n\nIt works almost the same way for async functions in Rust, except async functions are state machines that can be polled from any thread. So, the span is opened, and it‚Äôs entered+exited every time the async function is polled.\n\n(Wait, polling? Yes, but only when there‚Äôs work to be done: some socket is ready to be read from / written to, some timer has elapsed, some lock has been acquired, etc.)\n\nEach span has a name, a target, and a bunch of fields, which a good tracing platform like Honeycomb lets you use for filtering, sorting, heck, even monitoring. The duration of the span is tracked (from the time it‚Äôs opened to the time it‚Äôs closed), and when async functions are instrumented, we keep track of busy_ns too‚Äîhow much CPU time we spent on it, and idle_ns‚Äîhow long the task waited for something else to happen.\n\nIf you want to learn more about what Honeycomb feels like in conjuction with Rust tracing, I‚Äôve written a whole thing about it recently.\n\nThing is, if you instrument your main function‚Ä¶well not that, but a similarly high-level function, like handle_incoming_connections, that span opens, is entered and exited a bunch of times, but never closes until you stop listening.\n\n‚Ä¶sounds familiar?\n\nAnd our custom, should-have-thrown-it-out-along-time-ago HoneycombLayer had something along the lines of: parent_span.children.add(span) whenever a span was closed. Which means every request was leaking a tiny bit of memory.\n\nTo reiterate: the issue was our own prehistoric Honeycomb tracer implementation, not Honeycomb. We love Honeycomb and OpenTelemetry at Fly.io.\n\nWhen the proxy was restarted, that top-level task was ‚Äútripped‚Äù, exited, the associated span closed and freed, on some nodes, tens of gigabytes of memory ‚Äî letting most heap profilers think everything was just peachy.\n\nThe PR to fix it was the most satisfying kind: sending old code into retirement.\n\nBut did it fix the issue?\n\nAbsolutely. Memory usage barely went over 50MB during the load test.\n\nWhat‚Äôs that? ‚ÄúLeaked memory usage‚Äù keeps growing ever so slightly? Let‚Äôs focus on the part where it seems stable:\n\nWhy yes, that line does go up, still.\n\nShowing leaked allocations, grouped by backtraces, with graphs, but this time only between two precise timestamps, we can see what‚Äôs happening. And it‚Äôs not hard to explain!\n\nTake that one:\n\nThat‚Äôs a queue. Under load, backing storage is grown to accomodate more items being, well, queued. It‚Äôs never shrunk again, that would just be extra work. This is fine as long as it‚Äôs bounded in some way.\n\nSame deal here:\n\nBecause we keep track of latency for all requests, load generates a bunch of data points there ‚Äî we need to keep some data around to compute various percentiles, until an interval was elapsed and we send them out to Prometheus.\n\n\n## That‚Äôs it\n\nOf course you can leak memory, even in Rust. For even medium-sized long-running applications, lots of graphs from a good memory profiler can make life better. And they‚Äôll probably help you find the memory leak too."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-2022-06-10/",
    "content": "Fly.io makes it easy to host applications worldwide the same way a CDN hosts HTML pages. Our users ship us containers, and we transmute them into Firecracker microVMs that run on our hardware in data centers around the world. The easiest way to learn more is to sign up; if you‚Äôve got a working container now, it can be running in Sydney, Chennai, or Amsterdam in just a few minutes.\n\nHere‚Äôs our latest changelog. This week we‚Äôre putting the in-browser UI updates a little closer to all the other ones, to see if they‚Äôll play nicely together."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-2022-06-01/",
    "content": "We‚Äôre Fly.io. We take container images and run them on our hardware around the world. It‚Äôs pretty neat, and you should check it out; with an already-working Docker container, you can be up and running on Fly in well under 10 minutes.\n\nHere‚Äôs our latest changelog. Looking back over the week, our forum community has been quite a driver of (logged) change. When you‚Äôre done here, head over there to be a part of it!\n\nOur web dashboard still needs its own section:"
  },
  {
    "title": "The Fly Machines API is the latest thing",
    "url": "https://fly.io/blog/logbook-2022-05-26/",
    "content": "The other day, we unleashed the (Fly) Machines. Which is to say: we now officially have an API for starting and stopping fast-booting Firecracker VMs directly, bypassing orchestration. Normal Fly.io apps will carry on, oblivious to the fuss. You can spin up an app in a matter of minutes.\n\nWork leading up to the Fly Machines launch involved a multitude of changes by many of the cogs in this corporate machine, but that‚Äôs not to say the other production lines have been idle. For one thing, our web UI has been transforming before our eyes. We‚Äôve grouped its updates at the end, for easy digestion (and so we don‚Äôt have to type ‚Äúweb UI‚Äù so many times).\n\nHere‚Äôs our latest changelog:\n\nWeb UI improvements! We have a revamped dashboard and app page, and more:\n\nIf you‚Äôve got a Fly.io account, you can play with our new API for fast-booting Firecracker VMs."
  },
  {
    "title": "Play with the Machines API",
    "url": "https://fly.io/blog/fly-machines/",
    "content": "Fly.io turns Docker images into running VMs on physical servers all over the world. We built an API for booting VMs very quickly. Here‚Äôs what you need to know.\n\nFly Machines are VMs with a fast REST API that can boot instances in about 300ms.\n\nOur proxy can boot Fly Machines for you, and you can shut them down when they‚Äôre idle. Which means you can cost-effectively create VMs and keep them standing by to handle new requests.\n\nIf you‚Äôre already running an app on Fly.io, here‚Äôs what you need to know. Machines are basically the same as the VMs we manage with our orchestration system (Nomad). When you deploy an app, we create new VMs and destroy the old ones with your preferred image. Fly Machines are one level lower, and let developers manage individual VMs.\n\nWe built Machines for us. Our customers want their apps to scale to zero, mostly to save money. Also because it feels right. An app that isn‚Äôt doing anything shouldn‚Äôt be running. Fly Machines will help us ship apps that scale to zero sometime this year.\n\nFly Machines may be useful to you, too. A lot of y'all want to build your own functions-as-a-service. You can build a FaaS with Fly Machines.\n\n\n## How to boot VMs in a hurry\n\nWe said we want our VMs to boot fast. They already do; Firecracker is pretty darn fast to boot a given executable on a given host. Our job is to get our own plumbing out of your way, and get you close to local-Firecracker speeds.\n\nSpinning up a VM as fast as possible on a server somewhere is an exercise in reducing infrastructure latency. We need to play latency whack-a-mole.\n\nWhen you ask for a VM, you wait for network packets to travel to an API somewhere. Then you wait for them to come back. The API is also, at minimum, talking to the host your job will run on. If all your users are in Virginia and your API is in Virginia and the hardware running Firecrackers is in Virginia, this might take 20-50ms.\n\nIf your users are in Sydney and the hardware for the Firecrackers are in Sydney and the API is in Virginia, ‚Äúboot me a VM‚Äù takes more like 300ms. Three tenths of a second just waiting for light to move around is not fast.\n\nWe‚Äôre not done. You need something to run, right? Firecracker needs a root filesystem. For this, we download Docker images from our repository, which is backed by S3. This can be done in a few seconds if you‚Äôre near S3 and the image is smol. It might take several minutes (minutes!) if you‚Äôre far away and the image is chonk.\n\nWe solve this by making you create machines ahead of time. Accountants (not the TikTok kind; actual accountants) call this ‚Äúamortization‚Äù ‚Äì¬†pay the cost up front, yield the benefit over time.\n\n\n## The slow part\n\nHere‚Äôs what happens when you call the create machine endpoint:\n\nIf you‚Äôre an app developer in Los Angeles and you want a machine in S√£o Paulo, your request gets routed to your friendly local API server. We run API servers in every region, so this part of the process is fast.\n\nThe API server makes a preflight request to our centralized database in Virginia, which gives back a yay (or nay!) and an immutable Docker image URL.\n\nOur database in Virginia has to be looped in on machine creation. We want a strongly-consistent record that machines exist. We also want to make sure you can‚Äôt create a machine if you‚Äôre 8 months behind on bills or got banned for mining cryptocurrency with a stolen credit card.\n\nThe Los Angeles API instance then broadcasts a NATS message to the available hosts in S√£o Paulo saying ‚Äúhey, reserve me a machine with these specs‚Äù. Hosts with resources to spare reserve a slice of their capacity and reply with information about what they have to offer.\n\nThe API server evaluates each host‚Äôs offer, picks one, and says ‚ÄúOK, host, create a machine for reservation X‚Äù. The API server then records a machine record in our centralized database. The other hosts garbage-collect the unused reservations a few seconds later.\n\nYou might be thinking ‚Äúif I‚Äôm in Los Angeles and I request a machine in S√£o Paulo, won‚Äôt it take like a second for that whole dance to happen?‚Äù It would, yes.\n\nYou might also be thinking ‚Äúpulling that image from a remote repository was probably soul-crushingly slow, right?‚Äù Also true! You don‚Äôt want to do that any more times than you need to.\n\nWe made machines really cheap to create and keep stopped. In fact, right now, you pay for image storage; that‚Äôs it. What we want you do is: create machines ahead of time and start them when you need them.\n\n\n## The fast part\n\nYou should create machines just before you need them. Slightly earlier than just-in-time. All the stuff I just told you about is necessary to get to this point, but the protein is here: we designed Fly Machines for fast starts.\n\nWhen you‚Äôre ready, you start a machine in S√£o Paulo with a request to the nearest API server. This time, though, there‚Äôs no need to wait on our database in Virginia. The central accounting is done and the API server knows exactly which host it needs to talk to. And the OCI image for the VM‚Äôs filesystem is ready to go.\n\nHere‚Äôs what the start machine endpoint does:\n\nNow¬†the start is fast. How fast?\n\nWhen you run¬†fly machine start e21781960b2896, the API server knows that e21781960b2896 is owned by a host in S√£o Paulo. It then sends a message directly to that host saying ‚Äústart it up‚Äù. This message travels as fast as physics allows.\n\nThe host receives the start message‚Ä¶and starts the machine. It already has the image stored locally, so it doesn‚Äôt need to wait on an image pull.\n\nIf you‚Äôre in Los Angeles and start your machine in S√£o Paulo, the ‚Äústart‚Äù message gets where it needs to go in ~150ms. But if you‚Äôre in Los Angeles and start a machine in Los Angeles, the ‚Äústart‚Äù message might arrive in ~10ms.\n\nThe lesson here is ‚Äústart machines close to your users‚Äù; the operation is very fast. Here‚Äôs something cool about this, though: You don‚Äôt necessarily start the machine from where you are; an app can do it for you. In fact, this is kind of the point. Your application logic should be close to your users‚Äô machines.\n\nOr, you can forego the app and let fly-proxy boot machines when HTTP requests arrive. It can do all this for you.\n\nI should clarify: our infrastructure is fast to run start operations. Your application boot time is something you should optimize. We can‚Äôt help with that (yet!)\n\n\n## Stopping and scaling to zero\n\nStop commands are fast too. You may not want to issue stop commands, though. If your machine should stop when it‚Äôs idle, there‚Äôs a better way.\n\nFly.io users have been requesting ‚Äúscale to zero‚Äù since January 1st, 1970 at 00:00:00 UTC. Scaling up is pretty easy; it‚Äôs usually safe to boot a new VM and add it to a load balancer. Scaling down is harder‚Äîstop a VM at the wrong time and shit breaks.\n\nSo here‚Äôs how we modeled this: when you use Fly.io machines to run apps that need to scale down, make your process exit when it‚Äôs idle. That‚Äôs it. You‚Äôve exited the container, effectively stopping the machine, but it‚Äôs intact to pick up a future start request from a clean slate.\n\nThis works because your in-machine process has a strongly-consistent view of local activity and can confidently detect ‚Äúidle‚Äù.\n\nIf you‚Äôve got a Fly.io account, you can play with the Fly Machines API right now‚Äîeven if you‚Äôre not ready to [build your own FaaS](https://fly.io/docs/app-guides/functions-with-machines).\n\n\n## How Fly Machines will frustrate you (the emotional cost of simplicity)\n\nOne thing you may have noticed about our design: machines are pinned to specific hardware in our datacenters. This is a tradeoff that buys simplicity at the risk of your patience.\n\nPinning machines to specific hardware means that if the PSU on that host goes pop, your machine won‚Äôt work (kind of; we run redundant PSUs). Capacity issues will create more surprising failures. If you create a biggish 64GB RAM machine and leave it stopped, we might be out of capacity on that specific host when you attempt to start it back up.\n\nWe will mostly shield you from capacity issues, but you should definitely be prepared for the eventuality that your first-choice hardware is indisposed. Which really just means: plan to have two machines for redundancy.\n\nThe good news is that our API is pretty fast. Creating a machine is relatively slow, but you can do it in a pinch. If a machine fails to start, you can usually get another one running in a few seconds.\n\nThe best way to use machines is to think of a list of operations in priority order. If you‚Äôre trying to run some user code, come up with a list like this:\n\nThis cycle will account for all the predictable failures and should get you a machine any time you want one.\n\n\n## Pricing (the monetary cost of simplicity)\n\nRunning machines costs the same as running normal VM instances. The same goes for bandwidth, RAM, and persistent disks.\n\nStopped machines, though, are something we could use your feedback on. There‚Äôs a cost to keeping these things around. Right now, we just charge you for storage when a machine isn‚Äôt running. Like $0.15/mo for a 1GB Docker image.\n\nQuestions? Comments? Pricing ideas? Vitriol? Comment in our forum thread."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/logbook-2022-05-13/",
    "content": "Here‚Äôs absolutely everything people have made or changed here at Fly.io in the past week or so. Nothing else. Pretty sure. Spin up an app and see for yourself how it‚Äôs going!"
  },
  {
    "title": "Litestream SQLite, Postgres, CockroachDB, or any other database",
    "url": "https://fly.io/blog/all-in-on-sqlite-litestream/",
    "content": "I‚Äôm Ben Johnson. I wrote BoltDB, an embedded database that is the backend for systems like etcd. Now I work at Fly.io, on Litestream. Litestream is an open-source project that makes SQLite tenable for full-stack applications through the power of ‚ú®replication‚ú®. If you can set up a SQLite database, you can get Litestream working in less than 10 minutes.\n\nThe conventional wisdom of full-stack applications is the n-tier architecture, which is now so common that it‚Äôs easy to forget it even has a name. It‚Äôs what you‚Äôre doing when you run an ‚Äúapplication server‚Äù like Rails, Django, or Remix alongside a ‚Äúdatabase server‚Äù like Postgres. According to the conventional wisdom, SQLite has a place in this architecture: as a place to run unit tests.\n\nThe conventional wisdom could use some updating. I think that for many applications ‚Äì¬†production applications, with large numbers of users and high availability requirements ‚Äì¬†SQLite has a better place, in the center of the stack, as the core of your data and persistence layer.\n\nIt‚Äôs a big claim. It may not hold for your application. But you should consider it, and I‚Äôm here to tell you why.\n\n\n## A Brief History Of Application Databases\n\n50 years is not a long time. In that time, we‚Äôve seen a staggering amount of change in how our software manages data.\n\nIn the beginning of our story, back in the ‚Äò70s, there were Codd‚Äôs rules, defining what we now call ‚Äúrelational databases‚Äù, also known today as ‚Äúdatabases‚Äù. You know them, even if you don‚Äôt: all data lives in tables; tables have columns, and rows are addressable with keys; C.R.U.D.; schemas; a textual language to convey these concepts. The language, of course, is SQL, which prompted a Cambrian explosion of SQL databases, from Oracle to DB2 to Postgres to MySQL, throughout the '80s and '90s.\n\nIt hasn‚Äôt all been good. The 2000s got us XML databases. But our industry atoned by building some great columnar databases during the same time. By the 2010s, we saw dozens of large-scale, open-source distributed database projects come to market. Now anyone can spin up a cluster and query terabytes of data.\n\nAs databases evolved, so too did the strategies we use to plug them in to our applications. Almost since Codd, we‚Äôve divided those apps into tiers. First came the database tier. Later, with memcached and Redis, we got the caching tier. We‚Äôve got background job tiers and we‚Äôve got routing tiers and distribution tiers. The tutorials pretend that there are 3 tiers, but we all know it‚Äôs called ‚Äún-tier‚Äù because nobody can predict how many tiers we‚Äôre going to end up with.\n\nYou know where we‚Äôre going with this. Our scientists were so preoccupied with whether or not they could, and so on.\n\nSee, over these same five decades, we‚Äôve also seen CPUs, memory, \u0026 disks become hundreds of times faster and cheaper. A term that practically defines database innovation in the 2010s is ‚Äúbig data‚Äù. But hardware improvements have made that concept slippery in the 2020s. Managing a 1 GB database in 1996? A big deal. In 2022? Run it on your laptop, or a t3.micro.\n\nWhen we think about new database architectures, we‚Äôre hypnotized by scaling limits. If it can‚Äôt handle petabytes, or at least terabytes, it‚Äôs not in the conversation. But most applications will never see a terabyte of data, even if they‚Äôre successful. We‚Äôre using jackhammers to drive finish nails.\n\n\n## The Sweet Release of SQLite\n\nThere‚Äôs a database that bucks a lot of these trends. It‚Äôs one of the most popular SQL databases in the world, so standardized it‚Äôs an official archival format of the Library of Congress, it‚Äôs renowned for its reliability and its unfathomably encompassing test suite, and its performance is so good that citing its metrics on a message board invariably starts an argument about whether it should be disqualified. I probably don‚Äôt have to name it for you, but, for the one person in the back with their hand raised, I‚Äôm talking about SQLite.\n\nSQLite is an embedded database. It doesn‚Äôt live in a conventional architectural tier; it‚Äôs just a library, linked into your application server‚Äôs process. It‚Äôs the standard bearer of the ‚Äúsingle process application‚Äù: the server that runs on its own, without relying on nine other sidecar servers to function.\n\nI got interested in these kinds of applications because I build databases. I wrote BoltDB, which is a popular embedded K/V store in the Go ecosystem. BoltDB is reliable and, as you‚Äôd expect from an in-process database, it performs like a nitro-burning funny car. But BoltDB has limitations: its schema is defined in Go code, and so it‚Äôs hard to migrate databases. You have to build your own tooling for it; there isn‚Äôt even a REPL.\n\nIf you‚Äôre careful, using this kind of database can get you a lot of performance. But for general-purpose use, you don‚Äôt want to run your database off the open headers like a funny car. I thought about the kind of work I‚Äôd have to do to make BoltDB viable for more applications, and the conclusion I quickly reached was: that‚Äôs what SQLite is for.\n\nSQLite, as you are no doubt already typing into the message board comment, is not without its own limitations. The biggest of them is that a single-process application has a single point of failure: if you lose the server, you‚Äôve lost the database. That‚Äôs not a flaw in SQLite; it‚Äôs just inherent to the design.\n\n\n## Enter Litestream\n\nThere are two big reasons everyone doesn‚Äôt default to SQLite. The first is resilience to storage failures, and the second is concurrency at scale. Litestream has something to say about both concerns.\n\nHow Litestream works is that it takes control of SQLite‚Äôs WAL-mode journaling. In WAL mode, write operations append to a log file stored alongside SQLite‚Äôs main database file. Readers check both the WAL file and the main database to satisfy queries. Normally, SQLite automatically checkpoints pages from the WAL back to the main database. Litestream steps in the middle of this: we open an indefinite read transaction that prevents automatic checkpoints. We then capture WAL updates ourselves, replicate them, and trigger the checkpointing ourselves.\n\nThe most important thing you should understand about Litestream is that it‚Äôs just SQLite. Your application uses standard SQLite, with whatever your standard SQLite libraries are. We‚Äôre not parsing your queries or proxying your transactions, or even adding a new library dependency. We‚Äôre just taking advantage of the journaling and concurrency features SQLite already has, in a tool that runs alongside your application. For the most part, your code can be oblivious to Litestream‚Äôs existence.\n\nOr, think of it this way: you can build a Remix application backed by Litestream-replicated SQLite, and, while it‚Äôs running, crack open the database using the standard sqlite3 REPL and make some changes. It‚Äôll just work.\n\nYou can read more about how this works here.\n\nIt sounds complicated, but it‚Äôs incredibly simple in practice, and if you play with it you‚Äôll see that it ‚Äújust works‚Äù. You run the Litestream binary on the server your database lives on in ‚Äúreplicate‚Äù mode:\n\nAnd then you can ‚Äúrestore‚Äù it to another location:\n\nNow commit a change to your database; if you restore again then you‚Äôll see the change on your new copy.\n\nWe‚Äôll replicate almost anywhere: to S3, or Minio; to Azure, or Backblaze B2, or Digital Ocean or Google Cloud, or an SFTP server.\n\nThe ordinary way people use Litestream today is to replicate their SQLite database to S3 (it‚Äôs remarkably cheap for most SQLite databases to live-replicate to S3). That, by itself, is a huge operational win: your database is as resilient as you ask it to be, and easily moved, migrated, or mucked with.\n\nBut you can do more than that with Litestream. The upcoming release of Litestream will let you live-replicate SQLite directly between databases, which means you can set up a write-leader database with distributed read replicas. Read replicas can catch writes and redirect them to the leader; most applications are read-heavy, and this setup gives those applications a globally scalable database.\n\nThey all work on Fly.io; we do built-in persistent storage and private networking for painless clustering, so it‚Äôs easy to try new stuff out.\n\n\n## You Should Take This Option More Seriously\n\nOne of my first jobs in tech in the early 2000s was as an Oracle Database Administrator (DBA) for an Oracle9i database. I remember spending hours poring over books and documentation to learn the ins and outs of the Oracle database. And there were a lot. The administration guide was almost a thousand pages‚Äîand that was just one of over a hundred documentation guides.\n\nLearning what knobs to turn to optimize queries or to improve writes could make a big difference back then. We had disk drives that could only read tens of megabytes per second so utilizing a better index could change a 5-minute query into a 30 second query.\n\nBut database optimization has become less important for typical applications. If you have a 1 GB database, an NVMe disk can slurp the whole thing into memory in under a second. As much as I love tuning SQL queries, it‚Äôs becoming a dying art for most application developers. Even poorly tuned queries can execute in under a second for ordinary databases.\n\nModern Postgres is a miracle. I‚Äôve learned a ton by reading its code over the years. It includes a slew of features like a genetic query optimizer, row-level security policies, and a half dozen different types of indexes. If you need those features, you need them. But most of you probably don‚Äôt.\n\nAnd if you don‚Äôt need the Postgres features, they‚Äôre a liability. For example, even if you don‚Äôt use multiple user accounts, you‚Äôll still need to configure and debug host-based authentication. You have to firewall off your Postgres server. And more features mean more documentation, which makes it difficult to understand the software you‚Äôre running. The documentation for Postgres 14 is nearly 3,000 pages.\n\nSQLite has a subset of the Postgres feature set. But that subset is 99.9% of what I typically need. Great SQL support, windowing, CTEs, full-text search, JSON. And when it lacks a feature, the data is already next to my application. So there‚Äôs little overhead to pull it in and process it in my code.\n\nMeanwhile, the complicated problems I really need to solve aren‚Äôt really addressed by core database functions. Instead, I want to optimize for just two things: latency \u0026 developer experience.\n\nSo one reason to take SQLite seriously is that it‚Äôs operationally much simpler. You spend your time writing application code, not designing intricate database tiers. But then there‚Äôs the other problem.\n\n\n## The light is too damn slow\n\nWe‚Äôre beginning to hit theoretical limits. In a vacuum, light travels about 186 miles in 1 millisecond. That‚Äôs the distance from Philadelphia to New York City and back. Add in layers of network switches, firewalls, and application protocols and the latency increases further.\n\nThe per-query latency overhead for a Postgres query within a single AWS region can be up to a millisecond. That‚Äôs not Postgres being slow‚Äîit‚Äôs you hitting the limits of how fast data can travel. Now, handle an HTTP request in a modern application. A dozen database queries and you‚Äôve burned over 10ms before business logic or rendering.\n\nThere‚Äôs a magic number for application latency: responses in 100ms or less feel instantaneous. Snappy applications make happy users. 100ms seems like a lot, but it‚Äôs easy to carelessly chew it up. The 100ms threshold is so important that people pre-render their pages and post them on CDNs just to reduce latency.\n\nWe‚Äôd rather just move our data close to our application. How much closer? Really close.\n\nSQLite isn‚Äôt just on the same machine as your application, but actually built into your application process. When you put your data right next to your application, you can see per-query latency drop to 10-20 microseconds. That‚Äôs micro, with a Œº. A 50-100x improvement over an intra-region Postgres query.\n\nBut wait, there‚Äôs more. We‚Äôve effectively eliminated per-query latency. Our application is fast, but it‚Äôs also simpler. We can break up larger queries into many smaller, more manageable queries, and spend the time we‚Äôve been using to hunt down corner-casey N+1 patterns building new features.\n\nMinimizing latency isn‚Äôt just for production either. Running integration tests with a traditional client/server database easily grows to take minutes locally and the pain continues once you push to CI. Reducing the feedback loop from code change to test completion doesn‚Äôt just save time but also preserves our focus while developing. A one-line change to SQLite will let you run it in-memory so you can run integration tests in seconds or less.\n\n\n## Small, Fast, Reliable, Globally Distributed: Choose Any Four\n\nLitestream is distributed and replicated and, most importantly, still easy to get your head around. Seriously, go try it. There‚Äôs just not much to know.\n\nMy claim is this: by building reliable, easy-to-use replication for SQLite, we make it attractive for all kinds of full-stack applications to run entirely on SQLite. It was reasonable to overlook this option 170 years ago, when the Rails Blog Tutorial was first written. But SQLite today can keep up with the write load of most applications, and replicas can scale reads out to as many instances as you choose to load-balance across.\n\nLitestream has limitations. I built it for single-node applications, so it won‚Äôt work well on ephemeral, serverless platforms or when using rolling deployments. It needs to restore all changes sequentially which can make database restores take minutes to complete. We‚Äôre rolling out live replication, but the separate-process model restricts us to course-grained control over replication guarantees.\n\nWe can do better. For the past year, what I‚Äôve been doing is nailing down the core of Litestream and keeping a focus on correctness. I‚Äôm happy with where we‚Äôve landed. It started as a simple, streaming back up tool but it‚Äôs slowly evolving into a reliable, distributed database. Now it‚Äôs time to make it faster and more seamless, which is my whole job at Fly.io. There are improvements coming to Litestream ‚Äî improvements that aren‚Äôt at all tied to Fly.io! ‚Äî that I‚Äôm psyched to share.\n\nLitestream has a new home at Fly.io, but it is and always will be an open-source project. My plan for the next several years is to keep making it more useful, no matter where your application runs, and see just how far we can take the SQLite model of how databases can work."
  },
  {
    "title": "Shipping so fast it‚Äôs bananas!",
    "url": "https://fly.io/blog/logbook-2022-05-05/",
    "content": "Here‚Äôs some of what we‚Äôve been doing behind the scenes at Fly.io, to run your apps ever faster and better in their own VMs close to your users worldwide. If you haven‚Äôt already, deploy an app (it‚Äôll only take a few minutes) and get to know us better; you‚Äôll be more invested in the plot.\n\nFeatures and fixes are flying like dodgeballs in a school gym, and the Fly.io Changelog Enforcer could probably have done a better job patrolling‚Äîbut let‚Äôs have a look at our haul of updates since our first Logbook post.\n\nThere‚Äôs a fair amount of protein in this week‚Äôs mix. Let‚Äôs kick it off with improved remote builders you can activate for your organization!\n\nWe‚Äôre working hard to make Fly.io the place to run all the stacks."
  },
  {
    "title": "Fly.io is not just for Elixir/Phoenix/LiveView!",
    "url": "https://fly.io/blog/accessibility-clearing-the-fog/",
    "content": "Here‚Äôs Nolan Darilek on making sure LiveBeats‚Äîor any web app‚Äîhas a solid, accessible foundation to build on. If your app is already solid, deploy it in minutes on Fly.io‚Äôs global server network.\n\nHey, everyone. Last time we talked a bit about what accessibility is, why it‚Äôs important, and how you can incorporate it into your process. Today, using the time-travel superpowers of Git, I‚Äôll take you along as I start making LiveBeats more accessible for screen reader users.\n\nLiveBeats is a reference Phoenix LiveView app with real-time social features. We‚Äôd be reckless not to make sure all the parts are in good working order before real-time updates start moving them around on us. Let‚Äôs set our time machines to LiveBeats commit fad3706‚Äîor mid-November. Thanksgiving is on the horizon, winter is coming, and I‚Äôm starting to dig into this little app called LiveBeats (cue dreamlike harp glissandos)‚Ä¶\n\n\n## Low-hanging fruit\n\nSometimes, setting out to make an app accessible feels like surveying fog-shrouded terrain. Is that supposed to be a button? What‚Äôs all that clutter over there? Fortunately, we can clear away a lot of the murk with some easy early wins.\n\nLabels are a great way to give some definition to the landscape. You‚Äôve got a few tools to help with this, each of which has its own use cases:\n\nSo, back to fad3706. We use aria-label here to add accessible labels to controls; for example, this button that skips to the previous track in the playlist:\n\nBecause this is all in a HEEx template, our aria-label can be conditional, e.g. aria-label={if @playing do \"Pause\" else \"Play\" end}\n\nIdeally, you‚Äôd just add text as a child of the button, particularly since users with cognitive disabilities may struggle with what a given icon means. If you‚Äôre using an image for your button, add an alt attribute. Where neither is the case, aria-label is the ticket.\n\nLabeling meaningful elements is only part of the story, however. Hiding irrelevant images can be just as important. If my screen reader presents a thing, then it should be relevant to me. Decorations and placeholders usually aren‚Äôt, and should be hidden, either by applying aria-hidden=\"true\" to the element, or by adding a blank alt attribute to images.\n\nYou can see an example of hiding icons in f7db67f:\n\nHiding a decorative icon saves my screen reader from reading what appears to be an empty element. It‚Äôs a small thing, but half a dozen small things add up.\n\n\n## Role with it, but not too far\n\nWe‚Äôve labeled some things and hidden others. The fog is burning away, and we have a slightly clearer view of the land around us. It‚Äôs now time to fill in the details.\n\nRoles are powerful tools that make one thing appear to be another. Say you have a \u003cdiv/\u003e tag that needs to work as a button. You can make it seem like a button like so:\n\nUnfortunately, the above is the equivalent of slapping on a fake mustache and glasses. To my screen reader, it now looks like a button. But the role alone doesn‚Äôt make it act like a button; it doesn‚Äôt focus when I tab, and doesn‚Äôt click when I press Space or Enter.\n\nIt‚Äôs better to use a \u003cbutton/\u003e and get this button behavior built in. But if you can‚Äôt, or if you‚Äôre building a widget like a dropdown menu or tree, roles are crucial.\n\nRoles are great for signposting semantic regions on pages. Here are the most important:\n\nAll of the above roles have semantic HTML equivalents. This isn‚Äôt universally true‚Äîthere isn‚Äôt a semantic HTML equivalent of a tree control, for instance‚Äîbut you should prefer HTML where possible.\n\nThere‚Äôs a lot to unpack there. But as an example, commit 5cf58b2 combines some of what we‚Äôve discussed to achieve more intuitive navigation within LiveBeats.\n\nWe use the \u003cmain\u003e element to surround the page content that changes when new routes are visited. Using role=\"main\" would serve the same purpose, though less elegantly.\n\nThen, we use another technique to associate names with areas of the page:\n\nThis last snippet does a couple of things. Adding a new region landmark to the page with role=\"region\" gives us easier navigation into and out of it, via hotkey. In NVDA, I can jump between this and other interesting regions of the page by pressing d. Then, the aria-label attribute ties the label ‚ÄúPlayer‚Äù with that region, such that navigating into or out of the player explicitly announces the focus entering or leaving the player.\n\nIf you find yourself writing documentation with phrases like ‚ÄúIn the player, click Pause‚Äù or ‚ÄúFind this in the messages section of the dashboard,‚Äù named regions help make those instructions more relevant to screen reader users. Styling may make the player region visually obvious, but the region role and ‚ÄúPlayer‚Äù label makes it very apparent when the caret enters the audio player.\n\nRoles are powerful in large part because they‚Äôre promises. If you promise your user that a thing is a button, then it needs a tabindex for keyboard focus, as well as handlers to activate it when Enter or Space is pressed. If you‚Äôre curious about what these promises are, the WAI-ARIA Authoring Practices document is an exhaustive source of all commonly expected keyboard behaviors for a bunch of widget types. Want to know how a list box should behave when arrowing down past the last element? This resource has you covered.\n\nThat said, it is very possible to overuse roles in your application. Here are two examples of role misuse I often find in the wild.\n\n\n## Menus aren‚Äôt what you think\n\nrole=\"menu\" is not intended for every list of links in your app that might possibly be a menu if you tilt your head and the sunlight lands just so. These are either application menus like you‚Äôd find in a menu bar, or standalone dropdown menus that capture keyboard focus and expand when you click on them. Misusing role=\"menu\" won‚Äôt necessarily make a given control unusable, but it does cause confusion by presenting an element as something it isn‚Äôt.\n\nAny app that benefits from low-latency connections to users worldwide will love Fly.io.\n\n\n## This one weird trick makes your entire app inaccessible in 30 seconds!\n\nThis one gets its own section because it‚Äôs that bad. If you want your app to be so inaccessible that most screen reader users turn away in the first few seconds, slap role=\"application\" on one of the top-level elements. Explaining just why this is takes a bit of effort, so please bear with me.\n\nBroadly speaking, screen reader users browse the web in one of two modes. Caret browsing mode presents pages as if they‚Äôre documents. We can arrow through their contents line by line or character by character, select and copy text, etc. You can experience a limited version of this by pressing F7 in most browsers, though screen readers enable this mode by default. They also add conveniences like jumping between headings with h, buttons with b, landmarks with d, etc.\n\nFocus mode, sometimes called ‚Äúforms mode,‚Äù because it enables automatically when form fields are highlighted, passes keys directly through to the application. This is generally what you want when typing into forms, or when using keyboard commands that you don‚Äôt want filtered out by caret browsing. Incidentally, focus mode is closest to how native applications behave; you don‚Äôt normally read application screens as if they were documents, or jump between controls by element type.\n\nThat, more or less, is what role=\"application\" enforces. It enables focus mode by default, meaning your screen reader users can‚Äôt explore via caret browsing and its associated commands. It also changes the way the site presents itself to screen reader users, such that it appears to be a native app, and not a web document with a built-in interaction model. It‚Äôs a promise that you‚Äôll supply it all; you‚Äôve gone through the extraordinary effort to ensure all your controls are focusable, they all have sensible keyboard behaviors, and that your users won‚Äôt be struggling to read text that changes.\n\nYou might feel you have to use role=\"application\" just because, well, you‚Äôre making an application. But this is often not the right choice. If you‚Äôve ever been annoyed by an Electron app‚Äôs non-native behavior, multiply that by about 11 and you‚Äôre in the ballpark of how frustrating role=\"application\" can be when it‚Äôs not backed up by thorough and consistent handling of every interaction. While I‚Äôve got this podium: Slack, you‚Äôre one of the biggest perpetrators of this, and need to cut it out. Most of my usability issues with Slack spring from its use of role=\"application\" everywhere, with haphazard and nonstandard workarounds in place to patch in what HTML provides for free.\n\n\n## Closing thoughts\n\nWhile this post has been light on the real-time aspects of LiveBeats, harvesting this low-hanging fruit is an important step to making any web app accessible. We certainly don‚Äôt want it in the way next time, when we‚Äôll start going live, exploring the challenges and methods to accessibly presenting changes as they roll in over the wire. Meanwhile, if you have questions or topics you‚Äôd like covered, drop a line here and I‚Äôll try to work them in. Thanks for reading!"
  },
  {
    "title": "Shiny new things",
    "url": "https://fly.io/blog/logbook-2022-04-20/",
    "content": "This post is a changelog, and it‚Äôs also about the challenges of generating a changelog at Fly.io‚Äîwhere we run your apps in VMs on our hardware around the world. It only takes a few minutes to try out the latest and greatest Fly.io!\n\nHere‚Äôs a changelog covering our most recent activity (i.e. since we started compiling updates, a bit under two weeks ago):\n\n\n## How Do We Make a Changelog Happen at Fly.io?\n\nWe want to tell you about every interesting thing we‚Äôre doing, from adding a new option to a flyctl command, to speeding up Docker image pulls, to generating more enlightening error messages. How do we collect updates like this from a distributed company that‚Äôs grown from 7 people to 26 in the past 8 months? Turns out, it‚Äôs not easy. Here‚Äôs why it‚Äôs hard for us.\n\nThe job is basically:\n\nIn the spirit of exploration, we tried having one person compile a changelog by looking at all the status updates and git commits over the span of a few days. This helped crystallize some challenges for us; specifically:\n\nSince it‚Äôs not practical to delegate discovery and translation of everyone‚Äôs work to a team of changelog artisans, we‚Äôd better all just write good updates for our own work.\n\nThis means we all have to get good at the following:\n\nOn that second point: we do a tremendous amount of work to improve apps that already work on Fly.io, and most of this work is practically invisible.\n\nYou may have noticed that in today‚Äôs list we have new deployment support for two frameworks. We are definitely pedal-to-the-metal on making it easy to deploy all kinds of apps on Fly.io, and it‚Äôs easy to write an update for new features. But there‚Äôs also furious activity on improving the platform for all the apps running on us, and this is important and interesting, and we should showcase it, even if it‚Äôs a harder blurb to write.\n\nTrying RedwoodJS? Check out how easy we made it to deploy on Fly.io.\n\nIf everyone‚Äôs writing their own updates, this brings up its own minor issue: the mechanics of collecting all the changelog entries in one place. Fortunately, we‚Äôre nerds (or at least Kurt is), so we‚Äôve automated this part. When we have an update to emit, we tell it to a Slack bot that passes it to an app (that Kurt wrote), which in turn collates our updates into feeds ripe for the copypasting. It doesn‚Äôt solve the hardest problems, but it‚Äôs pretty damn cool.\n\nWe‚Äôll be making a special effort to cultivate good changelog habits so we can bring you all the interesting things."
  },
  {
    "title": "You don‚Äôt really have to think about any of this.",
    "url": "https://fly.io/blog/a-foolish-consistency/",
    "content": "Fly.io runs applications by transmogrifying Docker containers into Firecracker micro-VMs running on our hardware around the world, connected with WireGuard to a global Anycast network. Yours could be one of them! Check us out: from a working container, your app can be running worldwide in minutes.\n\nWe set the scene, as usual, with sandwiches. Dig if you will the picture: a global Sandwich Bracket application, ascertaining once and for all the greatest sandwich on the planet.\n\nFly.io wants our app, sandwich-bracket, deployed close to users around the world. Chicago users vote for Italian beefs on an instance of sandwich-bracket in Chicago; people who love b√°nh m√¨ are probably voting on a Sydney instance, egg salad on white bread, Tokyo.\n\nTo run a platform that makes this kind of thing work, we need a way to route incoming traffic to instances. The way we do that is with service discovery: a distributed catalog of all services running at Fly.io. The Fly.io service catalog lives in Consul. The catalog expands consciousness. The catalog is vital to space travel.\n\nThe catalog occupies more of our mental energy than just about anything at Fly.io. We‚Äôve sunk a huge amount of energy into keeping S√£o Paulo, Sydney, Singapore, and points between consistent in their view of what‚Äôs running on Fly.io, scaling a single global Consul cluster. What we think we‚Äôve learned is that keeping S√£o Paulo and Sydney on exactly the same page about what‚Äôs running in Mumbai is a mug‚Äôs game, and we shouldn‚Äôt be playing it.\n\nAnd so, to begin, it is my privilege to inflict Consul on you.\n\n\n## What the hell is Consul?\n\na ‚Äúservice‚Äù is whatever you say it is, but the classic example would be an HTTP micro-service server\n\nConsul is a distributed database that attempts to be a source of truth for which services are currently running. It‚Äôs one of several ‚Äúservice coordination‚Äù or ‚Äúservice discovery‚Äù databases; the other popular ones are Etcd, which once powered Kubernetes, and Zookeeper, the original service coordinator, important in the Java/Hadoop ecosystem.\n\nThe challenge of these databases, the reason they‚Äôre not just trivial MySQL instances, is that you can‚Äôt just have one of them. Once you start relying on service discovery, it can‚Äôt go down, or your applications all break. So you end up with a cluster of databases, which have to agree with each other, even as services come and go. These systems all expend a lot of effort, and make a lot of compromises, in order to cough up consistent answers on flappy networks with fallible servers where individual components can fail.\n\nHow Consul works is that you have a cluster of ‚ÄúConsul Servers‚Äù ‚Äî maybe 3, 5, or 7 ‚Äî and then all the rest of your machines run a ‚ÄúConsul Agent‚Äù that talks to the Servers. The Servers execute the Raft consensus protocol, maintaining a log of updates that form the basis for the database. Agents, in turn, inform the servers about events, such as an instance of a service terminating. An Agent can talk to any Server, but the Servers elect a leader, and updates are routed to the leader, which coordinates the Raft update to the log.\n\nWith me so far? Neither am I. But the specifics don‚Äôt matter much, as long as you understand that every machine in our fleet runs a lightweight Consul Agent that relays events to Consul Servers, which we only have a few of, locked in an unending and arcane ritual of consensus-tracking, producing: a map of every service running on Fly.io, and exactly where it‚Äôs running. Plus some other stuff.\n\nLet‚Äôs see it in action.\n\n\n## Consul at Fly.io\n\nA Consul ‚Äúservice‚Äù at Fly.io, in the main, is an exposed port on an app a user deployed here. An ‚Äúinstance‚Äù of a service is a VM exposing that port. A ‚Äúnode‚Äù is one of Fly.io‚Äôs own servers.\n\nWe run a couple different kinds of servers, among them lightweight ‚Äúedges‚Äù handling Internet traffic, and chonky ‚Äúworkers‚Äù running customer VMs. Both run fly-proxy, our Rust+Tokio+Hyper proxy server.\n\nEvery app running on Fly.io gets a unique, routable IPv4 address.\n\nThat‚Äôs how our CDN works: we advertise these addresses, the same addresses, from dozens of data centers around the world, with BGP4 (this is ‚ÄúAnycast‚Äù). Backbone routing takes you to the closest one. Say sandwich-bracket is currently deployed in Frankfurt and Sydney. Anycast means the votes for doner land on a Fly.io edge in Frankfurt, right next to a worker running sandwich-bracket; A b√°nh m√¨ vote lands on a Sydney edge, and is routed to a Sydney worker. We‚Äôre not deployed in Tokyo, so a vote for egg salad hits a Tokyo edge, and gets routed‚Ä¶ out of Japan.\n\nThe problem facing fly-proxy is, ‚Äúwhere do I send this egg salad vote‚Äù. ‚ÄúThe garbage‚Äù being, unfortunately, not a valid answer, fly-proxy needs to know which workers your sandwich-bracket app is running on, and then it needs to pick one to route to.\n\nHere‚Äôs the data we‚Äôre working with:\n\nEnter Consul. When you first created sandwich-bracket with our API, we:\n\nconsul-templaterb is Pierre Souchay‚Äôs Ruby rewrite of consul-template, which comes with Consul. Track Consul, write a template, and then run a program that eats the file.\n\nThe simplest way to integrate all that information, and what we did until a couple months ago, is: we‚Äôd run consul-templaterb and ask it to track every service in Consul and sync a JSON file with the data; when the file is updated, fly-proxy gets a signal and re-reads it into memory.\n\nTheoretically, Consul can tell us how ‚Äúclose‚Äù each of those services are ‚Äì Consul puts a bunch of work into network telemetry ‚Äî but we do that bit ourselves, and so there‚Äôs another JSON file that fly-proxy watches to track network distance to every server in our fleet.\n\nConsul experts: avert your eyes\n\nConsul doesn‚Äôt give us the load (in concurrent requests) on all the services. For a long time, we abused Consul for this, too: we tracked load in Consul KV. Never do this! Today, we use a messaging system to gossip load across our fleet.\n\nSpecifically, we use NATS, a simple ‚Äúbrokered‚Äù asynchronous messaging system, the pretentious way to say ‚Äúalmost exactly like IRC, but for programs instead of people‚Äù.\n\nUnlike Consul, NATS is neither consistent nor reliable. That‚Äôs what we like about it. We can get our heads around it. That‚Äôs a big deal: it‚Äôs easy to get billed for a lot of complexity by systems that solve problems 90% similar to yours. It seems like a win, but that 10% is murder. So our service discovery will likely never involve an event-streaming platform like Kafka.\n\nPutting it all together, you have a sense of how our control plane works. Say the World Sandwich Authority declares doner is no longer a sandwich, and Japanese biochemists invent an even fluffier white bread. Traffic plummets in Frankfurt and skyrockets in Tokyo. We move our Frankfurt instance to Tokyo (flyctl regions set syd nrt). This kills the Frankfurt instance, and Frankfurt‚Äôs Consul Agent deregisters it. JSON files update across the fleet. The Tokyo instance comes up and gets registered; more JSON churn.\n\nWe use Consul for other stuff!\n\n\n## It Burns\n\nIt looks like textbook Consul. But it‚Äôs not, really.\n\nConsul is designed to make it easy to manage a single engineering team‚Äôs applications. We‚Äôre managing deployments for thousands of teams. It‚Äôs led us to a somewhat dysfunctional relationship.\n\nTo start with, we have a single Consul namespace, and a single global Consul cluster. This seems nuts. You can federate Consul. But every Fly.io data center needs details for every app running on the planet! Federating costs us the global KV store. We can engineer around that, but then we might as well not use Consul at all.\n\n‚ÄúTracking‚Äù data in Consul generally involves long-polling an HTTP endpoint. Each HTTP request bears an index, which in turn tracks the Raft log; as a user, you don‚Äôt care about what this value means, only that if it changed, there‚Äôs new data. This is fine.\n\nConsul‚Äôs API was also not designed with our needs in mind (nor should it have been). It‚Äôs got a reasonable API for tracking some things, but not quite the things we need. So, for instance, there‚Äôs an HTTP endpoint we can long-poll to track the catalog of services. But:\n\nThat second problem is kind of a nightmare. We have tens of thousands of distinct services. We need per-instance metadata for every instance of those services.\n\nConsul can give us that metadata in two ways: by asking it about individual services, one-by-one, or by asking for service catalogs from each of our servers. We can‚Äôt long poll tens of thousands of endpoints. So, the way we get instance metadata from Consul is to ask it about servers, not services. We long-poll an API endpoint for each individual server. There‚Äôs no one endpoint that we can long-poll for all the nodes.\n\nYou might at this point ask why we‚Äôre storing this kind of stuff in instance metadata at all. That‚Äôs a good question with a complicated answer. Some of it has to do with Nomad, the orchestration service we use (and are gradually moving away from) to actually run VM jobs. Information about, say, what ports an app listens on percolates from a Nomad task description into its Consul service registration; that‚Äôs just how Nomad works.\n\nYou also can‚Äôt just factor the information out into, say, a Consul KV tree, because apps have versions, and different versions of apps listen on different ports, and fly-proxy needs to track them.\n\nThis stuff is all solvable! But, like, are you going to solve it by using Consul more carefully, or are you going to solve it by using Consul less?\n\nWhich is how it came to be that we found ourselves driving over 10 (t-e-n) gb/sec of Consul traffic across our fleet. Meanwhile, and I haven‚Äôt done the math, but it‚Äôs possible that the underlying data, carefully formatted and compressed, might fit on a dialup modem.\n\nThis, it turns out, was Not Entirely Our Fault. Long-suffering SRE Will Jordan, his brain shattered by ten gigabits of sustained Consul traffic, dove into the Consul codebase and discovered a bug: updates anywhere in Consul un-blocked every long-polling query. We had tens of thousands, N^2 (don‚Äôt email me!) in the number of nodes, all of which return a full refresh of the data they‚Äôre tracking when they unblock. Anyways, Will wrote a couple dozen lines of Go, and:\n\n\n## Extricating Ourselves\n\nSo, consul-templaterbis easy, but rough at the scale we work at. It runs Ruby code to to track updates that happen multiple times per second, each time writing giant JSON blobs to disk.\n\nOne of Consul‚Äôs big-ticket features is a built-in DNS server. We don‚Äôt use it; we need our server to do silly stuff like top3.nearest.of.my-app.internal.\n\nWe felt this acutely with private DNS. Consul propagates the data that our DNS servers use (it‚Äôs similar to the data that fly-proxy uses). Fly.io Postgres depends on these DNS records, so it needs to work.\n\nOur DNS server was originally written in Rust, and used consul-templaterb the way fly-proxy did, but wrote its updates to a sqlite database. At certain times, for certain workers, we‚Äôd experience double-digit second delays after instances came up ‚Äî or worse, after they terminated. This is a big deal: it‚Äôs a window of many seconds during which internal requests get routed to nonexistent hosts; worse, the requests aren‚Äôt being handled by our smart proxy, but by people‚Äôs random app networking code.\n\nWe blamed Consul and consul-templaterb.\n\nTo fix this, we rewrote the DNS server (in Go), so that it tracked Consul directly, using Consul‚Äôs Go API, rather than relying on consul-templaterb. We also had it take ‚Äúhints‚Äù directly from our orchestration code, via NATS messages, for instances starting and stopping.\n\nRewriting a Rust program in Go is sacrilege, we know, but Go had the Consul libraries we needed, and, long term, that Go server is going to end up baked into our orchestration code, which is already in Go.\n\nIt turns out that what we really want (if not our dream Consul API) is a local sqlite cache of all of Consul‚Äôs state. That way our proxy, WireGuard code, DNS servers, and everything else can track updates across our fleet without a lot of complicated SRE work to make sure we‚Äôre interfacing with Consul properly.\n\nBy rewriting our DNS server, we‚Äôd inadvertently built most of that. So we extracted its Consul-tracking code and gave it an identity of its own, attache. attache runs on all our hosts and tracks most of Consul in sqlite. In theory, infra services at Fly.io don‚Äôt need to know anything about Consul anymore, just the schema for that database.\n\nNew architectural possibilities are becoming apparent.\n\nTake that horrible N^2 polling problem. Since we‚Äôve abstracted Consul out, we really don‚Äôt need a better Consul API; we just need an attache API, so a ‚Äúfollower‚Äù attache can sync from a ‚Äúleader‚Äù. Then we could run a small number of leaders around the world ‚Äî maybe just alongside Consul Servers, so that almost all our Consul read traffic would be from machines local to the Consul Servers. We‚Äôd chain lots of followers from them, and possibly scale Consul indefinitely.\n\nWe can get clever about things, too. What we‚Äôd really be doing with ‚Äúleader‚Äù and ‚Äúfollower‚Äù attache is replicating a sqlite database. That‚Äôs already a solved problem! There‚Äôs an amazing project called Litestream that hooks sqlite‚Äôs WAL checkpointing process and ships WAL frames to storage services. Instead of building a new attache event streaming system, we could just set up ‚Äúfollowers‚Äù to use Litestream to replicate the ‚Äúleader‚Äù database.\n\nIt‚Äôs just a couple commands to get an app deployed on Fly.io; no service discovery required.\n\n\n## We Don‚Äôt Want Raft\n\nWe‚Äôre probably not going to do any of that, though, because we‚Äôre increasingly convinced that‚Äôs sinking engineering work into the wrong problem.\n\nFirst, if you haven‚Äôt read the Google Research paper ‚ÄúThe Tail At Scale‚Äù, drop everything and remedy that. It‚Äôs amazing, an easy read, and influential at Fly.io.\n\nThen: the problem we‚Äôre solving with attache is ‚Äúcreating a globally consistent map of all the apps running across our fleet‚Äù. But that‚Äôs not our real problem. Our real problem is ‚Äúgenerate fast valid responses from incoming requests‚Äù. Solving the former problem is hard. It‚Äôs one answer to the real problem, but not necessarily the optimal one.\n\nNo matter how we synchronize service catalogs, we‚Äôre still beclowned by the speed of light. Our routing code always races our synchronization code. Routing in Tokyo is impacted by events happening in S√£o Paolo (sandwich: mortadella on a roll), and events in S√£o Paolo have a 260ms head start.\n\nA different strategy for our real problem is request routing that‚Äôs resilient to variability. That means a distributing enough information to make smart first decisions, and smart routing to handle the stale data. We‚Äôve already had to build some of that. For instance, if we route a request from an edge to a worker whose instances have died, fly-proxy replays it elsewhere.\n\nYou can finesse this stuff. ‚ÄúThe Tail At Scale‚Äù discusses ‚Äúhedged‚Äù requests, which are sometimes forwarded to multiple servers (after cleverly waiting for the 95th percentile latency); you take the first response. Google also uses ‚Äútied‚Äù requests, which fan out to multiple servers that can each call dibs, canceling the other handlers.\n\nWe‚Äôd do all of this stuff if we could. Google works on harder problems than we do, but they have an advantage: they own their applications. So, for instance, they can engineer their protocols to break large, highly-variable requests into smaller units of work, allowing them to interleave heavyweight tasks with latency-sensitive interactive ones to eliminate head-of-line blocking. We haven‚Äôt figured out how to do that with your Django POST and GET requests. But we‚Äôre working on it!\n\nA lot of our problems are also just simpler than Raft makes them out to be. We used to use Consul to synchronize load information. But that‚Äôs kind of silly: Consul is slower than the feed of load events, and with events sourced globally, we never could have had a picture that was both fresh and accurate. What we needed were hints, not databases, and that‚Äôs what we have now: we use NATS (which isn‚Äôt necessarily even reliably delivered, let alone consensus-based) to gossip load.\n\nSame goes for health checks. We already have to be resilient to stale health check information (it can and does change during request routing). Consul keeps a picture of health status, and we do still use it to restart ailing instances of apps and report status to our users. But fly-proxy doesn‚Äôt use it anymore, and shouldn‚Äôt: it does a fine job generating and gossiping health hints on its own.\n\n\n## But, As Always: DNS\n\nWe can make our routing resilient to stale health and load information, and push orchestration decisions closer to workers to be less reliant on distributed health events. We have a lot of flexibility with our own infrastructure.\n\nThat flexibility stops at the doors of the VM running your Django app. We don‚Äôt own your app; we don‚Äôt really even know what it does. We‚Äôre at the mercy of your socket code.\n\nSo one place we‚Äôre kind of stuck with Consul-style strongly consistent service maps is DNS. Normal apps simply aren‚Äôt built to assume that DNS can be a moving target. If your app looks up sandwich-postgres.internal, it needs to get a valid address; if it gets nothing, or, worse, the address of a VM that terminated 750ms ago, it‚Äôll probably break, and break in hyper-annoying ways that we don‚Äôt yet have clever ways to detect.\n\nWe‚Äôve spent the better part of 9 months improving the performance of our .internal DNS, which is a lot for a feature that was an afterthought when I threw it together. We‚Äôre in a sane place right now, but we can‚Äôt stay here forever.\n\nWhat we‚Äôre going to do instead ‚Äì¬†you‚Äôll see it soon on our platform ‚Äì¬†is play the ultimate CS trump card, and add another layer of indirection. Apps on Fly.io are getting, in addition to the DNS names they have today, ‚Äúinternal Anycast‚Äù: a stable address that routes over fly-proxy, so we can use the same routing smarts we‚Äôre using for Internet traffic for Postgres and Redis.\n\nYou‚Äôll still get to be fussy about connectivity between your internal apps! Internal Anycast is optional. But it‚Äôs where our heads are at with making internal connectivity resilient and efficient.\n\n\n## And So\n\nWe mostly like Consul and would use it again in new designs. It‚Äôs easy to stand up. It‚Äôs incredibly useful to deploy infrastructure configurations. For example: we write blog posts like this and people invariably comment about how cool it is that we have a WireGuard mesh network between all of our machines. But, not to diminish Steve‚Äôs work on flywire, that system falls straight out of us using Consul. It‚Äôs great!\n\nBut we probably wouldn‚Äôt use Consul as the backing store for a global app platform again, in part because a global app platform might not even want a single globally consistent backing store. Our trajectory is away from it."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/intro-to-accessibility/",
    "content": "Hi, I‚Äôm Nolan, a totally blind software developer working on accessibility at Fly.io. I‚Äôve worked on website remediation, I‚Äôve written screen readers from scratch, and I also create 2-D audio-only arcade games. Here‚Äôs a screenshot of my latest game:\n\nLike every developer, I rely on a number of products and services to manage and promote my many side projects. But I‚Äôve had more than one neat idea go down in flames because I simply couldn‚Äôt use the amazing, must-have service that would have made that project feasible to complete.\n\nA few weeks back, Chris McCord launched LiveBeats, a real-time collaborative MP3 player, inspired by turntable.fm. LiveBeats is cool on its own merits (check it out), but, just as importantly, it‚Äôs designed to be a model real-time Phoenix LiveView application, a blueprint other people can start with for their own apps. And, well, the model real-time Phoenix LiveView application needs to be more accessible. Today, I‚Äôm going to get you into the right mindset by talking about what that means, and why it matters.\n\nReading about accessibility is good, but engaging is better. For obvious reasons, I‚Äôm serious about this stuff. If you‚Äôve got questions, or accessibility thoughts of your own, hit me up here. I‚Äôm more than willing to chat about this stuff.\n\nDoing accessibility well isn‚Äôt difficult, especially if you plan ahead. So I‚Äôd like to talk a bit about what accessibility is, why you should care, and a few common missteps teams make on their journey.\n\n\n## What Is Accessibility, Really?\n\nBroadly speaking, accessibility refers to whether someone can use your product or service regardless of how they interact with it.\n\nAs a blind developer, I naturally focus on screen reader accessibility. Now, it‚Äôs important not to fall into the trap of assuming that accessibility is one-size-fits-all. But improvement in one area generally helps across the board. It‚Äôs OK to start somewhere and branch out.\n\nAccessibility is not a destination, but a process. It‚Äôs meeting your users where they are, and working to improve their experiences. As your product evolves, it gets faster. More reliable. More featureful. You should expect it to get more accessible too.\n\n\n## Why Should I Care?\n\nFirst, you care because it makes your app better. Accessibility isn‚Äôt just about whether your software or service is usable by the 15% of the world‚Äôs population that identify as having a disability.\n\nServices with fewer interaction limitations are almost certainly easier and more pleasant to use for everybody. Common sense, right? Take mobile accessibility. For me, labeled controls and clearly-defined focus areas are essential. But those same ‚Äúaccessibility features‚Äù could just as easily serve you with reliable voice operation while driving, or just with your phone safely tucked away. And if you‚Äôre already investing in good UX, many of those same practices transfer to building solid accessibility.\n\nNext, you care because any of us might acquire a disability at any time. One day, I stepped off a curb, twisted my ankle, and had a hard time supporting myself on that foot for several months. I don‚Äôt normally need physical supports, so my home wasn‚Äôt built with them. I had a lot of trouble doing things myself.\n\nMy partner has a mobility-related disability, however, and her apartment has a number of grab rails and transfer bars. My life was so much easier while I was there. Most of the time for me, accessibility is primarily about whether and to what extent I can achieve something non-visually. But for those few months when I had a new temporary disability, every painful struggle in my home was a moment when I desperately wished we designed more universally, not waiting for someone to need accessibility before haphazardly slapping it on.\n\nEventually age will likely impair your hearing, eyesight, mobility, or cognition. But even if age treats you well, injury or illness may temporarily render you unable to use products and services on which you‚Äôve come to rely. If you‚Äôd like to keep doing the things you enjoy, then you owe it to yourself and others to care about accessibility before it becomes something you depend on.\n\n\n## Build Accessibility in From the Start\n\nAccessibility is an ongoing conversation, not a single step. Putting off an important chat until the last minute is usually a recipe for disaster. So is deferring accessibility planning until your customers squawk about it.\n\nDeque University‚Äôs accessibility courses are a thorough foundation for any team wanting to learn lots about web accessibility.\n\nImagine planning an industry event that you‚Äôd like to be accessible. You‚Äôll need to select a venue, line up presenters, and arrange printed material for distribution to attendees. Treating accessibility as something you can retrofit might work out, if you‚Äôre lucky. More likely, you‚Äôll miss the fact that the venue is atop a flight of stairs. Deaf participants won‚Äôt be able to participate in presentations. You‚Äôll dump a pile of paper on people who can‚Äôt even read it. And inevitably someone will write an angry blog post about it. I‚Äôve been there. Trust me, the person writing the angry blog post wishes they didn‚Äôt have to write it.\n\nAll of these missteps are significantly more difficult to fix at the last minute. Incidentally, they‚Äôre also mistakes I see people in our industry make, over and over again, when they host events ‚Äì¬†even events they describe as ‚Äúaccessible‚Äù.\n\nCompared to an accessibility checklist, an accessibility-focused process asks what barriers each choice might impose, then works to either address or eliminate those before they‚Äôre set in stone. Doing this requires knowing which questions to ask, and asking them before they become costly mistakes that are hard to fix.\n\n\n## Gather Your Tools\n\nModern tools can identify many of the most common access pitfalls. Though imperfect, automated testing can usually catch enough issues to make an inaccessible app at least usable. A minimally-usable app can then be audited ‚Äúlive‚Äù, which provides far more effective feedback.\n\nDeque‚Äôs Axe DevTools extension runs a number of automated in-browser accessibility checks. They‚Äôre no replacement for live testing, but the extension can catch a number of issues that you can fix before bringing in outside help.\n\nOn the web, lots of useful checks are baked directly into modern browsers‚Äô development tools. Even more advanced tests can be installed via extensions. Mobile apps have similar tooling, either run directly on-device or included in your IDE.\n\n\n## Test With Real Users\n\nIf you‚Äôre serious about a product, you test it for usability. In the same way, you should test your product and its overall design to see how it works for users with specific access needs.\n\nKnowbility is an excellent provider of accessibility consulting, training, and testing.\n\nSome teams learn the basics of the most popular screen readers and magnifiers. That‚Äôs a great way to build empathy. But unless you use these tools daily, you‚Äôre not using them the way people who rely on them do, and so you‚Äôre probably not using them correctly or effectively. This is a solvable problem! Accessibility testing services help you identify specific access needs, scope tests to address them, and recruit testers to provide actionable feedback.\n\n\n## Hire Lived Experience\n\nOne of the best ways to be better at accessibility is to recruit and hire people with disabilities. You don‚Äôt need someone with every disability. A decent dose of empathy goes a long way! But we‚Äôre usually (by necessity) the experts on our access needs, or are better equipped to ask the right questions and identify the most important next steps. Having people with disabilities on your team is one of the easiest ways to sort out the ‚Äúmust haves‚Äù from the accessibility ‚Äúnice to haves‚Äù.\n\nAnd there‚Äôs a large pool of experts out there. The unemployment rate among disabled folks is staggering‚Äîaround 60% in the US blind population, for example. That‚Äôs sixty. A six, followed by a zero. And the numbers are similarly high across other disabilities.\n\nA word of caution, though: if your group goes this route, please don‚Äôt make the disabled person your company‚Äôs ‚Äúaccessibility czar‚Äù. Hiring lived experience should be a¬†part¬†of your accessibility strategy, but not the whole thing. We‚Äôre already expending extra energy advocating for our access needs alone¬†outside¬†of work. Making accessibility entirely our responsibility at work is a sure path to burn-out.\n\n\n## Back to LiveBeats, For A Moment\n\nA big part of what I‚Äôm doing at Fly.io over the coming weeks is taking LiveBeats, the model Phoenix LiveView application, and making it accessible. I want people to learn not just the right way to stand up a Phoenix application, but also some of the most important tools to make all their applications accessible.\n\nSo, LiveBeats lets you play and listen to music with friends. All web apps have a bunch of generic accessibility problems. But LiveView posed a few additional unique challenges:\n\nThese are all fixable issues, and I‚Äôll post articles about solving them in turn. But I‚Äôm interested in hearing from you as well. If you have questions about what I‚Äôve shared, or are curious about other aspects of making a real-time web app accessible, please let me know! I‚Äôll be reading and responding to comments in the Fly.io community forum."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/our-user-mode-wireguard-year/",
    "content": "We‚Äôre Fly.io. We run container images on our hardware around the world, linked to our Anycast network with a WireGuard mesh. It‚Äôs pretty neat, and you should check it out. You can be up and running on Fly.io in single-digit minutes. That‚Äôs the last thing we‚Äôre going to say about how great Fly.io is in this post.\n\nWireGuard is fundamental to how Fly.io works.\n\nThat goes for users, too. F‚Äôrinstance: to SSH into an instance of an app running on Fly.io, you bring up a WireGuard peer to one of our gateways and SSH to an IPv6 private network address reachable only over that WireGuard session.\n\nThis kind of stuff is mostly hidden by flyctl, our command-line interface, which is how users interact with their apps on Fly.io. On most platforms, ‚Äúhow do you SSH to an instance‚Äù is a boring detail. But flyctl isn‚Äôt boring.\n\nI‚Äôm about to explain why, for better and worser, flyctl is exciting. But first, a disclaimer. In the ordinary course, a company writes a blog post such as this to crow about some achievement or ‚Äúunique selling proposition‚Äù. This post is not that. This is an exercise in radical candor.\n\n\n## A Houdini With The Manacles Of Sound Engineering\n\nRecap: It‚Äôs February 2021, and you can deploy a Docker container to Fly.io, with flyctl, and our platform will dutifully convert it to a Firecracker VM running on a worker in our network and connected (again, via WireGuard) to our Anycast proxy network so that your customers can reach it on the Internet.\n\nWhat you couldn‚Äôt do is easily log into that VM to monkey around with it.\n\nThat wouldn‚Äôt do; what‚Äôs the point of having a fleet of VMs if you can‚Äôt monkey with them? The team agreed, being able to pop a shell on a running app was table-stakes for the platform.\n\nWe noodled a bit about how to do it. For awhile, we thought about building a remote-access channel into our Rust Anycast proxies. But we‚Äôd just rolled out 6PN private networking, making it easy for Fly.io apps to talk to each other. SSH seemed like an obvious example of a service you might run over a private network.\n\nThe trick was how to get users access to 6PN networks from their laptops. It was pretty easy to build an SSH server to run on our VMs, and APIs for certificate-based access control and building WireGuard peers for 6PN networks. But the client side is tricky: WireGuard changes your network configuration, and mainstream operating systems won‚Äôt let you do that without privileges. It wouldn‚Äôt do to require root access to run flyctl, and having to do a bunch of system administration to set up system WireGuard before you could run flyctl wasn‚Äôt attractive either.\n\nSo we came up with a goofy idea. You have to be root to add a network interface, but any old user can speak the WireGuard protocol, and once you‚Äôve got a WireGuard session, TCP/IP networking is just building and shuttling a bunch of packet buffers around. Your operating system doesn‚Äôt even have to know you‚Äôre doing it. All you need is a TCP stack that can run in userland.\n\nI said this within earshot of Jason Donenfeld. The next morning, he had a working demo. As it turns out, the gVisor project had exactly the same user-mode TCP/IP problem, and built a pretty excellent TCP stack in Golang. Jason added bindings to wireguard-go, and an enterprising soul (Ben Burkert, pay him all your moneys, he‚Äôs fantastic) offered to build the feature into flyctl. We were off to the races: you could flyctl ssh console into any Fly.io app, with no client configuration beyond just installing flyctl.\n\nI want to pause a second here and make sure what we ended up doing makes sense to you, because it still seems batshit to me a year later. Want to SSH into a Fly.io app instance you started? Sure. Just run flyctl ssh console, and it will:\n\nAnd this works! You can do it right now! Why am I the only person that thinks it‚Äôs bananas that this works?\n\nIt Is Bananas That This (Mostly) Works\n\nAlright, radical candor.\n\nThe nerd quotient on flyctl ssh console is extreme, which is a strong argument in favor of it. But there are countervailing reasons, and we ran into them.\n\nHere‚Äôs a simple problem. When you tell flyctl ssh console to bring up a WireGuard session like this, that running instance of flyctl on your machine ‚Äî you know, the one that shows up in ps ‚Äî is effectively another computer on the Internet. It has an IPv6 address. It is the only machine on the Internet that can have that IPv6 address. So what happens when you open up an SSH session in one window, and then another session in a different window?\n\nIn March of 2021, the answer was ‚Äúit knocked the first SSH session off the Internet‚Äù. That‚Äôs how WireGuard works! Your peer keeps track of the source socket address that‚Äôs talking to it, and when a new source appears, that‚Äôs the host it starts talking to. It‚Äôs one of the great things about WireGuard, and why you can bring up a WireGuard connection, close your Macbook, walk to the coffee shop, open your Macbook back up, and still be connected to WireGuard.\n\nI tried to rationalize this ‚Äúone SSH session at a time‚Äù behavior for a couple weeks, but, come on.\n\nThere were two paths we could have taken to address this problem. The easy-seeming way would be to have each flyctl instance make a new WireGuard peer, each with its own IPv6 address and public key pair. There were things I didn‚Äôt like about that, like the fact that it would crud our WireGuard gateways up with zillions of random ephemeral WireGuard sessions. But the dealbreaker in Spring 2021 was that creating a new WireGuard peer configuration was slow. We will return to this point shortly.\n\nThe other way forward was to not have multiple instances of flyctl speaking WireGuard. Instead, when you made a WireGuard connection, we‚Äôd spawn a background process ‚Äî the flyctl agent. flyctl ssh console runs would come and go, each talking to the agent, which would stick around holding open WireGuard sessions. Sure, why not!\n\nI know how much you all love random background agent processes. I‚Äôm here to tell you that my Spring 2021¬† flyctl agent was all you could have imagined it would be. It only worked on Unix. Concurrency management? Try to start a new agent, and it‚Äôll just ask the old one to die over that same socket, and then take over. Configuration changes? I‚Äôm just a simple unfrozen caveman agent, what changes would I need to know about?\n\nCustom Resolver Dialers don‚Äôt work on Windows in Go?\n\nFortunately for everyone else, I‚Äôm not the only developer on this team, and the agent got some professional help. The team got Unix domain sockets working on Windows. They wrote a new DNS resolver that worked on Windows as well. The agent will only run one of itself at a time. It notices configuration changes after it starts, and doesn‚Äôt get out of sync and stale. If you use flyctl today, you‚Äôre missing a whole lot of debugging fun.\n\n\n## Doubling Down On Banana Futures\n\nUser-mode WireGuard and TCP/IP via IPC with a background agent is an awful lot of mechanism just to run an SSH session. A lesser engineer might look at this and say ‚Äúthe thing to do here is to get rid of some of this mechanism‚Äù. We chose instead ‚Äúdo more stuff with that mechanism‚Äù. I mean, I say ‚Äúwe chose‚Äù. But I was the last to know; I arose from a long slumber at some point in the middle of the year to find that our deploys were running over user-mode WireGuard.\n\nHere‚Äôs another challenge users run into when deploying Docker apps on Fly.io: they‚Äôre often not running Docker. An engineer of limited imagination such as myself would look at this as a documentation problem, the solution to which would be an instruction to users to ‚Äúinstall Docker‚Äù. But we‚Äôre a clever lot, we is.\n\nFlip back to that 1-2-3 process for popping a shell over user-mode WireGuard. Here‚Äôs a new problem: ‚Äúfrom a directory with a Dockerfile, deploy a Docker image on Fly.io if you‚Äôre not running Docker locally‚Äù. Here‚Äôs what you do:\n\nIt‚Äôs just 5 steps! It all happens in the background; you just run flyctl deploy! What could go wrong?\n\nThis pattern repeats. The horror of user-mode WireGuard and TCP/IP is that it is a whole lot of mechanism. But the beauty of it is that it‚Äôs mind-bogglingly flexible. A little later in the year, we launched Fly.io Postgres. Want to bring up a psql shell on your database? flyctl pg connect. Do I need to rattle off the 1-2-3 of that? For that matter, what if you have a cool client like Postico you want to use? No problem! flyctl proxy 5432:5432. A proxy isn‚Äôt even 3 whole steps!\n\nHere‚Äôs where shit gets real. One can rationalize the occasional SSH connection janking out. SSH wasn‚Äôt even a feature we had at the beginning of the year. But deploys? Deploys have to work.\n\n\n## The Deploys, They Were Not Always Working\n\nMore radical candor.\n\nWe have some good ideas on what made remote builds over WireGuard shaky, and builds have gotten a lot better. But I can‚Äôt tell you we‚Äôve nailed down every failure mode. Here are two big ones.\n\nFirst: bringing up new WireGuard peers was slow. Real, real slow.\n\nIt‚Äôs Fall of 2021 and here‚Äôs what happened when you asked us to create a new WireGuard peer:\n\nThis is very bad. It only happens the first time you use WireGuard on a machine; the next time you go light up that WireGuard peer, it‚Äôll come right up, because it‚Äôs already installed. But guess who‚Äôs making a WireGuard connection with flyctl for the first time? That‚Äôs right: someone who just decided to try out Fly.io and followed our Speedrun instructions. No fair! It looks like all of Fly.io isn‚Äôt working, when in fact the only part of Fly.io that isn‚Äôt working is the part that allows you to use it.\n\nThat‚Äôs Will Jordan, on our SRE team.\n\nThere was low-hanging fruit to pick here. For instance, Will took one look at our WireGuard resync shell script and cut its runtime from 10 seconds to a few dozen milliseconds. But consul-templaterb ‚Äî well, it is what it is. ‚ÄúThings will go as they will, and there‚Äôs no need to hurry to meet them‚Äù, it says. ‚ÄúI am on nobody‚Äôs side, because nobody is on my side, little orc.‚Äù\n\nWe have, in our prod infrastructure, two basic ways of communicating state changes: Consul and NATS. Consul is slow and ‚Äúreliable‚Äù; NATS is fast, but doesn‚Äôt guarantee delivery. A few weeks ago, we switched from consul-templaterb to a system we call attache, which, among other things, does NATS transactions to update WireGuard peers. In the new system, creating a new WireGuard peer looks like this:\n\nThe whole process might take a second or two. It‚Äôs fast enough that you could imagine revisiting the decision to have a flyctl agent; with a little bit of caching, you could just make new WireGuard peers whenever you need them, and we could garbage collect the old ones.\n\nThere Is A More Ominous Problem I Don‚Äôt Like Talking About\n\nThe remote builds, they‚Äôre way better. You can stick flyctl in your Github actions for CI and it‚Äôll work.\n\nBut I have a nagging feeling it can‚Äôt be working perfectly for everybody, because it involves running WireGuard, and, as you may already know, WireGuard doesn‚Äôt run over 443/tcp.\n\nIf you‚Äôre on a corporate network with a proxy firewall, or on a janky VPN, or some random CI VM, 51820/udp might be blocked. In fact, for all we know, all UDP might be blocked. I‚Äôve tried telling Kurt ‚Äúthat‚Äôs their problem‚Äù, but I haven‚Äôt won the argument.\n\nThere is, in the Github project for flyctl, a branch that addresses this problem. Our WireGuard gateways all run a program called wgtcpd. It is as elegant as it is easy to pronounce. It runs an HTTPS server (with a self-signed certificate, natch!) with a single endpoint that upgrades to WebSockets and proxies WireGuard. The flyctl tcp-proxy branch will run WireGuard over that, instead of UDP.\n\nI‚Äôm here to tell you that for all the nattering about how problematic UDP-only WireGuard is, it turns out not to involve a lot of code to fix; the WebSockets protocol for this is just ‚Äúsend a length, then send a packet, read a length, then read a packet‚Äù.\n\nI originally called DERP ‚ÄúWebRTC‚Äù and Tailscale took offense, saying that the proper term would be ‚Äúnot-WebRTC not-ICE not-TURN but-kinda-similarish DERP NAT Traversal thingy‚Äù.\n\nWe could do something even more clever here; for instance, our friends/archnemeses at Tailscale run a global network of something they call ‚ÄúDERP‚Äù, which is part of their NAT-traversal proxy system; we could have our gateways connect to their DERP servers and register our public keys, and then you‚Äôd be able to connect to the same DERP servers and talk to us, and that seems like a fun project because there‚Äôs apparently nothing they can do to stop us.\n\nBut we‚Äôre still in denial about this problem and waiting for it to smack us in the face; we haven‚Äôt even merged the WebSockets branch of flyctl, because maybe it‚Äôs just not an issue? We only just solved the peer creation lag problem, and we‚Äôre waiting for things to even out. But if you needed to, you could run a WebSockets build of flyctl today.\n\n\n## Where This Leaves Us\n\nI‚Äôve painted a picture here, and you might infer from it that I regret user-mode WireGuard and TCP/IP. But the truth is, I love it very much; it is one of those Fly.io architectural features that makes me happy to work here. I‚Äôd say that for the first half of 2021, it probably wasn‚Äôt paying its way in complexity and operational cost, but that it‚Äôs opened up a bunch of possibilities for us that will let us build other bananas features without having to change anything in our prod infrastructure.\n\nThere‚Äôs a fun side to flyctl WireGuard. For instance, it has its own dig command, which talks directly to our private .internal nameservers. What‚Äôs that, you say? The same feature would be a couple dozen lines of Ruby in a GraphQL API? Shut up!\n\nOr, how about this: you can ping things now. Ping! Of all things! You can flyctl ping my-app.internal and we‚Äôll ping each instance of my-app for you. I know how much you love pinging things. And what‚Äôs the fun of using a hosting platform if you don‚Äôt get to pilot Howl‚Äôs Moving Castle Of Weird Network Engineering to check the latency on your app instances?\n\nAlso, I blame Jason Donenfeld and Ben Burkert for all of this.\n\nAs I said at the top, this is one of those posts that isn‚Äôt trying to sell Fly.io, but just provide a somewhat honest accounting of the experience of building it, and a peek into the way we think about stuff. Having said all that: you should take Fly.io for a spin, because when flyctl ssh console is working ‚Äî and it‚Äôs working pretty much all the time now ‚Äî it is slick as hell."
  },
  {
    "title": "You‚Äôve got a parachute.",
    "url": "https://fly.io/blog/new-turboku/",
    "content": "No need to choose between us! Effortlessly clone your Heroku app to Fly.io with our new, faster Turboku launcher. You can even talk to your Heroku Postgres, no data migration required.\n\nIn early 2020, before we launched our VM platform, we made a little landing page called Turboku. It was a one-click launcher to deploy a Heroku app on Fly.io. It let people try us out without spending time porting an app.\n\nMany of our first customers found us through this page. Sadly, we neglected it for some time, but we‚Äôve come back to give it some love. Today we‚Äôre excited to introduce you to New Turboku.\n\n\n## Where we‚Äôre coming from\n\nHere‚Äôs how Turboku worked: you‚Äôd visit the landing page in your browser, authenticate with Fly.io and Heroku, select your coffee-bean recommender from a list of your Heroku apps, and click ‚ÄúLaunch app on Fly.‚Äù At that point the build was farmed out to AWS CodeBuild.\n\nCodeBuild is Amazon‚Äôs fully-managed build service. You specify (or provide) a build environment, source code and commands, and CodeBuild runs it on AWS hardware. Our CodeBuild builders were stock Linux CI VMs with scripts and a sequence of commands to run. We could have run all that on our hardware, except for one small detail: at the time, we didn‚Äôt have storage attached to our VMs.\n\nWhen the build was done, the CodeBuild VM would deploy your convoluted bitter bean chooser on Fly.io.\n\n\n## Bringing it home\n\nYou may recall that, like Old Turboku, all our remote builds used to use CodeBuild. This worked, but CodeBuild builds were so slow that we hated subjecting our users to them. Fly Volumes changed the game, letting us run builders on our own hardware, which in turn turbocharged our remote builds.\n\nWe want Turboku builds to be fast, too. So we rebuilt Turboku with flyctl integration, skipping third-party services in favor of our speedy remote builders. flyctl lives on your local machine and talks to our API. It‚Äôs your CLI command center, and now it can deploy your Heroku app!\n\nHere‚Äôs how that goes:\n\n\n## Peek inside\n\nDid you know flyctl is open source? You can check out the Turboku integration here. We promise Golang won‚Äôt hurt you.\n\n\n## Turboku launcher joins the battle\n\nNow Turboku is turbofast, and we love that you can do it all with flyctl, but we know that some people might see this as (gasp!) a step backwards from a web launcher.\n\nWe‚Äôre not offended. That‚Äôs fine! Because we also love Phoenix LiveView for building Fly.io web launchers.\n\nWe‚Äôve already launched a launcher for Livebook, and one for Jupyter Notebooks, and users seem to dig them. Now we have a matching one for Heroku migration too.\n\nWhen you connect to Heroku and Fly.io through the Turboku launcher, you don‚Äôt have to copy-paste your token, and we can give you a nice list of your Heroku apps to choose from. We‚Äôll ask you for some minimal configuration, and then it‚Äôs time to smash that GUI deploy button!\n\nHere‚Äôs where we‚Äôre pleased with our flyctl-loving selves. We could have rewritten the deployment code in Elixir, but that would be wasting good flyctl magic (and hard work!).\n\nInstead, we spawn a Fly.io machine that runs flyctl for us! You can sit back and watch as it massages your Heroku app into shape for deployment on Fly.io. Turboku handles everything from creating the Dockerfile to building your app on our remote Docker builders.\n\nThe video above is in real time. It‚Äôs hard to catch without pausing, but it takes about two seconds for the launcher to spin up a new VM running flyctl. By about eight seconds in, it‚Äôs created and configured the app, and started the deploy process. It calls out to a remote builder and starts transferring Docker build context.\n\nGranted, we‚Äôre not done yet. The actual build and the creation of a VM running your app on a worker take a further finite time.\n\nA byproduct of this process is an app with a name like flyctl-host-personal-1234 . When the initial Turboku deploy is finished, this app sits around (for free) waiting for webhook updates or another Turboku deploy.\n\n\n## Taking stock\n\nNew Turboku makes migrating from Heroku faster and, if we say so ourselves, more elegant.\n\nTurboku (still) doesn‚Äôt delete your Heroku app from Heroku. That part is up to you. You can keep it up in both places, if you really want to! And if you‚Äôre using Heroku‚Äôs Postgres with your app, or an add-on like Redis, its Fly.io incarnation should be able to use it too, because Turboku brings your app‚Äôs environment variables over with it.\n\nAs with Old Turboku, the new Turboku launcher sets up webhooks on the Heroku app for automatic updates on new Heroku deploys. We also watch for changes to environment variables like DATABASE_URL, which Heroku promise that they will change on you.\n\nWebhook integration is one feature that flyctl turboku doesn‚Äôt have, so if this is important to you, the web launcher is the way to go.\n\nWhile the connection to your Heroku database has to happen over the internet, it should be very fast - we measured about a millisecond of added latency for a US-based app. The special sauce for this is that non-enterprise Heroku apps all live either in Dublin or Virginia, so Turboku will deploy in our closest region (London or Virginia) to be next door, unless you specify a different region.\n\nTurboku makes it a breeze to add your Heroku app to Fly.io‚Äôs Rust-powered Anycast network‚Äìand it stays up on Heroku, so you don‚Äôt have to commit to try it.\n\nBut let‚Äôs face it: if your Postgres is still stuck in one region, it‚Äôs hard to take advantage of our global edge network to suggest sophisticated beans in Singapore. We don‚Äôt have a similarly painless solution for cloning and syncing databases! üôÄ\n\nObviously, we hope that once you‚Äôve tried Fly.io, you‚Äôll be convinced it‚Äôs worthwhile to make the jump over and run Globally Distributed Postgres!\n\nIf you have a Heroku app you‚Äôd like to bring closer to your users, give the Fly Turboku Launcher a spin."
  },
  {
    "title": "Go global with Fly.io.",
    "url": "https://fly.io/blog/livebeats/",
    "content": "Fly.io is now free for small Phoenix projects. This post is about building a wicked LiveView app with realtime collaboration features. If you already have a Phoenix app to deploy, try us out. You can be up and running in just a few minutes.\n\nWe decided that 2022 was a good year to ship a full-stack Phoenix reference app.\n\nThe ‚Äúfull stack‚Äù metaphor has progressed beyond its humble beginnings of some REST endpoints and sprinkles of JS and CSS. Showing off a todo app is also no longer state of the art. A reference app should really stress a framework and match the needs of apps being built today. Remember turntable.fm? That‚Äôs a more interesting challenge.\n\nA good full-stack framework should help you solve ALL the problems you need to build something like turntable.fm quickly, and then iteratively make it more powerful. Live updates are no longer optional, and a solo full stack developer should be able to deliver on these features with the same productivity of a CRUD Rails app in 2010.\n\nMeet LiveBeats, a social music application we wrote to show off the LiveView UX, while serving as a learning example and a test-bed for new LiveView features. As such, it is, of course, open source ‚Äî follow the development here!\n\n\n## üî•üé∂üî• Try LiveBeats Now! üî•üé∂üî•\n\nIf you‚Äôre not familiar with LiveView, our overview states:\n\nLiveView strips away layers of abstraction, because it solves both the client and server in a single abstraction. HTTP almost entirely falls away. No more REST. No more JSON. No GraphQL APIs, controllers, serializers, or resolvers. You just write HTML templates, and a stateful process synchronizes it with the browser, updating it only when needed. And there‚Äôs no JavaScript to write.\n\nHere are some of the things LiveBeats demonstrates:\n\nA ‚Äúlive‚Äù, shared UI. What one person does is visible to everyone else who‚Äôs connected.\n\nFile management. Uploads should be quick, with a live UI. And the framework should make it easy to use different storage backends. Third party object storage is one way to do this, but sometimes development is simpler with just a filesystem. LiveView uploads is great for both!\n\nPresence! Apps are more interesting when your friends show up.\n\nWe can accomplish all this with just Phoenix and LiveView, in a shockingly small amount of code. It‚Äôs also super fast.\n\nTo really see what‚Äôs special about LiveView and its new features, you need to experience it for yourself. Sign in with GitHub OAuth, upload a playlist of MP3‚Äôs, and listen to music with your friends. Playback syncs in real-time, presence shows who is currently listening, and file uploads are processed concurrently as they are uploaded.\n\nHere‚Äôs a two-minute demo to see it in action:\n\n\n## Playlist Sync and Presence with Phoenix PubSub\n\nThe hallmark of any ‚Äúlive‚Äù or social app is seeing your friends‚Äô activity as it happens. In our case, we want to show who is currently listening to a given playlist, and sync the playback of songs as the owner drives the song selection. This is what it looks like:\n\nPhoenix PubSub makes this trivial. In a few lines of code in our business logic, we broadcast updates, then with a few LOC in the LiveViews we listen for the events we care about. When they come in, we update the UI. Everyone connected sees the pages update, even if they are on different horizontally scaled servers.\n\nWith friction-free PubSub at your fingertips, any feature that makes sense to be real-time gets to be real-time‚Äîeven changing URLs on the fly.\n\nFor example, user profiles are served at their username, such as livebeats.fly.dev/chrismccord. But we allow users to update their username in the app, which changes that URL. We don‚Äôt want other users listening to their profile to be stuck with an invalid URL that fails on refresh, sharing, or with a click on the profile link.\n\nHandling the URL change took a whopping six lines of code in our Profile LiveView!\n\nWe are already subscribed to profile notifications, so we just have to handle the PublicProfileUpdated event, update the template profile state, and then push_patch to the client to trigger a pushState browser URL change.\n\nFor a traditional application, this kind of feature would take standing up WebSocket connections, ad-hoc HTTP protocols, polling the server for changes, and other complexities.\n\nLet‚Äôs see it in action:\n\n\n## Concurrent Upload Processing\n\nLiveView uploads are handled over the existing WebSocket connection, providing interactive file uploads with file progress out-of-the-box - with no user-land JavaScript.\n\nIt also allows other neat features like processing the file on the server before writing it to its final location. You might be thinking handling files on your server is sooo mid 2000‚Äôs, but hear me out! Servers can have volumes‚Ä¶ and you can like write files to them!\n\nThink about it. There aren‚Äôt any Lambdas to wire up (and pay for per invocation), no webhooks to wire up, and no external message queues to configure, because it all happens over the existing LiveView connection. Compared to a typical cloud upload solution, we can cut out probably a few paid products and just as many failure modes.\n\nLiveView also guarantees the temporary uploaded file is on the same load-balanced instance of the LiveView processing the page. There‚Äôs no external state to jump around between servers. So you write regular code that takes the file, post-processes or verifies the bits on disk, then writes it to its final location ‚Äì all the while reporting to the UI the progress of each step.\n\nAnd we do need to extract data out of those binary blobs one way or another, because before we write that MP3 to storage, we want to know that (a) it‚Äôs not a malicious file masquerading as an MP3 and (b) it‚Äôs less than 20 minutes long.\n\nSo the user drags and drops a handful of MP3s into the app, we upload them concurrently over the WebSocket connection, then we concurrently parse the binary MP3 data in a temporary file to verify it‚Äôs a valid MP3 of acceptable duration. Once complete, we show the calculated duration on the UI and the user can save their playlist.\n\nAn aside: It turns out calculating MP3 duration from file content is actually a pain.\n\nMP3s can contain ID3 tag metadata about the file, but answering the simple question of ‚Äúhow long is this song?‚Äù is surprisingly difficult. To properly calculate the duration of an MP3, you must walk all the frames and take bitrate encodings into consideration. There‚Äôs a great write-up here on what‚Äôs involved with step-by-step Elixir code to make it happen.\n\n\n## Components\n\nLiveView recently shipped a new HEEx template engine that supports React-style template syntax with function components. Function components are reusable functions that encapsulate a bit of markup to be used throughout your UI. Think dropdowns, modals, tables, etc. We have a post all about function components, if you want to dive deeper.\n\nFor example, one of our components is a TailwindUI dropdown, which looks like this:\n\nAnd this is what it looks like in code to use anywhere you‚Äôd like a dropdown:\n\nAlong with function components, LiveView includes slots, which allows a component to specify a named area of the component where arbitrary content can be placed by the caller. The actual dropdown component is just a simple function that encapsulates all the markup and classes of our Tailwind dropdown:\n\nThere‚Äôs a lot going on inside the function with Tailwind classes, SVGs, list item building, accessibility attributes, etc, but the internal details aren‚Äôt important. The idea is: we define our dropdown in a single place in our application, complete with how it‚Äôs styled and accessible, and then we use \u003c.dropdown\u003e throughout our UI.\n\n\n## Going Global\n\nElixir is a distributed programming language and we exploit this fully. LiveBeats is deployed on five continents and the servers cluster together automatically over a private network to broker updates. This is what the future of full-stack looks like.\n\nYou should serve your full-stack app close to users for the same reason we all agree CDNs are necessary for serving assets close to users. Less latency and more responsiveness improves the experience, and conversions are increased across the board. Folks know this intuitively, but historically we‚Äôve only applied it to assets. With Elixir, our entire stack can take advantage of regional access.\n\nDistributed by design, Elixir Phoenix apps are a perfect fit with the globe-spanning Fly.io network.\n\nFor LiveBeats, we went global with only minor changes. First, we set up file proxying between servers, where we stream data from one region to another. Next, we set up Postgres read replicas in each region and we perform standard replica reads against mostly static data.\n\nWe even set up a ping tracker for each user. You can view your own pings, along with the locations and pings of any other visitors, to visualize where your friends are connected across the globe. It‚Äôs also neat to see what kind of speedy UX they have on the app.\n\nCheck it out:\n\nHere we have a user connecting from the US (iad) and one from Australia (syd), both with fast pings to the regional LiveBeats instances.\n\nHere‚Äôs a bonus video giving a window into the process of scaling LiveBeats across regions on Fly.io.\n\n\n## Client-side interactions\n\nThe LiveView paradigm necessarily requires a server to be connected, but this doesn‚Äôt mean all interactions should go to the server. Operations that can immediately happen on the client should stay on the client.\n\nLiveView has a JS command interface that allows you to declare client-side effects that work seamlessly with server-issued UI updates. Things like opening modal dialogs, toggling menu visibility, etc happen instantly on the client without the server needing to be aware - just as it should.\n\nLet‚Äôs see it in action using the LiveBeats modal:\n\nWhen a user attempts to delete a song, we show a modal component asking for confirmation. The key aspect is this: when ‚ÄúDelete‚Äù is clicked, our on_confirm attribute asynchronously pushes a delete event to the server, but immediately hides the modal and song in the playlist.\n\nSo we get instant user interaction without any latency, while the song is deleted on the server, just like traditional optimistic UI patterns in a JavaScript framework.\n\nSince the JS command interface is regular Elixir code, we can compose functions to handle client-UI operations, such as hide above:\n\nThis wraps JS.hide and uses :transition to give it a slick Tailwind-based CSS transition when hiding elements.\n\n\n## What‚Äôs Next\n\nWith LiveBeats fleshed out, we‚Äôll continue to add features and use it as a test-bed for in-progress LiveView development. Expect future content deep diving into approaches and considerations when bringing your LiveView application close to users around the world!"
  },
  {
    "title": "Store it here.",
    "url": "https://fly.io/blog/free-postgres/",
    "content": "Postgres on Fly.io is now free for small projects. The hard part about free Postgres is storage, so this post is also about free storage. Read about it here, or try us out first. You can be up and running in just a few minutes.\n\nWe like building side projects and also hate paying for hosting for side projects. We also know that y'all like free stuff. And we think that when you use free stuff for side projects, there‚Äôs a pretty good chance you‚Äôll pay for similar stuff for real work.\n\nWe‚Äôve had a free tier since we launched ten years ago (in 2020). Until today, our free tier covered a little bit of CPU time, a little bit of RAM, and a little bit of bandwidth. We didn‚Äôt include storage.\n\nThere‚Äôs a good reason for this. CPU, RAM, and bandwidth are easy. It costs almost nothing to keep a process idling. It‚Äôs easy to migrate application processes. And, importantly, it‚Äôs easy to bounce back when hardware goes poof. These things are easy because there‚Äôs almost no state involved. An application process that stops and starts fresh on different hardware continues to be valuable.\n\nYour enormous database of sandwiches is different, though.\n\nStored data occupies space all the time, though, whether your app is running or not. Recovering data from hardware failure means you have to be storing it with some redundancy. You need replication and backups. This means more disk space, but it also means more management. Disks are not easy!\n\nBut how boring are apps with no state? You could store some sandwiches right in your app, but you‚Äôd have to deploy it again every time an ingredient changed. And you wouldn‚Äôt remember who‚Äôd ordered from you before, let alone what their favorite condiments are. The best apps save data, and the best-best apps keep track of it in a database so it‚Äôs easy to remix and share: what better way to let your customers create a poll for the office lunch order?\n\n\n## What IS free Postgres?\n\n‚ÄúFree Postgres‚Äù really means we‚Äôve added 3GB of persistent volume space to our free offering. Specifically, you‚Äôre getting LVM block devices as small as 1GB. Match that up with our free VM and RAM allowance, and you‚Äôve got what you need for a side-project Postgres setup.\n\nThe free VMs themselves are just a bit of memory and some idling CPU, but the state is obnoxious for us to manage.\n\nWe‚Äôve done the work to sort out some of the management headaches with our baked-in Postgres. ‚ÄúProduction‚Äù deployments set up 2-node Postgres clusters intentionally. They should have higher availability than a single node, and they have data redundancy. When you run HA Postgres, you‚Äôre getting data storage on two isolated NVMe drives. We also take snapshots of these volumes, to give your delicious data some buffer against mishap.\n\nFree Postgres would start with a single ‚ÄúDevelopment‚Äù node, but if you scale it, it seamlessly becomes a leader-replica cluster for high availability.\n\n\n## So‚Ä¶we‚Äôre really giving you free volumes\n\nThe lede is ‚Äúfree Postgres‚Äù because that‚Äôs what matters to full stack apps. You don‚Äôt have to use these for Postgres. If SQLite is more your jam, mount up to 3GB of volumes and use ‚Äúfree SQLite.‚Äù Yeah, we‚Äôre probably underselling that.\n\nIt‚Äôll take just a few minutes to add free Postgres to your Fly.io app.\n\n\n## A note about credit cards\n\nEven for our free services, we require a credit card number. We know that‚Äôs the worst and it gives you heartburn. It‚Äôs not because we plan to charge you.\n\nBut here‚Äôs what happens if you give people freemium full access to a hosting platform: lots and lots of free VMs mining for cryptocurrencies.\n\nWe could tell you we want to prevent crypto mining because we care about the planet, and that would be true. We also have a capitalism nerve that hurts when people spend our money gambling. Your credit card number is the thin plastic line between us and chaos."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/self-service-account-deactivation/",
    "content": "Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a post about deactivating accounts on Fly.io, but if you‚Äôre new, you might like to try us out first; you can be up and running in just a couple of minutes.\n\nWe‚Äôve just introduced a self-service UI for deleting your Fly.io account.\n\nFrom time to time people ask us to deactivate their account. It happens! Maybe you just tried a walkthrough out of interest, or you created a new account with a different name, or (snif) you‚Äôre moving your app to (snif) another platform. Whatever the case, until recently, to get your account deactivated, you had to email support.\n\nWhy was this? At first glance it seems simple. Just build a self-service ‚Äúdelete everything‚Äù button. However, to bring this feature to life, we need to consider Factors. Users share organizations, and have apps running. You need to understand what you‚Äôre doing, and then make a decision, and our account deletion features need to help you with that.\n\nIt comes down to this: if we‚Äôre not careful, the easier we make it to deactivate your account, the easier we make it to accidentally destroy a bunch of infrastructure in one dumb click. We‚Äôre determined not to fall into that trap.\n\nLet‚Äôs look at what we came up with:\n\nFirst, we need to delete your apps. Once your account is deleted, these apps are gone; you can‚Äôt go back and fix things. We want to make sure you‚Äôre not accidentally abandoning anything important. Also, we allocate resources to each app you own, so if you‚Äôre breaking up with us, we‚Äôll take our volume storage space, mix tapes, and favorite sweater back.\n\nIt gets trickier. If you‚Äôre migrating to (snif) another provider, there‚Äôs more homework to take care of. When you point a domain to a Fly.io app you‚Äôll have configured certificates, and when leaving you probably need to change some DNS records.\n\nFinally, in your time here you might‚Äôve created an organization, and then invited friends (please do). So you‚Äôre an admin of shared apps now. Given you‚Äôre the admin, you probably want to either delete your organization or transfer it to someone else, so we will carefully remind you of that before leaving.\n\nWith all this in mind, we built a UI to guide you through these steps without having to look it all up in documentation. Following the links should bring you to the right page for each task.\n\nWhether you‚Äôre just tearing down a test account or migrating an app off out platform, this should be all you need. Most of the time, you probably only have to delete an app or two, and then you‚Äôre good to go.\n\nThat being said, there are still some cases where we can‚Äôt automatically deactivate your account. If you‚Äôre sharing organizations or are the admin of someone else‚Äôs organizations, we can‚Äôt automatically zap your account. At least for now, you will need a little help to get going in this case. The UI will let you know if it detects that you would just get stuck on the organization step.\n\nIf you‚Äôre excited about this new feature, we want to hear from you about how we can make Fly.io the platform you‚Äôre excited to try again in the near future. Reach out at our community forum."
  },
  {
    "title": "Launch your Livebook now",
    "url": "https://fly.io/blog/creating-the-livebook-launcher-in-liveview/",
    "content": "Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a post about writing our LiveView launcher that launches a Livebook instance for you on Fly.io. If you‚Äôre more interested in deploying your own project, the easiest way to learn more is to try it out; you can be up and running in just a couple minutes.\n\nThe Livebook launcher was super fun to create. Everything happens in a single LiveView process. There was a problem though‚Ä¶ it wasn‚Äôt great if we were in the middle of deploying the app for the user and one these things happened:\n\n\n## The Problem\n\nThe problem was there are generated secrets like liveview_password. Once we send that off to have our Livebook deployed, we could never get those values back! They are stored in Vault and even we are protected from being able to access secrets stored there. After all, in cases like this, they aren‚Äôt our secrets!\n\nThis presented a UX problem. We had private data that was only temporarily stored in a LiveView process. If anything went wrong with the user‚Äôs connection or the server was restarted, the UI would lose the secrets forever! Part of the UI design is to show all the information to the user at the end of the process.\n\nHow could we do that in a situation like this? I considered using a database or a Redis server to store that state. I didn‚Äôt like it though because we‚Äôre trying not to keep that data around on our servers.\n\n\n## The Solution\n\nThe solution was to encrypt the private data we want to keep using Phoenix.Token.encrypt/4. This not only signs the data to prevent tampering but it also encrypts the contents keeping them private. Then we could store that encrypted text blob in the browser‚Äôs SessionStorage. That way is stays under the users‚Äôs control. This also means they are available to be pushed back up to the server!\n\nLiveView has some powerful features. One is called hooks and they let the server and client communicate in both directions. I defined some simple hooks in a Javascript file like this.\n\nIf a user reloads the page, the server sends a push_event command over the websocket to the browser asking it to \"restore\" any settings it had under the ‚Äúlivebook‚Äù key in the SessionStorage. If the client has something to send, the server gets back the data, decrypts it using Phoenix.Token.decrypt/4, and resumes tracking the user‚Äôs deployment progress.\n\nLikewise, if the server is restarted when we deploy a new version, the client provides the private data and after a little UI catch up, it continues tracking the progress of the deploy.\n\nThat‚Äôs pretty much all the javascript I had to write do deliver this feature!\n\nAfter the deployment succeeds or fails, we \"clear\" the cached data from the browser‚Äôs SessionStorage just to keep things tidy. Also, when the user closes their browser tab everything in their SessionStorage is cleared out as well.\n\nHope that sheds some light on what‚Äôs happening and gives you some ideas for solving similar problems!\n\nDeploy your Livebook instance now. Hit refresh during the deployment process and see it recover."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-safari-ruined-my-tuesday/",
    "content": "Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a post about a bug in Safari, but if you just want to ship a Phoenix app, the easiest way to learn more is to try it out; you can be up and running in just a couple minutes.\n\nSafari 15 shipped in September 2021 and included an obscure CSS bug that broke most LiveView applications. The LiveView client operates in two modes ‚Äì connected, when it can talk to the server over websockets, and disconnected when offline. When LiveView disconnects, it‚Äôs useful to communicate the change to users. CSS includes a handy pointer-events attribute that makes buttons and links flip to non-interactive. Set pointer-events: none, and elements won‚Äôt get a pointer cursor or visibly respond to clicks.\n\nCSS cascades (it‚Äôs literally in the name!) so we can make a bunch of elements non-interactive at once by adding this rule, then toggling a .phx-disconnected class on container elements:\n\nThis has been part of the default app.css in LiveView apps for three years. So we‚Äôre some of the first to feel the pain of a Safari 15 bug that makes pointer-events: none stick ‚Äì forever. It stays none even if you update classes or explicitly overwrite pointer-events. Which means, LiveView stays non-interactive for Safari users even when we reconnect.\n\nIn the whole of the internet, there‚Äôs only one, lonely Stack Overflow thread talking about this issue affecting mobile Safari. And it includes tiny HTML and CSS example reproduction. You can see the bug in action below. The link remains in a none state an clicks aren‚Äôt registered when we remove the class.\n\nThis was an annoyingly difficult issue to debug. Safari‚Äôs web inspector will tell you the computed property of the elements is pointer-events: auto, but it‚Äôs lying. Elements won‚Äôt respond to click events. Bad times. It‚Äôs like living with IE6 again\n\nThere is a workaround for LiveView apps, though. Removing point-events: none; from app.css and carry on. LiveView already ignores on clicks on Phoenix links and buttons when disconnected, so pointer-events: none is mostly redundant. Phoenix LiveView 0.17 works around this issue ‚Äì¬†by using an entirely different class name for disconnected state.\n\nSadly, this isn‚Äôt an elegant breaking change. But it‚Äôs a fix that allows us to avoid applying the erroneous CSS rules from existing user-land CSS while still supporting custom styling of loading states. It looks like no bug is currently being tracked on the Webkit issue tracker, but the latest Safari Development Preview build might fix the underlying bug. Maybe even accidentally. Safari release cycles are quite slow, though, so a breaking CSS change in LiveView is the best way to reduce the end user impact.\n\nI spent my entire Tuesday tracking this one down. I‚Äôll be able to write Elixir again someday, right? üòÑ In the meantime, feel free to read a mildly frustrated commit message."
  },
  {
    "title": "Fly.io apps get routable IPv4 addresses.",
    "url": "https://fly.io/blog/32-bit-real-estate/",
    "content": "Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a post about one of the major costs of running a service like ours, but if you‚Äôre more interested in how Fly.io works, the easiest way to learn more is to try it out; you can be up and running in just a couple minutes.\n\nTwo obvious costs of running Internet apps for users on your own hardware: hardware and bandwidth. We buy big servers and run them in racks at network providers that charge us to route large volumes of traffic using BGP4 Anycast. You probably have at least a hazy intuition for what those costs look like.\n\nBut there‚Äôs a non-obvious cost to what we do: we need routable IPv4 addresses. To make Anycast work, so users in Singapore are routed to Singapore instances, particularly for non-HTTP applications, like UDP DNS servers or TCP game servers, we assign distinct public IPv4 addresses to each app running on Fly.io.\n\nYou don‚Äôt have to think about any of this stuff when you deploy on Fly.io. We just assign apps addresses and get out of the way. But we have to think a lot about this stuff, because IPv4 addresses cost money.\n\n\n## You can‚Äôt, like, own a number, man!\n\nLet‚Äôs take a second to reflect on what it means to acquire an IPv4 address. To own an IP address is to control what it routes to. The rock-bottom source of truth for IPv4 routing is BGP4. You announce IP address prefixes over IPv4 to your peers, your peers relay those announcements, and the world learns that traffic for your addresses needs to be sent your way.\n\nLots of providers will accept a BGP4 peering session with a customer. But no reputable provider will take a random announcement over that session. You‚Äôll need to demonstrate authority over the prefixes you plan to announce.\n\nTechnically, nobody owns an IPv4 address. They‚Äôre administered, ostensibly as a public benefit, by the 5 Regional Internet Registries ‚Äî ARIN, RIPE, APNIC, AFRINIC, and LACNIC. In the long long ago, it was the job the RIRs to allocate blocks of addresses to providers. Today, their IPv4 hives are void of honey, and their new job is just to keep track of who‚Äôs claiming which long-allocated blocks as they get shuffled around between their new, er, stewards.\n\nAll this is to say, if you want to ‚Äúown‚Äù an IPv4 address, you find its previous owner and get them to arrange a transfer with an RIR. There is paperwork involved, and it‚Äôs sometimes whispered, a usage justification.\n\nWith the transfer completed, your newly ‚Äúacquired‚Äù IPv4 addresses will be associated with your BGP4 ASN (oh, you‚Äôll need to shell out a couple hundred bucks for the ASN, too), and you can hope that an upstream provider will accept announcements for them. The addresses are now ‚Äúyours‚Äù.\n\nFor illustration: here‚Äôs a block of IPv4 addresses we control in IRR Explorer.\n\n\n## Certain Plans Require Additional Gribbles\n\nSo, Anycast. Easy. You have a routable IPv4 address. You have machines in a bunch of places. In each of those places, you peer BGP4 with your upstream and advertise that exact same IPv4 address. Global BGP4 routing ensures that when people try to connect to to that address, they‚Äôll be sent to the ‚Äúclosest‚Äù machine. That‚Äôs how Fly.io‚Äôs Anycast network functions: we have blocks of addresses, machines in a bunch of regions, BGP4 peering in each of them, and a Rust-based proxy that picks up connections for those addresses.\n\nYou can think of ways to work around needing routable IPv4 addresses for Anycast applications. None of them work for us.\n\nHere‚Äôs the first thing that doesn‚Äôt work: IPv6. Let‚Äôs get it out of the way first. Yes, there is such a thing as IPv6. Yes, it has enough address space to give distinct addresses to each electron in every atom of every living human. And, yes, we do a lot with IPv6, including allocating public IPv6 prefixes to all of our applications. We‚Äôre IPv6 fans.\n\nBut if I had to place a bet, it‚Äôd be that you aren‚Äôt using IPv6. And you‚Äôre nerdy enough to be reading a blog post on how much IPv4 addresses cost! The silent majority of non-nerd users are nowhere close to being reliably IPv6-connected.\n\nThe next thing that doesn‚Äôt work is name-based hosting. Applications can share IPv4 addresses using TLS SNI. We could park most apps on a single address and just let browsers tell our proxies which apps people are looking for. So far so good: the people who want their own IPv4 addresses would be flying business class, and we could charge them 2x as much.\n\nA word about market segmentation. As a software business operator, you have two basic goals: you want everyone to use your software, and you want to earn profits. Those goals are in tension. The classic b-school 101 answer is to identify the customers who derive the most value from your software. Now figure out their distinctive use cases, charge more for them, and less for everything else. I can keep explaining this poorly, but Joel Spolsky has already explained it well, and you should go read his post.\n\nThere‚Äôs a standard market-seg playbook for SAAS companies. You‚Äôve seen it every time you‚Äôve looked at a product pricing grid. Two of the most infamous segmentation premiums are HTTPS (thankfully, this is going extinct) and the SSO tax (still very much alive).\n\nWe‚Äôre not above market segmentation. If you‚Äôre a billion dollar company, rest assured, we will find ways to extract money from you! But IPv4 addresses aren‚Äôt one of those ways, because charging extra for them gets in everyone‚Äôs way. For what it‚Äôs worth: when enough people ask, we‚Äôll ship SSO. But we won‚Äôt tax it.\n\nWe like money as much as anyone else. But SNI breaks down for apps that don‚Äôt use TLS, and we want to support all kinds of apps, not just web apps. Now, most Fly.io apps are web apps. And we know which apps are which. So we could use SNI for the majority of our apps and IPv4 addresses for the exceptions. But that feels weird, and worse, means we‚Äôd be maintaining multiple provisioning paths for customers. It‚Äôs not worth the hassle, for us or our customers, so IPv4 addresses are just bundled in for everyone.\n\n\n## IPv4: The Internet‚Äôs Real Estate Market\n\nThere are lot of organizations holding addresses they don‚Äôt need. Internet routing used to be real, real dumb, and to give a big company enough addresses to number their machines, you often had to give them way more addresses than they really needed. Apple has a /8 ‚Äì¬†that‚Äôs 256 /16s, each of which has 256 /24s, each of which is 256 addresses. So does Prudential Insurance(?!), and so does Ford. Lots of other organizations had ‚Äúsmaller‚Äù blocks that are still unfathomably large to modern sensibilities. So, like a crappy cryptocurrency, IPv4 is pre-mined.\n\nIf you invested in IP addresses 15 years ago, you‚Äôre doing pretty well. If you invested in IP addresses 15 months ago, you‚Äôre also doing well.\n\nWhen we wrote the first draft of this post, the going rate for smaller IPv4 blocks¬†was $45 per IP address. was the rate in September of 2021. We looked just now, and the spot price was $50. We give up quoting them; by the time you get to this sentence, they‚Äôll have gotten more expensive. Here‚Äôs some recent transactions.\n\nDid you know big cloud providers publish their IP ranges? They do this because it‚Äôs useful for automating firewalls. It's¬†also¬†useful if you want to guess how much IPv4 wealth they‚Äôre sitting on. You could even¬†write a Ruby script¬†to tell you. Which I did. (Dan Goldin did the same thing 6 months before me).\n\nHere‚Äôs what I saw in March of 2021 when IPv4s cost $25 each:\n\nAnd here‚Äôs what it looked like in September of 2021:\n\nThese estimates are low: Amazon announces more than 100 million IP addresses. That‚Äôs 2.4% of all possible IPv4 addresses!\n\nIP prices vary by block size. It‚Äôs helpful to think of a 256-address /24 as the irreducible block size. That /24 is 10-15% less expensive than a /18, which is 64 contiguous /24s. There are reasons for the price difference, and we don‚Äôt know all of them. One big reason is that if you‚Äôre doing classical network engineering and you need to number 10,000 hosts, the routing is easier to do with a /18 than with 40 /24s. Another reason is probably vanity.\n\nA fun, useless fact: time was when there were particularly valuable /24s. Backbone routing was done principally with Cisco 7500 routers, which used TCAM memory ‚Äî think ‚Äúhardware hash table‚Äù ‚Äî to make quick forwarding decisions. TCAM space was limited, and the Tier 1 network providers used BGP filters to enforce a maximum prefix length; for instance, in the late 1990s, Sprint wouldn‚Äôt forward anything longer than a /19, which meant in effect that if you wanted to be reachable to anyone through Sprint, you needed at least a /19 to play.\n\nBut because IPv4 allocation had been so janky in the early 1990s (it literally only worked on octet boundaries, so you either had a /24 or you had a /16, with nothing in between), there were legacy prefixes that had to be grandfathered in through the filters. These were known as ‚Äúswamp space‚Äù; a swamp /24 was announceable across the Internet, even though a commodity /24 wasn‚Äôt.\n\nNone of this matters anymore, and you can announce all the /24s you want today.\n\nAnyways, AWS‚Äôs average block comprises 18,000 IPv4s. Bonkers. Giant booksellers who sell cloud services as a side hustle don‚Äôt even bother with IPv6. Here is a complete list of the IPv6 addresses dig aaaa amazon.com returns:\n\n\n## Financial Network Engineering\n\nImagine for a moment that you‚Äôre us. Take a drink, eat an Italian beef sandwich, pet your dog. Now get apps working for developers in, say, twenty different cities. You can buy a beast of a server for $20k USD, chop it up into 500+ virtual machines, and build an API for turning those on and off very quickly. People are pretty happy to pay for virtual machine time; you‚Äôre almost there.\n\nVMs are mostly useful when they can talk to the Internet. Your developers need IP addresses. Say it costs $45 for a single IP address. That means it costs $23,040 to handle 512 virtual machines with IPv4 addresses. We‚Äôve now doubled our costs.\n\nBut hold up for a second.\n\nServers depreciate; that‚Äôs their job. That $20k server depreciates to $500 relatively quickly. But IP blocks are, at least for now, appreciating. Alarmingly.\n\nThis is a hard market to time. Nobody believes the Internet will be IPv6-only within the next few years. There are credible people who believe IPv4 addresses will be scarce and useful indefinitely. We might put money on you getting away with an IPv6-only app 20 years from now. But we‚Äôd have said that 20 years ago, too.\n\nSo, unfortunately, the smart thing to do if you own $1B worth of IPv4 addresses is to buy $1B more IPv4 addresses. Fortunately, everyone is starting to recognize this, so you have some flexibility in financing.\n\nIf you‚Äôre a farmer and you need a new tractor, you probably don‚Äôt just go buy it in cash. You go to a bank and get a loan, collateralized by the value of the tractor. If you‚Äôre a company with $500,000 in receivables, you don‚Äôt necessarily have to wait for your customers to remit payments; financial institutions will loan you money collateralized by your invoices. And, of course, when you buy a house, you live in it while you pay back a mortgage to the bank.\n\nYour local credit union won‚Äôt collateralize a loan on IPv4 blocks like it would for a car or tractor. IPv4 addresses are just numbers, man. Logan Paul can convince unsophisticated people to put a dollar value on completely useless numbers. But your bank isn‚Äôt so forward thinking, and the conversation putting a $50 value to a single 32-bit number isn‚Äôt going to be productive.\n\nBut startups have another option: venture debt. Banks will happily lend startups money based almost entirely on the credibility of a startup‚Äôs investors, and, transitively, their blog posts. When a company raises a $12MM round, they‚Äôll spend the next few weeks raising $3MM of debt from banks friendly with that firm.\n\nVenture debt is surprisingly simple. It has an interest rate, which can be pretty close to prime. It usually includes an option grant ‚Äì¬†the bank gets a small amount of equity (much less than employees get). And venture debt typically requires that you do all your banking with the bank that gave you the loan. Unlike raising a funding round, once you‚Äôve got investors, there isn‚Äôt much pitching involved in raising debt. Banks win or lose venture debt deals based on how much certain founders like their web interface. It‚Äôs not not the most important factor when choosing a bank.\n\nAll this is to say, you can in a sense take out a mortgage on a block of IP addresses.\n\nYou can run more than just an HTTPS app here; DNS servers, game servers, and media relays work too. If your app has a Docker container, you should be able to get it running here easily.\n\n\n## How much sense does this make long-term?\n\nIPv6 will be viable ‚Äì most people will be able to run apps with no IPv4 addresses ‚Äì someday. Could happen in 2 years. Far more likely to happen in 20.\n\nThe kinds of companies that benefit the most from a decisive transition to IPv6 tend to look a lot like us: new, small, minimal legacy infrastructure. Not coincidentally, those companies also tend not to have much market power. Strong incentive, weak market power: not exactly a recipe for change.\n\nThere are other reasons to think IPv4 addresses will retain value. Path dependence is one of them; .COM names are still pricey despite the universal acceptance of new TLDs. Switching costs are another; lots of very large networks are IPv4-only, some with significant numbers of nodes that can‚Äôt easily be dual-stack. It‚Äôs easy to imagine a world in which IPv6-only is viable, but IPv4 addresses remain scarce.\n\nStill, IPv4 will be obsolete some day. Addresses will increase in value and then one night, while we‚Äôre all sleeping, they‚Äôll become worthless.\n\nThis could be a knock against using a bunch of debt to pay for IP assignments, or not.\n\nAn alternative is leasing: companies of varying levels of sketchiness will happily lease IP blocks. We‚Äôve had a bunch of offers for two bucks per month per address. Maybe we could negotiate that down to one bucks. A three year lease for 65,000 IP addresses at $1/mo each works out to $2.34 million. Four years is $3.12 million.\n\nWe‚Äôd almost certainly get more addresses by leasing. And we wouldn‚Äôt have the inventory risk of holding thousands of IPv4 addresses that could conceivably be worth $0 if IPv6 takes over. But leases have fixed terms, and running apps don‚Äôt. Meanwhile, we do know empirically that people hate renumbering. So do the IPv4 lessors. We‚Äôd be in no position to negotiate a good rate on lease renewal ‚Äì in fact, we‚Äôd be in a bad position even if IPv4 addresses got much cheaper. We want to know that when we assign an address to an app, the people running that app will get to keep using that address indefinitely. Leasing isn‚Äôt worth the hassle for us.\n\n\n## IPv4 is exhausting\n\nAt scale, routable IPv4 addresses are surprisingly expensive. But they‚Äôre appreciating. Like an office building or a house, we have tools for financing them that we don‚Äôt for machines or salaries. So, part of our business is acquiring addresses outright, and attaching them to apps people run here.\n\nFor most ordinary apps, this doesn‚Äôt matter much; HTTPS web applications would run just as well with SNI and shared (or temporary) addresses as they do with permanent IPv4 allocations. But not all apps are ordinary, and we want the oddball apps to work well on Fly.io without anyone having to think much about them.\n\nMostly, we just believe that people who run apps on Fly.io now will still be running them in 10 years, and Fly.io is better if we can keep that IP for them the entire time.\n\nReally, though, we‚Äôre nerds, and we think it‚Äôs funny that for all the talk of NFTs and ICOs, the Internet itself has been running a high-dollar token auction for the last 20 years.\n\nOne last thing: a new 5-digit ASN will cost you about $500 from ARIN, but there are auctions for 4-digit ASNs, and they run into mid-five-figures. If any of you can explain this to us, we‚Äôd be grateful."
  },
  {
    "title": "Phoenix screams on Fly.io.",
    "url": "https://fly.io/blog/how-we-got-to-liveview/",
    "content": "I‚Äôm Chris McCord. I work at Fly.io and created Phoenix, an Elixir web framework. Phoenix provides features out-of-the-box that are difficult in other languages and frameworks. This is a post about how we created LiveView, our flagship feature.\n\nLiveView strips away layers of abstraction, because it solves both the client and server in a single abstraction. HTTP almost entirely falls away. No more REST. No more JSON. No GraphQL APIs, controllers, serializers, or resolvers. You just write HTML templates, and a stateful process synchronizes it with the browser, updating it only when needed. And there‚Äôs no JavaScript to write.\n\nDo you remember when Ruby on Rails was first released? I do. Rails was also a revolution. It hit on the idea of using an expressive language and a few well-chosen, unifying abstractions to drastically simplify development. It‚Äôs hard to remember what CRUD app development was like before Rails, because the framework has been so influential.\n\nToday, I work in a language called Elixir. I spend my days building Phoenix, which is Elixir‚Äôs goto web framework. Unlike Rails, Phoenix is more than just an Elixir web framework. In the process of building Phoenix, I believe we‚Äôve hit on some new ideas that will change the way we think about building applications in much the same way Rails did for CRUD apps.\n\nThat‚Äôs a big claim. To back it up, I want to talk you through the history of Phoenix, what we were trying to do, and some of the problems we solved along the way. There‚Äôs a lot to say. I‚Äôll break it down like this:\n\nLet‚Äôs get started.\n\n\n## 2013: Ruby on Rails\n\nI think it‚Äôs safe to say that there wouldn‚Äôt be a Phoenix framework if I‚Äôd gotten the job I applied for at 37signals in 2013. I liked building with Rails, and I‚Äôd doubtless be working on new Rails features instead of inventing Phoenix.\n\nThat‚Äôs because Rails is an amazingly productive framework for shipping code. It‚Äôs about the flow. When you build something the Rails team had in mind, the flow is powerful and enjoyable. Rails people have a name for this; they call it ‚ÄúThe Golden Path‚Äù.\n\nBut when you need ‚Äúreal-time‚Äù features, like chat or activity feeds, you‚Äôre off the Golden Path and out of the flow. You go from writing Ruby code with Ruby abstractions to Javascript and a whole different set of abstractions. Then you write a pile of HTTP glue to make both worlds talk to each other.\n\nSo, I had a simple idea: ‚Äúwhat if we could replace Rails‚Äô render with sync and the UI updates automatically?‚Äù. A few days later, I had sync.rb.\n\nSync.rb works like this: the browser WebSockets to the server, and as Rails models change, templates get re-rendered on the server and pushed to the client. HTML rendered from the server would sign a tamper-proof subscription into the DOM for clients to listen to over WebSockets. The library provides JavaScript for the browser to run, but sync.rb programmers don‚Äôt have to write any themselves. All the dynamic behavior is done server-side, in Ruby.\n\nThis actually worked! Kind of.\n\nSync.rb provides a concurrent abstraction, and Rails wasn‚Äôt especially concurrent. So I built on top of EventMachine, which is a Ruby/C++ library that runs an IO event loop. EventMachine is a useful library, but it has an uneasy relationship with core Ruby and its ecosystem, most of which doesn‚Äôt expect concurrent code.\n\nMy EventMachine threads would silently die without errors. I had to check if the EventMachine thread had secretly died, and restart it, for every call, for every user, every time we wanted to async publish updates. I wanted to build dynamic features in a stack I knew and loved, but I didn‚Äôt have the confidence the platform could deliver what I was trying to build.\n\nStill, it worked. I had a minimal viable demo, and knew the approach could work if I could find a way to make it reliable. I looked to see how other languages addressed the ‚Äúreal-time‚Äù problem at scale. I found Elixir.\n\nElixir is Jos√© Valim‚Äôs developer-focussed language built on the Erlang VM. Erlang is a battle-tested language designed for telecom applications, notable for the quality and reliability of the virtual machine it runs on. Erlang powered WhatsApp, which served 10 million users per server. The $19B Facebook acquisition was a nice calling card as well.\n\nOne look at Elixir and I was instantly hooked. I saw all the power and heritage of the Erlang VM, but also modern features like protocols, the best build tool/code runner/task runner I‚Äôd ever seen, real macros, and first-class documentation. All it needed was a web framework.\n\n\n## 2014-2015: Phoenix\n\nI created Phoenix to build real-time applications. Before it could even render HTML, Phoenix supported real-time messaging. To do that, Phoenix provides an abstraction called Channels.\n\nTo understand Channels, you need to know a few things about Erlang. Concurrency in the Erlang VM is first-class. Erlang apps are comprised of ‚Äúprocesses‚Äù, which are light-weight threads that communicate with messages rather than shared memory. You can run millions of processes in a single VM. Erlang is famously resilient: processes can exit (or even crash) and restart seamlessly. Processes message each other with abstractions like mailboxes and PubSub; messages are routed transparently between servers.\n\nChannels exploit Elixir/Erlang messages to talk with external clients. They‚Äôre bidirectional and transport-agnostic.\n\nChannels are usually carried over WebSockets, but you can implement them on anything ‚Äì the community even showed off a working Telnet client.\n\nIn a web app, a browser opens a single WebSocket, which multiplexes between channels and processes. Because Elixir is preemptively scheduled, processes load-balance on both IO and CPU. You can block on one channel, transcode video on another, and the other channels stay snappy.\n\nThis is starkly different from how Rails applications work. Rails uses ‚Äúprocesses‚Äù too, but they‚Äôre the ‚Äúheavy‚Äù kind. Generally, a Rails request handler monopolizes a whole OS process, from setup to teardown. Rails is ‚Äúconcurrent‚Äù in the sense that the underlying database is concurrent, and multiple OS processes can update it. This complicates things that should be simple. For instance, running a timer that fires events on an interval requires a background job framework, and a scheme for relaying messages back through persistent storage. It‚Äôs a level of friction that changes and limits the way you think about building things.\n\nIn Phoenix, Channels hold state for the lifetime of a WebSocket connection, and can relay events across a server fleet. They scale vertically and horizontally.\n\nIn November 2015, we put the Elixir/Erlang promise to the test. We load-tested Channels, sending 2 million concurrent WebSocket connections to a single Phoenix server, broadcasting chat messages between all the clients. It took 45 client servers just to open that many connections. Our 128GB Phoenix server still had 45GB of free memory. We‚Äôd have run more clients, but we stopped when we ran out of file descriptors supported by our Linux kernel.\n\nPhoenix did what sync.rb couldn‚Äôt. Millions of concurrent websocket users with trivial user land code. We knew we had the foundation for a developer friendly real-time application framework. We just needed to figure out how to make it sing.\n\n\n## 2016-2017: Presence\n\nIn 2016, developers didn‚Äôt usually think they were building real-time applications. ‚ÄúReal-time‚Äù generally made people think of WhatsApp and Twitter and Slack. Many didn‚Äôt have a firm idea of why they‚Äôd even want real-time, or the costs were too high to implement such things. Still, everyone had a need for standard web apps. The core team spent a lot of time making Phoenix a great MVC framework for building conventional database based web apps. But my head was still in real-time features.\n\nSo we released Phoenix Presence, a distributed group manager with metadata backed by CRDTs. Presence broadly solves the ‚ÄúWho‚Äôs currently online?‚Äù feature.\n\nImagine your boss asks you to display how many other visitors are viewing a nearly sold out item to convert sales, or your client asks how hard it would be to show which team members are currently online. You no longer have to figure these things out. Apps become more interesting with live presence features. In fact, most collaborative apps get more interesting with this feature, even if they aren‚Äôt ‚Äúreal-time‚Äù in any other way.\n\nThere are two important things to understand about Presence. The first is that it just works, with self-healing and without dependencies on external message queues or databases. Elixir has all this built in and Presence takes full advantage. You don‚Äôt have to think about how to make it work.\n\nThe second is that Presence exploits a powerful, general abstraction. CRDT-based statekeeping has applications outside of online buddy lists. Presence gets used in IOT apps to track devices, cars, and other things. If you don‚Äôt have something like Presence, you probably don‚Äôt think to build the kinds of features it enables. When you do have it, it changes the frontiers of what you can reasonably accomplish in an application.\n\n\n## 2018: LiveView\n\nIt‚Äôs 2018. We‚Äôve had multiple minor point releases under our belt. Two Phoenix books have been printed, and the community is growing and happy.\n\nI was thrilled with Channels. They‚Äôre a fantastic abstraction for scalable client-heavy applications, where the server is mostly responsible for sending data and handling client events. They‚Äôre still the Phoenix go-to for JavaScript and native applications.\n\nBut I knew I wasn‚Äôt all the way there yet. Phoenix still relied on SPA libraries for real-time UI. But I‚Äôd moved on from that architecture, and internalized a new way of building applications in Elixir.\n\nThink about how a normal web application works. Everything is stateless. When a user does something, you fetch the world, munge it into some format, and shoot it over the wire to the client. Then you throw all that state away. You do it all over and over again with the next request, hundreds or thousands of times per second. It worked this way in PHP and Perl::Mason, and it still works this way in modern frameworks like Rails.\n\nBut Elixir can do stateful applications. It allows you to model the entire user visit as a cheap bit of concurrency on the server, and to talk directly to the browser. We can update the UI as things happen, and those updates can come from anywhere - the user, the server, or even some other server in our cluster.\n\nYou don‚Äôt want to build applications in Elixir the way you would in other frameworks.\n\nI had a glimpse of how stateful UI applications worked on the client-side with React.js. We borrowed that React programming model and moved it to our Elixir servers.\n\nWith React, any state change triggers a re-render, followed by an efficient patch of the browser DOM. LiveView is the same, except the server re-renders the template and holds the state. Where React templates annotate the DOM with client events, such as \u003cbutton onClick={this.clicked()}, in LiveView it‚Äôs RPC events, like \u003cbutton phx-click=\"clicked\"\u003e . With React, you‚Äôre still context switching and gluing things with HTTP and serializers. Not with LiveView. Interactive features are friction-free.\n\nPhoenix is a win anywhere. But Fly.io was practically born to run it. With super-clean built-in private networking for clustering and global edge deployment, LiveView apps feel like native apps anywhere in the world.\n\nThis, again, alters the frontiers of what you can reasonably do in an application. Elixir applications don‚Äôt need to arrange ensembles of libraries and transfer formats and lifecycles to build dynamically updating UI; dynamic server rendering is built in. Once you acclimate to having serverside-stateful UI, you look at your applications differently. There‚Äôs no SPA complexity tax to pay for making features dynamic, and so whatever makes sense to be dynamic gets to be dynamic.\n\nStill, at this point, we had only a worse-is-better approach compared to SPA alternatives. Any tiny change re-rendered the entire template. The server not only re-computed templates, but re-sent redundant data, usually just to patch a tiny part of the page. To make LiveView the first tool developers picked up off the shelf, we needed it to scale better.\n\n\n## 2019-2020: LiveEEx\n\nPart of the magic of React is that it‚Äôs a powerful abstraction with an efficient implementation. When a small piece of the state underlying a React app changes, React constrains the update, and diffs it into place, minimizing changes to the DOM.\n\nThrough Jos√© Valim‚Äôs work on the LiveEEx engine, Phoenix LiveView templates became diff-aware. They extract static and dynamic data from the template at compile-time. We compute a minimal diff to send to the client of only the dynamic data that has changed.\n\nThe abstraction doesn‚Äôt change. You write the same code you did before. But now we send a payload better optimized than the best hand-written JSON API. It sounds too good to be true. Let‚Äôs run through a tiny template to understand how this checks out.\n\nImagine we‚Äôre building the interface for a thermostat with LiveView. We‚Äôll call it ‚ÄúRoost‚Äù. Roost can display the current temperature, and you can bump it up and down with UI controls.\n\nThe first time you load the Roost interface it send a regular HTML response with your page. Next, a WebSocket connection is established, and LiveView sends an initial payload to the browser with the static and dynamic parts of the template split out:\n\nThe browser now has a cache, not just of the template but of the state data that backs it. And the browser knows how to merge diffs from the server. UI updates are easy. LiveView‚Äôs browser code zips the static values with the dynamics to produce a new string of HTML for the template. It hands that string off to morphdom to efficiently patch only those areas of the DOM that have changed.\n\nSay a houseguest wants it warmer. They click the \u003cbutton phx-click=\"inc\"\u003e+\u003c/button\u003e element. This wires up to an RPC function on the server which only increments @val:\n\nThe server propagates the update. But not much changed! Up above, you might have noticed that we delivered the current temperature (a dynamic state variable) under the 2 key in our initial state. That‚Äôs all we need to update; we just send {2: 70}to browsers. Here‚Äôs what happens:\n\nWars have been fought over encoding formats for application state updates. Here we‚Äôve sidestepped the controversy. A variable changes, we recompute linked templates, and the result goes on the wire.\n\nNow, someone has left a window open, and the temperature is changing. The server notices, and lets clients know about it:\n\nEach LiveView process picks up the broadcast:\n\nEveryone‚Äôs browser receives another tiny {2: 68} payload, regardless of which server they are connected to. We also don‚Äôt execute other template code like \u003c%= strftime(@time, \"%r\") %\u003e, because we know it didn‚Äôt change.\n\nOf course, it‚Äôs not just numbers. Let‚Äôs build an online dictionary, and let‚Äôs give it autocomplete. Bear with me, this is neat.\n\nHere‚Äôs a simple HTML form:\n\nWhen a user types something, we fire the suggest event, along with the contents of the text field. Here‚Äôs a handler:\n\nWe update matches for this connection, which updates the datalist in the template. What falls out of the diffing engine is only the new strings! And just as importantly, we don‚Äôt think about how that happens or make arrangements in advance; it just works.\n\nHere‚Äôs a problem that‚Äôs familiar to lots of people who‚Äôve built dynamic server rendered applications. You want to provide CSS feedback when users click a button. But the server can, at any point, be sending the client unrelated updates. Those in-flight update wipes out the CSS loading states you wanted for feedback. It looks like the server handled the click, but because of the race condition, the UI is lying. This leads to confusing and broken UX.\n\nThe Phoenix team solved this by annotating the DOM with phx-ref attributes that we attach to client diff payloads. These refs indicate that certain areas of the DOM are locked until that specific message ref is acknowledged by the server. All of this happens automatically for the user.\n\nFeatures like this aren‚Äôt glamorous, but they‚Äôre the difference between a proof of concept for dynamic real-time applications and an actual, working application.\n\n\n## 2021: LiveView Uploads, Redirects, and HEEx\n\nFirst, we shipped binary uploads. Uploads use the existing LiveView WebSocket connection. You get trivial interactive file uploads, with file progress and previews. Once again, you don‚Äôt have to think about how to make it work; it just does.\n\nWe also landed on a solution that allows you to perform direct-to-cloud uploads with the same code. You can start simple and upload direct-to-server, then shift to cloud uploads when that makes sense. Let‚Äôs see it in action:\n\nThe code that makes this happen is shockingly small. Above, we have file progress, file metadata validation, interactive file selection/pruning, error messages, drag and drop, and previews. This takes just 60 lines.\n\nYou don‚Äôt have to understand all that code, but I want to call attention to this:\n\nNotice how the declarative allow_upload API allows you to specify what uploads you want to allow, then the template defined in render/1 is reactive to any updates as they happen.\n\nUploading over WebSockets is neat. We‚Äôre absolutely certain that the file landed on the same Elixir server our visitor initially randomly load-balanced to. This allows the LiveView process (your code) to do any post-processing with the local file right there on the same box. You can watch a live coding upload deep-dive here.\n\nWe also shipped a live_redirect and live_patch feature which allows you to navigate via pushState on the client without page reloads over the existing WebSocket connection. This might look like pjax or turbolinks. But it‚Äôs not. Navigation happens over WebSockets. There‚Äôs no extra HTTP handshake, no parsing authentication headers, no refetching the world. The UX is faster and cleaner than an SPA framework, latency is reduced, and it‚Äôs easier to build.\n\nYou may have also noticed the ~H Elixir sigil in the previous examples. This is HEEx, and it‚Äôs new. It‚Äôs an HTML-aware template syntax, including validation at compile-time. Like React JSX, components can be expressed as custom tags. It looks like this:\n\nThe HEEx engine extends standard Elixir EEx templates with tag syntax, such as \u003cAvatar.icon for={@user} ‚Ä¶ which internally compiles to a \u003c%= component \u0026Avatar.icon/1, for: @user %\u003e macro call. It‚Äôs structured markup and it composes nicely with HTML.\n\nWith the base HEEx engine in place, we foresee a budding Elixir ecosystem of off-the-shelf components for building out LiveView applications. Watch this space while we continue to extend our HEEx engine with features like slots and declarative assigns for compile-time checks.\n\n\n## The Future\n\nWe are excited to take LiveView, quite literally, around the globe. Fly.io makes it easy to deploy your LiveView processes close to users ‚Äì¬†and when LiveView is close to your users, interactions are immediate. We‚Äôre already exploring optimistic UI on the server, where we can front-run database transactions with Channels, reflect the optimistic change on the client, and reconcile when database transactions propagate. Much of this is already possible today in LiveView with a little work, but with a turn-key global deployment under our feet, the Phoenix team can really dig in and explore making these kinds of previously unheard of ideas the status quo for day to day LiveView applications. Stay tuned!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-io-is-hiring-full-stack-developers/",
    "content": "Fly.io makes it easy to host applications worldwide the same way a CDN hosts HTML pages. Our users ship us containers, and we transmute them into Firecracker micro-vms that run on our hardware in data centers around the world. The easiest way to learn more is to sign up; if you‚Äôve got a working container now, it can be running in Sydney, Chennai, or Amsterdam in just a few minutes.\n\nWe‚Äôre working on super fun problems and are looking for more people to join us. In particular: we‚Äôre looking for full-stack developers. We expect this will be a good role for early-to-mid-level career developers.\n\n\n## Some Background\n\nFly.io‚Äôs users interact with us almost entirely with flyctl, our CLI interface. We are a CLI-first team. But our UI and UX work is starting to get more ambitious. And that‚Äôs what ‚Äúfull-stack‚Äù means here: the parts of our product that users interact directly with, particularly when those parts have user interfaces.\n\nOur UX stack today is a combination of Rails, GraphQL, and some Go (our platform stack is Rust and Go). But a few months ago, we fell in with a bad crowd, and got ourselves a little hooked on Elixir (Elixir is easy to love, but it also really sings on Fly.io). So there‚Äôs Elixir in our plans as well; probably more new Elixir than Rails.\n\nWe don‚Äôt care if you‚Äôre a Rails or Elixir expert, or if you have a mile-long resume. All we really care about is whether, if we give you a problem that involves reading or writing some Rails or Elixir code, you can marshal the resources needed to solve it. If that involves having Programming Elixir open in front of you while you type your first ever Elixir symbols into an editor, that‚Äôs fine, as long as your work holds up!\n\n\n## The Part Where We Sell The Role\n\nThere are lots of different dev jobs out there and different gigs are good fits for different people. Here‚Äôs a stab at some things that might indicate this role is a good fit for you:\n\nSome other things you‚Äôd want to know about us:\n\n\n## How We Hire People\n\nWe are weird about hiring. We‚Äôre skeptical of resumes and we don‚Äôt trust interviews (we‚Äôre happy to talk, though). We respect career experience but we aren‚Äôt hypnotized by it, and we‚Äôre thrilled at the prospect of discovering new talent.\n\nThe premise of our hiring process is that we‚Äôre going to show you the kind of work we‚Äôre doing and then see if you enjoy actually doing it; ‚Äúwork-sample challenges‚Äù. Unlike a lot of places that assign ‚Äútake-home problems‚Äù, our challenges are the backbone of our whole process; they‚Äôre not pre-screeners for an interview gauntlet.\n\nWe don‚Äôt time candidates or look over their shoulders. We‚Äôre not interested in gotchas; we want you to show yourself in your best light, and we want the environment you work in to be comfortable and realistic (every one of us has 90 Google tabs open looking up random programming stuff).\n\nAs a heads up, here‚Äôs some stuff you‚Äôd want to be comfortable getting yourself into gear to work on:\n\n\n## If You‚Äôre Interested\n\nShoot us an email at jobs+fullstack@fly.io. You can tell us a little about yourself if you like. Either way, tell us a short story about the last language or framework you picked up that you didn‚Äôt enjoy (maybe you like it now, though).\n\nWe‚Äôll get back to you with an opportunity to connect with us on a call at your convenience (we don‚Äôt do phone screens, but we will certainly take the time to answer any questions you have) and details about our work sample challenge."
  },
  {
    "title": "If this was a post about deploying on Fly.io, you‚Äôd have been done 22 minutes ago.",
    "url": "https://fly.io/blog/api-tokens-a-tedious-survey/",
    "content": "We‚Äôre Fly.io. This post isn‚Äôt about Fly.io, but you have to hear about us anyways, because my blog, my rules. Our users ship us Docker containers and we transmute them into Firecracker microvms, which we host on our own hardware around the world. With a working Dockerfile, getting up and running will take you less than 10 minutes.\n\nThis is not really a post about Fly.io, though I‚Äôll talk about us a little up front to set the scene.\n\nThe last several weeks of my life have been about API security. I‚Äôm working on a new permissions system for Fly.io, and did a bunch of research into my options. We even recorded a podcast about it. I won‚Äôt leave you hanging and tell you right up front: we‚Äôre working on rolling out a Macaroon-based scheme, which you‚Äôll read more about later.\n\nThis post is long. You may be interested in just one kind of token. I‚Äôll make easy for you: here‚Äôs a table of contents:\n\nFly.io is an application hosting platform. Think of us as having a control plane that applications running on Fly.io interact with, and an API that our users interact with ‚Äî mostly through our CLI, flyctl. It‚Äôs that flyctl piece we‚Äôre talking about here.\n\nToday, Fly.io API access is all-or-nothing. Everyone has root credentials. What we want is fine-grained permissions. Here‚Äôs two big problems we want to solve:\n\nThis is the API job people generally refer to as IAM. There are a bunch of different ways to do the IAM job, and they‚Äôre all fun to nerd out about.\n\n\n## Let‚Äôs First Clarify Some Stuff\n\nWhat I‚Äôm interested in here is API security, for end-users; ‚Äúretail‚Äù security.\n\nThere‚Äôs a closely related API security problem I‚Äôm not talking about: inter-service authentication. Modern applications are comprised of ensembles of small services. Ideally, there‚Äôs a security layer between them. But nobody does retail API IAM with Kerberos or mTLS. If you want to read more about these approaches, I wrote a long post about them elsewhere.\n\nAnother related problem is federated authentication and single sign-on. Google, Apple, and Okta will give you tokens that map requests to identities on their platforms. Those token formats are relevant here, but I want to be clear that federated identity is not what I‚Äôm after.\n\nMost API security schemes boil down to a token that accompanies API requests. The tokens are somehow associated with access rules. The API looks takes the request, extracts the token, finds the access rules, and decides how to proceed.\n\nSome questions to keep in your brain as you read through this:\n\n\n## Let‚Äôs Take Passwords Off The Table\n\nIt‚Äôs 2021 and so I don‚Äôt need to tell you that having your API pass a username and password through HTTP basic authentication is a bad idea. Your tokens should look large and random, whatever they are.\n\n\n## Simple Random Tokens: Unsung Heroes\n\nHere is a token generator that, from a security perspective, is pretty hard to beat:\n\nKeep a table of random tokens, associate them with a table of users, and associate those users with allowed actions. You don‚Äôt need me to tell you how to do this; it‚Äôs simply how CRUD apps work.\n\nWhat you might need me to tell you is that this is a good way, even over the long term, to handle the IAM problem. Random tokens aren‚Äôt cryptographically scary. They‚Äôre easily revoked and expired. The accompanying permissions logic is clean and expressive; it‚Äôs just your API code.\n\nFrankly, the biggest knock against simple random tokens is that they‚Äôre boring. If you can get away with using them ‚Äî and most applications can ‚Äî you probably should. Give yourself permission by saying you‚Äôre doing it for security reasons. Security is a problem for all the fancy tokens I‚Äôm going to talk about from now on.\n\n\n## Platform Tokens\n\nAssume we‚Äôre trying to minimize the fraction of requests that have to hit the database. Mainstream web application frameworks tend to already have features that help with this.\n\nRails, for instance, has MessageVerifier and MessageEncryptor. Give them a bag of attributes and get back a tamper-proof (optionally encrypted) string, using HMAC-SHA2 and encrypt-then-MAC AES-CBC. Put the string in a cookie. The server only remembers a root secret, and can pull user data out of the cookie instead of the database. This is how Rails sessions work.\n\nPython frameworks have similar features, but also the excellent Python pyca/cryptography libraries, which include Fernet, which provides the same functions optimized for tokens.\n\nYou can‚Äôt generally use general-purpose user sessions for API tokens (the defining feature of an API token is that it doesn‚Äôt log out). But you can use the same features to build API tokens. Share the root secret among multiple services ‚Äî maybe that‚Äôs fine ‚Äî and microservices don‚Äôt have to rely on a central service.\n\nPlatform tokens are relatively simple, and can be stateless. What‚Äôs the catch? Well, you‚Äôre effectively using tokens as a database cache, and cache consistency is frustrating.\n\nRight off the bat, you‚Äôve lost the simplest form of token revocation. The whole premise is that you‚Äôre not validating tokens against the database, so now you have to come up with another way to tell if they‚Äôve been revoked. Without a standard protocol for renewing them, short-expiry tokens don‚Äôt work either.\n\nA pattern I‚Äôve seen a bunch here, and one that I kind of like, is to ‚Äúversion‚Äù the users. Stick a token version in the user table, have tokens bear the current version. To revoke, bump the version in the database; outstanding tokens are now invalid. Of course, you need to keep state to do that, but the state is very cheap; a Redis cache of user versions, falling back to the database, does the trick.\n\n\n## OAuth 2\n\nAll exhibits and addenda attached previously to the section on ‚ÄúPlatform Tokens‚Äù is hereby incorporated into this section and made a part thereof.\n\nBy design, OAuth is a federation protocol. Canonically, OAuth lets a 3rd party post a tweet with your account. That‚Äôs not the problem we‚Äôre trying to solve.\n\nBut OAuth 2.0 is popular and has bumped into every tedious problem you‚Äôre likely to encounter with tokens, and they‚Äôve come up with solutions of varying levels of grossness. You can, and lots of people do, draft off that work in your own API IAM situation.\n\nFor instance, OAuth 2.0 has a built-in solution for short-expiry tokens; OAuth 2.0 has a ‚ÄúRefresh Token‚Äù, which exchanges for ‚ÄúAccess Tokens‚Äù. Access Tokens are the ones you actually do stuff in the API with, and they expire rapidly. OAuth 2.0 libraries know how to use Refresh Tokens. And they‚Äôre easy to revoke, because they‚Äôre used less frequently and don‚Äôt punish the database.\n\nOAuth 2.0 Access Tokens are opaque strings, so you can do the same things with them that you would with a Platform Token (or just stuff a Platform Token in there).\n\nThe ‚Äúcryptography‚Äù in OAuth 2.0, such as it is, is simple. It gets tricky in standalone single-page applications, but so does everything else. I used to snark about people cargo-culting OAuth into simple client-server apps. Not anymore.\n\n\n## JSON Web Tokens\n\nA brief history lesson. We got OAuth, and apps could tweet on behalf of users, and God saw what he had made and it was good. Then someone realized that if you could post a tweet on behalf of a user, you could use that capability as a proof of identity and ‚Äúlog users in with Twitter‚Äù. The tweet itself became extraneous and people just used OAuth tokens that could, like, read your user profile as an identity proof.\n\nThis is a problem because the ability to read your user profile isn‚Äôt a good identity proof. You might grant that capability to applications for reasons having nothing to do with whether they can ‚Äúlog in with Twitter‚Äù to a dating app. People found a bunch of vulnerabilities.\n\nEnter OpenID Connect (OIDC). OIDC is the demon marriage of OAuth 2.0 and a cryptographic token standard called JWT. OIDC‚Äôs is unambiguous: it gives you an ‚ÄúIdentity Token‚Äù, JWT-encoded, that tells you who‚Äôs logging in.\n\nWe‚Äôre not so much interested in OIDC here, but the eldritch rituals that brought OIDC into being unleashed a horde of JWTs into the world, and that‚Äôs now a thing we have to think about.\n\nFrom a purely functional perspective, JWT isn‚Äôt doing much more than a Platform Token embedded in OAuth 2.0. But JWT is standardized, and ‚ÄúJSON encrypted with Fernet and embedded in OAuth Access Token‚Äù isn‚Äôt, and so a whole lot of dev UX has sprung up around JWT. So, unfortunately, JWT has really good ergonomics.\n\nWhat makes that unfortunate? JWT is bad.\n\nThis is not a post about why JWT is bad, though I do hope you come away from this agreeing with me. So I‚Äôll be brief.\n\nFirst, JWT is a design-by-committee cryptographic kitchen sink. JWTs can be protected with a MAC, like HMAC-SHA2. Or with RSA digital signatures. Or encrypted, with static-ephemeral P-curve elliptic curve Diffie Hellman. This isn‚Äôt so much a footgun as it is the entire Rock Island Arsenal deployed against your feet. If you‚Äôre an aficionado of crypto vulnerabilities, you almost have to love it. Where else outside of TLS are you going to find invalid curve point attacks?\n\nNext, the JSON semantics of JWT are not thoughtfully designed. JWT doesn‚Äôt bind purpose or even domain parameters to keys, and JWT libraries are written with the assumption that RSA and HMAC-SHA2 are just interchangeable solutions to the same problem. So you get bugs where people take RSA-signed JWTs and switch the JWT header from RS256 to HS256 (don‚Äôt even get me started on these names), and the libraries obliviously treat public signing keys as private MAC keys. Also, there‚Äôs alg=none.\n\nJWT is so popular that it has become synonymous with the concept of stateless authentication tokens, despite the fact that stateless tokens are straightforward without (and were in wide use prior to) JWT.\n\nThere‚Äôs a sense in which complaining about JWT is just howling at the moon, because it‚Äôs non-optional in OIDC, and OIDC is how Google and Apple implement single sign-on. Friend-of-our-dumb-podcast Jonathan Rudenberg has a good observation about this: if your application retains direct connectivity to (say) Apple, you can somewhat safely use OIDC JWT simply by dint of trusting the TLS connection you have to Apple‚Äôs servers; you don‚Äôt so much even need to care about the cryptographic misfeatures of the token itself.\n\n\n## Aside: Never, Ever SAML\n\nThere are rituals even demons won‚Äôt stomach. OIDC‚Äôs competitor is SAML, which is based on XML DSIG, which is a way of turning XML documents into signed tokens. You should not turn XML documents into signed tokens. You should not sign XML. XML DSIG is the worst cryptographic format in common use on the Internet. Take all the flaws JWT, including the extensive parsing of untrusted data just to figure out how to verify stuff. Mix in a DOM model where a single document could potentially have dozens of different signed subtrees, then add a pluggable canonicalization layer that transforms documents before they‚Äôre signed. Make it complicated enough that there is essentially a single C-language implementation of the spec that every SAML library wraps. You‚Äôre obviously not going to use to authenticate your API, but, in case you can‚Äôt tell, I‚Äôm getting some stuff out of my system here.\n\n\n## PASETO\n\nPASETO (rhymes with ‚Äúpotato‚Äù) is hipster JWT. I mean that in the nicest way. It has essentially the same developer UX as JWT, but tries to lock the token into modern cryptography.\n\nJWT is a cryptographic kitchen sink. PASETO is the smaller bathroom vanity sink. I‚Äôm critical here, because PASETO has, for some good reasons, done well among token nerds and doesn‚Äôt need my help.\n\nThere are today four versions, each of which define two kinds of token, a symmetric ‚Äúlocal‚Äù and an asymmetric ‚Äúpublic‚Äù. Version 1 uses ‚ÄúNIST-compliant‚Äù AES-CTR, HMAC-SHA2, and RSA. Version 2 has XChaPoly and Ed25519. Version 3 replaces RSA with a P-384 ECDSA. Version 4 replaces XChaPoly with XChaCha and a Blake2 KMAC. You can swap v4 with v4c to use CBOR instead of JSON. It‚Äôs a lot.\n\nMy issue with PASETO is that it‚Äôs essentially the same thing as JWT. You could almost build it from JWT, by adding some algorithms and banning some others.\n\nPASETO advocates for the now-accepted practice of versioning whole protocols rather negotiating parameters on the fly. That should be a powerful advantage. But PASETO has 8 versions, 4 of which are ‚Äúcurrent‚Äù, and I think part of the idea of protocol versioning that PASETO misses is that you‚Äôre not supposed to keep multiple versions flying around. Versions 3 and 4 are partly the result of a vulnerability (not a super serious one) Thai Duong found. PASETO libraries support multiple versions, in some cases dynamically. Kill the old versions!\n\nThe IRTF CFG is the IETF‚Äôs cryptography review board. For reasons I will never understand, the PASETO authors submitted it to CFRG for consideration. Never do this. In the thread, Neil Madden pointed out that it had managed to inherit JWT‚Äôs RSA/HMAC problem; all the PASETO versions now have an ‚ÄúAlgorithm Lucidity‚Äù warning telling people to make sure they‚Äôre strongly typing their keys.\n\nI don‚Äôt think this is PASETO‚Äôs fault so much as I think that the fundamental idea is an impossible trinity: cryptographic flexibility, cryptographic misuse-resistance, and Javascript-y developer UX.\n\nAlso: the ‚ÄúNIST-compliant‚Äù PASETO versions were an unforced error.\n\nI‚Äôm peeved by JSON tokens that authenticate bags of random user attributes alongside token metadata like issuance dates and audiences. Cryptography engineers hear me rant about this and scratch their heads, but I think they‚Äôre mostly thinking about OIDC JWTs that carry minimal data, and not all the weird JWTs developers cook up, where user data mingles with metadata. This, too, seems like an unforced error for me. So does the fact that a lot of this metadata is optional. Why? It‚Äôs important!\n\nStill, you‚Äôre far better off using PASETO than JWT. My take regarding PASETO is that if you use it, you should find real lucidity about whether you want symmetric or asymmetric tokens; they‚Äôre two different things with different use cases. Support just one version.\n\n\n## Protocol Buffer Tokens: The Anti-PASETO\n\nYou can get essentially the thing PASETO tries to do, without any of the downsides, just by defining your own strongly typed protocol format. David Adrian calls these ‚ÄúProtobuf Tokens‚Äù.\n\nAll you do is, define a Protocol Buffer schema that looks like this:\n\nPush all your token semantics into the Token message, and marshal it into a string with a first pass of Protobuf encoding. Sign it with Ed25519 (concatenate a version string like ‚ÄúProtobuf-Token-v1‚Äù into the signature block), stick the token byte string in the token field of a SignedToken, and populate the signature. Marshal again, and you‚Äôre done.\n\nThis two-pass encoding gives you two things. First, there‚Äôs only one way to decode and verify the tokens. Second, everything in the token is signed, so there‚Äôs no ambiguity about metadata being signed. The tokens are compact, easy to work with, and can be extended (Protocol Buffers are good at this) to carry arbitrary optional claims.\n\n\n## Authenticated Requests\n\nYou don‚Äôt need tokens at all. You can instead just have keys. Use them to authenticate requests. That‚Äôs how the AWS API works.\n\nWe tend to send normal HTTP requests to our APIs, and pass an additional header carrying a ‚Äúbearer token‚Äù. Bearer tokens are like bearer bonds, in that if you have your bear paws on them, it‚Äôs game over. Authenticated requests don‚Äôt have this problem.\n\nTo do this, you need a canonicalization scheme for your HTTP requests. The same HTTP request has multiple representations; we need to decide on just one to compute a MAC tag. This seems easy but was a source of vulnerabilities in early implementations of the AWS scheme. Just use AWS‚Äôs Version 4.\n\nCompute an HMAC over the canonicalized request (with AWS, you‚Äôd just use your AWS_SECRET_ACCESS_KEY) and attach the resulting tag as a parameter.\n\nGod help you, you could use X509 here and issue people certificates and keys they can use to sign requests, which is a thing Facebook apparently did internally.\n\nThere are nice things about authenticated requests. No bearer tokens, no bears. The biggest problem is logistical: it‚Äôs a pain to build request authenticating code, so, unless your app gets huge, the only way to talk to it will be with your official SDK that does all the request signing work.\n\n\n## Facebook‚Äôs CATs\n\nSo, here‚Äôs a cool trick. You‚Äôve got a bunch of services, like Messages and Photos and Presence and Ivermectin Advocacy. And you‚Äôve got a central Authentication service, to which both your services and your users can talk.\n\nAuthentication holds a root key. Messages comes on line, and makes (say) an identity-proving mTLS connection to Authentication. It‚Äôs issued a service key, which is HMAC(k=root, v=‚ÄúMessages‚Äù).\n\nNow a user ‚ÄúAlice‚Äù arrives. Authentication issues her a key. It‚Äôs HMAC(k=HMAC(k=root, v=‚ÄúMessages‚Äù), v=‚ÄúAlice‚Äù).\n\nSee what we did there? Messages doesn‚Äôt have Alice‚Äôs key. But her key is simply the HMAC of her username under the Messages key, so the service can reconstruct it and verify the message.\n\nYou can use a CATS-like construction to sign requests, or to sign a Protobuf Token (with HMAC or an AEAD, rather than Ed25519). You‚Äôre getting some of the decoupling advantage of public key cryptography. Messages requires only sporadic contact with Authentication, to enroll themselves and periodically rotate keys. That‚Äôs enough to authenticate requests from all comers, trusting that the only way Alice got her key was if Authentication OK‚Äôd it.\n\nWe don‚Äôt need 5,000 words to tell you how to get an application running close to users around the world, from Sydney to Amsterdam. All it takes is a working Dockerfile\n\n\n## Macaroons\n\nWe can go for a walk where it‚Äôs quiet and dry and talk about Macaroons.\n\nImagine a golden ticket for your service, an authenticated token permitting any action. It‚Äôs much too dangerous to pass around as a bearer token.\n\nNow imagine adding caveats to that golden token. You‚Äôre allowed only to read, not to write. Only for a single document. Only on a request from a specific IP, or on a session independently authenticated to a specific user ID. This attenuated token is much less dangerous. In fact, you might get it so locked down that it‚Äôs not even sensitive.\n\nWe exploit the same trick CAT uses to derives user keys. Start with your golden ticket and HMAC it under a root key. Now you want to make it read-only, so you add another message layer to the token, and you MAC that new layer, using the MAC tag of the previous layer as the key. The holder of the new token can‚Äôt work out the original MAC tag of the golden ticket; the token carries only the new chained MAC tag. But services have the root key and can re-derive all the intermediate values.\n\nMacaroons are a token format built around this idea. They do three big things.\n\nAttenuation: user can restrict tokens without talking to the issuing service. All the caveats must evaluate true. You can‚Äôt undo previous caveats with new ones. The service just knows about the basic caveat types and doesn‚Äôt need special-case code for all the goofy combinations users might want.\n\nConfinement: If you have the right caveat types, you can set it up so there are useful Macaroons that are safe to pass around, because they‚Äôre only (say) valid on a session under a particular mTLS client certificate, or at a particular time of day.\n\nDelegation: Macaroons have ‚Äúthird-party caveats‚Äù, which delegate logic to other systems. Third-party caveats are encrypted; users can see only the URL of a third-party service they can talk with to resolve them. The third-party system issues a ‚Äúdischarge Macaroon‚Äù, which is submitted alongside the original Macaroon to resolve the caveat.\n\nThese ideas synergize. You can delegate authentication to an IAM service, and then add additional service-specific access rules as first-party caveats. A revocation service verifies a user‚Äôs tokens; the rest of your system doesn‚Äôt need to know how revocation is implemented. The same goes for audit logging, and for anti-abuse.\n\nSometimes there‚Äôs so much beauty in the world I feel like I can‚Äôt take it, like my heart‚Äôs going to cave in.\n\nBut Macaroons are unpopular for good reasons.\n\nFirst: there‚Äôs a library ecosystem for Macaroons and it‚Äôs not great. No library can support all or even most of the caveats developers will want. So ‚Äústandard‚Äù Macaroons use an untyped string DSL as their caveat format and ask relying services to parse them.\n\nThey‚Äôre also clunky. With most of the previous formats, you can imagine slotting them into OAuth 2.0. But third-party caveats break that. Your Macaroon API will be fussy. Users might have to make and store the results of a bunch of queries to issue a real request.\n\nMacaroons rely on symmetric cryptography. This is good and bad. It radically simplifies the system, but means you have to express relationships between your services with shared keys. You have to do that with HS256 JWT too, of course, but unless you depart pretty radically from the Macaroon paper, you can‚Äôt get the public key wins, without coming up with something like CAT-caroons.\n\nIn practice, caveats can be tricky to reason about. It‚Äôs easy to write a loop over a set of caveats that bombs as soon as one evaluates false. But you can accidentally introduce semantics that produce caveats that expand instead of contract authority. You‚Äôve got code that wants to answer ‚Äúcan I do this?‚Äù questions by asking the database about a user ID, and you can write caveat constructions that do similar things, which is never what you want in a coherent Macaroon design.\n\nI have more to say about these problems! For now, though, it suffices to say that I spent many years beating the drum for Macaroons, and then I went and implemented them, and I probably won‚Äôt be beating that drum anymore. But where they work well, I think they probably work really well. My take is: if all three of attenuation, confinement, and delegation resonate with your design, Macaroons will probably work fine. If you skip any of the three, consider something else.\n\n\n## Biscuits\n\nFinally, there‚Äôs Geoffroy Couprie‚Äôs Biscuits. Biscuits are what you‚Äôd get if you sat down to write an over-long blog post like this one, did all the research, and then decided instead to write a cryptographic token to address the shortcomings of every other token.\n\nBiscuits are heavily influenced by Macaroons (Couprie claims they‚Äôre JWT-influenced as well, but I don‚Äôt see it). Like Macaroons, users can attenuate Biscuits. But unlike Macaroons:\n\nBiscuits rely on public key signatures instead of HMAC, which somewhat dampens the need for third-party caveats.\n\nRather than simple boolean caveats, Biscuits embed Datalog programs to evaluate whether a token allows an operation.\n\nBiscuits are incredibly ambitious.\n\nTo begin with, swapping out the simple cryptography in Macaroons for public key signatures isn‚Äôt an easy task. The cryptographic process of adding a caveat to a Macaroon is trivial: you just feed the MAC tag from the previous caveat forward as the HMAC key for the new caveat. But there‚Äôs no comparably straightforward operation for signatures.\n\nThe cryptography proposed for Biscuits started with pairing curve moon math. Keller Fuchs pulled them back to low-earth orbit with curve VRFs. Then they took a detour into blockchainia with aggregated Gamma-Signatures. Ultimately, though, Biscuit‚Äôs core cryptography came back to Earth with a pretty straightforward chaining of Ed25519 signatures.\n\nThe caveat structure of Biscuit tokens is flexible, probably to a fault, but formally rigorous, which is an interesting combination. It works by evaluating a series of signed programs (compiled and marshaled with Protocol Buffers). Services derive fact patterns from requests, like ‚Äúyou‚Äôre asking for cats2.webp‚Äù or ‚Äúthe operation you‚Äôre requesting is WRITE‚Äù. The tokens themselves include rules that derive new fact patterns, and checkers that test those patterns against predicates.\n\nHonestly, when I first read about Biscuits, I thought it was pretty nuts. If the proposal hadn‚Äôt lost me at ‚Äúpairing curves‚Äù, it had by the time it started describing Datalog. But then I implemented Macaroons for myself, and now, I kind of get it. One thing Biscuits get you that no other token does is clarity about what operations a token authorizes. Rendered in text, Biscuit caveats read like policy documents.\n\nThat‚Äôs I think the only big concern I have about them. I wonder whether taking real advantage of Biscuits requires you to move essentially all your authorization logic into your tokens. Even with Macaroons, which previously held the title for ‚Äúmost expressive token‚Äù, the host services were still making powerful choices about what caveats could be expressed in the first place. Biscuits strips the service‚Äôs contribution to authorization policy down to what seems like their constituent atoms, and derives all security policy in Prolog. I see how that could be powerful, but also how you‚Äôd kind of have to buy into it wholesale to use it.\n\n\n## Now What?\n\nHere‚Äôs a scorecard:\n\nBelieve it or not, with the exception of passwords and SAML, I think there‚Äôs something to like in all of these schemes.\n\nI continue to believe that boring, trustworthy random tokens are underrated, and that people burn a lot of complexity chasing statelessness they can‚Äôt achieve and won‚Äôt need, because token databases for most systems outside of Facebook aren‚Äôt hard to scale.\n\nA couple months ago, I‚Äôd have said that Macaroons are underrated in a different way, the way Big Star‚Äôs ‚Äú#1 Record‚Äù is. Now I think there‚Äôs merely underrated like the first Sex Pistols show; everyone who read about them created their own token format. We‚Äôre moving forward with Macaroons, and I‚Äôm psyched about that, but I‚Äôd hesitate to recommend them for a typical CRUD application.\n\nBut, don‚Äôt use JWT."
  },
  {
    "title": "Deploy that Elixir App Today",
    "url": "https://fly.io/blog/better-business-intelligence-in-elixir-with-livebook/",
    "content": "Fly runs apps (and databases) close to users, by taking Docker images and transmogrifying them into Firecracker micro-vms, running on our hardware around the world. You should try deploying an Elixir app, right now: it only takes a few minutes.\n\nAs a developer, has your manager ever come and asked a question like, ‚ÄúHow much money are we making?‚Äù If you were a line-of-business developer at a global insurance company, you‚Äôd reach for your handy, nosebleed-expensive Business Intelligence (BI) suite to answer this question. But you‚Äôre not, so how did you answer it for them?\n\nObviously, you‚Äôd do what we all do. You‚Äôd SSH into your server, start an Elixir iex session or a Rails console, then run some scripts to query data, sum numbers, and come up with answers.\n\nWell, give yourself a raise! Because you just built a BI suite.\n\nIt may not seem super sophisticated, but it solves the business need. And for problems like this, Livebook can be a better BI tool for Elixir developers.\n\n\n## BI What?\n\nWhat is a BI tool?\n\nBusiness intelligence (BI) comprises the strategies and technologies used ‚Ä¶ for the data analysis of business information.\n\nTranslated from Gartner-speak, that means any tools you run to get a picture of how the business is doing are BI tools.\n\nIn the last bootstrapped startup I worked at, management routinely asked backend developers for business numbers. It was simple stuff, like:\n\nAs simple as this stuff seems, it‚Äôs really important for those business focused leaders to understand and make better decisions. That‚Äôs why global insurance companies with applications that are too complicated to bring up a Rails console on spend six figures on BI suites.\n\nHow did we get those answers? Using our Elixir iex, or interactive shell. We ran some scripts and gave them CSV friendly rows they could add to their spreadsheets. In that early stage startup, we were using iex as our BI tool. At a startup before that, we did the same thing but using the Rails console.\n\nIf you‚Äôre using the Rails console, Elixir‚Äôs iex, or another REPL to examine your data, then that‚Äôs your BI tool for now. But with Elixir, we can do better. Livebook gives you data, charts and graphs too, but because it‚Äôs executing your Elixir code, it can also call out to your other integrations and pull in even more.\n\nTo understand why Livebook can be a better tool, let‚Äôs go further and talk about BI tools in general, not just your REPL.\n\n\n## Old School Business Intelligence\n\nOur premise in this post is that we can give ‚Äúserious‚Äù BI tools a run for their money with Elixir and Livebook. Let‚Äôs see what we‚Äôre up against.\n\nCompanies spend lots of money every year on their BI tools. You hear some of the numbers and it seems bananas. But it‚Äôs because they add a lot of value. Spotting trends in your data and customer behavior can make the difference of success and failure for a company.\n\nMost BIS tools are commercial. But there‚Äôs a handful of credible open source projects. Metabase, for example, is an open source BI tool that works quite well. It connects directly to an application‚Äôs database and helps you do some spelunking, aggregation, and shiny graphing. You can even create and share custom dashboards. Think of it as Grafana, but for MBAs ‚Äì it‚Äôs a great tool.\n\nDeploying Metabase alongside your app might look like this:\n\nMetabase is an application that probably shouldn‚Äôt be exposed publicly and it needs direct access to your database. It‚Äôs also a bit of a mammoth ‚Äì one doesn‚Äôt just walk into Metabase and expect to get anything done, there‚Äôs a real learning curve even when you know how to write SQL. It can be a heavy tool when you just want to do some quick poking around.\n\nIt‚Äôs also an app you need to keep running. We sell hosting, so we‚Äôre generally OK with that. In fact, Metabase ships a Docker image and Fly lets you quickly deploy apps using Docker. Money.\n\nThis is fine when you want a dedicated data dashboard or you want to let non-developers see reports and graphs and be business-intelligent. However, when a project is young and you‚Äôre a developer, digging with code is powerful. This is where Livebook can help!\n\n\n## Why is Livebook Better?\n\nLet‚Äôs start with what Livebook is.\n\nLivebook started out as Elixir‚Äôs version of Jupyter Notebooks. Jupyter is pretty great. But it turns out that code notebooks on Elixir are something special; they do something you usually can‚Äôt pull off in Python. That‚Äôs because Elixir has powerful built-in clustering, built on Erlang‚Äôs BEAM/OTP runtime. Livebook notebooks can talk directly to running Elixir apps. And so we can do analysis and visualization directly off the models in our applications.\n\nLivebook really sings on Fly.io. We make it easy to deploy clusters of Elixir applications that can talk privately between themselves. More importantly: it‚Äôs easy to bring up a secure WireGuard tunnel to those applications. So I can run Livebook locally on my machine, and attach it to any of my apps running on Fly.io!\n\nFor lean-and-mean startups, this is a win. You only need your app and your database running. Then, on an ‚Äúas-needed‚Äù basis, you connect to your app with Livebook for analysis. Inside Livebook, analysis is done using your app‚Äôs Elixir code. Livebook therefore doesn‚Äôt need to connect directly to the database to run queries, and, even better, we get to re-use the business logic, associations, and schemas our apps already have.\n\nAs a BI tool, Livebook notebooks have these benefits:\n\nSome of this you can pull off using just your REPL and some raw SQL queries. But why would you? It‚Äôs easier to use your project‚Äôs code, database models (Ecto schemas for Elixir) and associations.\n\nFurther, your apps probably rely on external services like Stripe. Because Livebook talks directly to your Elixir code, you can query those external services with that code, and then combine the results with your data. This is a combination that even dedicated tools like Metabase have a hard time beating.\n\n\n## Connecting to Your App on Fly\n\nGreat! You have a notebook that loads and visualizes some data! To get the benefits, you need to connect it to your app running on Fly.io.\n\nFollow the Fly.io Elixir Guide for Connecting Livebook to your App in Production to connect Livebook to your app.\n\nWith Livebook connected to your app, you can run your notebook and start gaining insight to your data!\n\n\n## Gaining Intelligence\n\nHow you use Livebook depends on your application and your industry.\n\nHere are some ideas to get the brain juices flowing. Each of these example notebooks would be a ‚Äúmodule‚Äù in a serious commercial BI suite, and you‚Äôd pay $45k for a license for it.\n\n\n## User Account Setup\n\nHow many of your users have fully set up their accounts?\n\nBuild and share a notebook that tracks where users are in your onboarding process.\n\n\n## Users Bouncing From The App\n\nWhere are accounts stalling out in onboarding?\n\nGraph the stages accounts are at and let analysts drill into the onboarding funnel.\n\n\n## Sales Analysis\n\nHow were sales for your products or services last month?\n\nGraph a multi-series chart comparing sales across products.\n\nWhat about total sales per week?\n\nBuild a notebook with a date input making it easy to switch the week being charted.\n\n\n## Integrations\n\nWhat external financial systems are you integrated with?\n\nNotebooks can execute your code to query those services and visualize refund rates, processing fees, and more.\n\n\n## Wrapping Up\n\nWhat‚Äôs great about the Livebook approach is you are writing working Elixir code. When you are ready to build an Admin Dashboard page in your app, you‚Äôve already done the hard work of figuring out what data is valuable and even the code for how to get it!\n\nYou‚Äôve got that cool Elixir app you‚Äôve been working on, Fly.io is a great place to deploy it! It‚Äôs easy to get started."
  },
  {
    "title": "Fly ‚ù§Ô∏è Elixir",
    "url": "https://fly.io/blog/phoenix-moves-to-esbuild-for-assets/",
    "content": "The Phoenix Framework is the go-to web framework for Elixir developers. A recent PR was merged that replaces the use of node, npm, and webpack with esbuild.\n\nFor those new to esbuild, it is written in Go so it compiles to native code and runs really fast. It is a JavaScript bundler that performs many common asset pipeline tasks.\n\nTo understand why the change happened, it‚Äôs good to get a sense of the problems the Phoenix team has been dealing with.\n\nOf the Phoenix project‚Äôs 2,034 total issues, npm was involved in 591, webpack played a role in 79, and brunch was a factor for 171.\n\nSome of the issues overlapped into multiple areas, so we can safely say that around 30% of reported Phoenix issues were related to JavaScript packaging! That‚Äôs a lot of issues to support and they aren‚Äôt even for the language the framework is written in!\n\nChris McCord, the creator and maintainer of Phoenix had this to say:\n\nI say this in the fairest way possible after years of support \u0026 churn ‚Äì I now consider placing the burden of node/npm/webpack on new users as actively harmful. Also, long-term reproducible builds are essential for maintainable software \u0026 node has not stood this test of time. With esbuild, those that want to take advantage of the innovation in the node community need only to npm install and esbuild will handle it, so the opt-in path is as simple as it gets when folks are doing complex client-side development that necessarily involves node tooling.\n\nThis change allows newcomers a seamless getting started experience, long-term moderate js/css users a rock-solid reproducible build for the lifetime of their projects, and advanced client-side SPA users a trivial opt-in path. So esbuild allows us to execute on all fronts!\n\nIf your team does a lot of front-end javascript, you likely have your own customized setup using webpack or other tools anyway. The Phoenix change to esbuild does not prevent you from using whatever asset processing tools you need.\n\nSpeaking of the esbuild change, Jos√© Valim, the creator of Elixir said:\n\nThis matters because it gives us full ownership of the getting started process. If someone runs the phx.new installer for v1.6 in 5 years from now, they should still get a fully functional project.\n\nIn the last weeks, phx.new has been broken because of node-sass and then npm v7. Plus other reasons in the past. We always have to catch up.\n\nIt is extremely important that your first ever Phoenix command always succeeds, and this gets us very close to that!\n\nInterested in trying it out right now? Jos√© Valim explains how:\n\nIf you are moving an existing project, then running [Phoenix] v1.6 should be totally fine.\n\nIf you want only esbuild, you don‚Äôt need to move to v1.6 (see example), but remember esbuild can still leave zombie processes until a PR is merged upstream.\n\nThe change to esbuild will be part of the upcoming Phoenix 1.6 release which is expected ‚Äúsoon‚Äù. As the Phoenix release nears and more people are trying it out, we‚Äôll get a better sense of any snags or migration issues.\n\nThis is an exciting change! Over the years, I‚Äôve fought many times with broken JS build pipelines because some dependency updates were needed. I‚Äôm very interested in seeing if this meets the goal of being a more stable system over time.\n\nFly is an awesome place to run your Elixir apps. Deploying, clustering, connecting Observer, and more is all supported!"
  },
  {
    "title": "Cluster all the things!",
    "url": "https://fly.io/blog/last-mile-redis/",
    "content": "Fly runs apps (and databases) close to users, by taking Docker images and transmogrifying them into Firecracker micro-vms, running on our hardware around the world. You should try deploying an app, right now: it only takes a few minutes.\n\n100 milliseconds is the magic number. For a backend application, a sub-100ms response time is effectively instantaneous, and people love using ‚Äúinstant‚Äù apps. Since we‚Äôre all dirty capitalists, we‚Äôd add: if people love your app, you‚Äôll make more money. But you can also chase sub-100ms for the endorphins.\n\nWhen app developers need to shave tens of milliseconds off their request times, they start with their database. But optimizing databases is painful. When it comes to making ‚Äúthings-that-involve-data‚Äù fast, it‚Äôs easier to add a layer of caching than to make database changes. Even basic cache logic can shave tens of milliseconds off database queries. So backend developers love caches.\n\nIf you want to skip the end, all you need to know is we‚Äôve built a Redis app that runs globally, includes ‚Äúinstant‚Äù purge, and works with whatever backend framework you build apps with. Here‚Äôs the source.\n\nWe love caches too. And we think we can do better than incremental 10ms improvements.\n\nWhat‚Äôs better than good cache logic in centralized app servers? Caching data close to end users, regardless of whether they‚Äôre in Singapore or Amsterdam. Geo-routing can shave hundreds of milliseconds, plural, off application response times. When a request hits a server in your own city, and is served hot from data conveniently stored in memory, its response is perceptibly faster than it would have been had it instead schlepped across an entire continent to get to you.\n\nDoes this sound like promotional content? That‚Äôs because we believe it. It‚Äôs why we built Fly.io: we think backend apps can, uh, scream, when you ship them on CDN-like infrastructure.\n\nAnd, as it happens, Redis has very interesting knobs that make it work well when you scatter instances around the world.\n\n\n## One weird CDN thing\n\nWhen you build a CDN, you learn stuff. Here‚Äôs an important thing we learned about geographic caching.\n\nCache data overlaps a lot less than you assume it will. For the most part, people in Singapore will rely on a different subset of data than people in Amsterdam or S√£o Paulo or New Jersey.\n\nStop and think about it and it makes sense. People in Singapore eat in restaurants in Singapore, send cash to their friends in Singapore, and talk to their Singapore friends about meeting up. People in New Jersey care about show schedules in New Jersey and the traffic in New Jersey. They care a lot more about hoagies than people in Singapore. Humans are data magnets. They tend to work in companies together with people who live relatively near them, and talk with their friends, who, again, are relatively close to each other.\n\nWhat you find when you look at a CDN cache is that for most apps, data is only duplicated in one or two regions. It almost never shows up in all the regions.\n\nThis simplifies things. Take an app that needs ten Redis servers to keep up with its load. The conventional way to build that system is to park all those servers in us-east-1, and then implement sharding logic to spread the load across the cache servers. That sharding logic infects the rest of the system. But we can usually exploit our CDN observation to avoid that: if we can deploy our app in multiple geographic regions, we can just have one server per region, without any explicit sharding logic. Cities. They‚Äôre nature‚Äôs shards!\n\nLet‚Äôs talk a bit about how you‚Äôd do this.\n\nWe have to talk our book here for a second, because it‚Äôll make the rest of this make sense. The whole premise of Fly.io is that we make it trivial to get a Docker container running in a bunch of different geographic regions. There are other ways to run containers around the world, and if you prefer them, what we have to say here still makes sense. Just take it as a given that you can easily boot stuff up in Singapore, Newark, and Amsterdam.\n\n\n## JBOR\n\nThe most boring way to exploit geographic cache locality is ‚Äújust a bunch of Redii‚Äù.\n\nRun standalone Redis servers and app servers in each region you care about. Treat them as independent caches. When a user looks up the review score for Johnny‚Äôs Beef in Chicago, the Chicago app server checks the Chicago Redis cache. Everyone involved in the Chicago request is blissfully unaware of whatever is going on in Singapore.\n\nWe lean on caches because apps are read-heavy. But writes happen. If you‚Äôre running caches all over the world, they can eventually drift from their source of truth. Bad cache data will really irritate people. It can break apps entirely, which is why you have the keyboard shortcut for ‚Äúhard refresh‚Äù in muscle memory. So, when data changes, the global cache fabric should also change, even when the cache fabric is JBOR.\n\nThis sounds distributed-systems-hard. But it doesn‚Äôt have to be. We can use a key based cache invalidation scheme to keep things fresh for an app with standalone cache servers. Key-based invalidation inverts the intuitive roles of keys and values: instead of a durable key pointing to changeable value, values never change, only the keys (for instance, by timestamping them). Database changes generate new keys; stale cache values eventually expire from neglect. When all is right with the world, this can be good enough.\n\nBut we live in a fallen world. Apps have bugs. So do people. Bad information eventually pollutes caches. If we can purge bad cache data our life is easier. If we can purge it everywhere, instantly, we‚Äôll be as wizards. Wizards with a ‚Äúbuild a whole CDN and take it public‚Äù level of power.\n\nLet‚Äôs seize this power for ourselves.\n\n\n## Abusing replication for instant cache purge\n\nRedis has a simple replication model: we can start a Redis server with replicaof primary-redis.internal 6379 and it will grab a copy of the existing database and keep it in sync until we shut it down. The primary server doesn‚Äôt even need to know ahead of time. It‚Äôs blissfully simple.\n\nWe can exploit this. Create a primary Redis in Dallas. Add replicas in Singapore, Amsterdam, and Sydney. Now: write to the primary. The whole world updates. We‚Äôve got a global cache fabric that‚Äôs always up to date.\n\nLike any distributed cache fabric, we‚Äôll inevitably cache something we shouldn‚Äôt. Somehow, the cache key global-restaurant-ranking-johnnys-beef reads 105. Not OK! But we can just issue a DEL global-restaurant-ranking-johnnys-beefand it‚Äôll be back to 1 , everywhere, fast enough to seem instant.\n\nThis seems great. But there‚Äôs a catch: that CDN observation we made earlier. If Singapore shared a lot of information with Chicago, this would be close to the ‚Äúright‚Äù global Redis configuration. But they don‚Äôt; very little data overlaps between regions.\n\nSo we‚Äôre not quite there yet. But we have more tricks.\n\n\n## Eventually consistent, never consistent: why not both?\n\nOne kind of database cluster has ‚Äústrong‚Äù consistency: once data is written, we trust that subsequent reads, anywhere in the cluster, see the new data. More frequently, we have some degree of ‚Äúeventual‚Äù consistency: the data will populate the whole cluster‚Ä¶ at some point, and we‚Äôre not waiting.\n\nOur Redis replica scheme has eventual consistency. We write to a primary Redis instance and trust the replicas will get themselves in sync later on.\n\nMeanwhile, the JBOR cluster is never consistent ‚Äì in the same way that two people who‚Äôve never met each other ‚Äúaren‚Äôt dating‚Äù. But on the other hand, we like it because it optimizes storage by storing only what each region needs.\n\nWhat we need is a way to treat each region mostly independently and sync some changes from a central source of truth.\n\nWhich gets us to the an interesting config option: replica-read-only. This setting defaults to yes, and does what you‚Äôd expect. But check this out:\n\nNow we have a replica that also accepts writes. üò±. This is terrible for a backend database. But this isn‚Äôt a database; it‚Äôs a cache fabric. Our primary is in Dallas. But Singapore‚Äôs app server can still write directly to Singapore‚Äôs Redis replica. And it still syncs changes from the primary! It‚Äôs still in charge!\n\nSo, when we need to make sure bad cache data goes away everywhere, we can perform an ‚Äúinstant‚Äù cache purge by issuing that DEL to the the Dallas primary.\n\nOur favorite thing about Fly.io is how easy it is to prototype and deploy stuff like this. From a working Docker container locally, you can have a globally distributed cluster up and running in single-digit minutes.\n\n\n## Beyond the purge\n\nA thing that happens when you build a platform for running clusters of globally-distributed stuff is that you stumble onto interestingly simple distributed designs. This seems like one of them. We like it!\n\nThis sort of implied, selective replication potentially gives us a lot more than just global cache purge. We can push any global content to all the cache regions, simply by writing to the primary:\n\nOr, we can simulate a distributed fan-out queue by pushing to a list on the primary‚Ä¶\n\n‚Ä¶ and then popping from that list in each region.\n\nWe can let regions selectively replicate by choosing when to write to their local cache and when to write globally, in a manner similar to the one we used to globally distribute Postgres. No doubt there‚Äôs a zillion other things we haven‚Äôt thought about. It‚Äôs been hard to play with these ideas, because almost nobody runs multi-region AWS for simple applications. But anyone can run a multi-region Fly.io app with just a couple commands.\n\nWe‚Äôre psyched to see what else people come up with.\n\nWant to know more? Join the discussion."
  },
  {
    "title": "Livebook works awesome on Fly",
    "url": "https://fly.io/blog/livebook-with-kino/",
    "content": "Livebook 0.2 was released. The big news here was the announcement of ‚ÄúKino‚Äù (meaning ‚Äúcinema‚Äù). It‚Äôs a client-side focused feature that animates data changes. Jos√© Valim created a video demonstrating some of the new Livebook features. Use this link to jump to the part of the video, it‚Äôs 18:52 in, that shows Kino in action.\n\nAnother big change is the addition of inputs! Accessing an input‚Äôs data in your Elixir cell looks like this: IO.gets(\"input name: \") Input values come in as a string so it may need to parsed or processed for your particular use.\n\nThe other big Kino feature is called data_table. With this, you can easily show any data as a table with pagination and sorting.\n\nLivebook is actually a big deal. It lowers the bar on a lot things for Elixir. It can be used to:\n\nI‚Äôm bullish on Livebook and what I think it can do for the entire Elixir community. Livebook is definitely something to watch!\n\n\n## Other Elixir News\n\nIn other Elixir news, conferences are returning! Many have been only virtual by necessity. While this has been nice because I could attend virtually without the need to travel. I have missed seeing people and meeting new people the way I did at physical conferences.\n\nTwo significant conferences were announced that are hybrid (physical and virtual) or physical for 2 days and virtual for 2 days. Some interesting experiments going on here!\n\nIn order of their dates:\n\nElixirConfEU - A hybrid conference being held in Warsaw, Poland and virtual as well. The dates are September 9-10.\n\nElixirConf US 2021 is being held physically in Austin, TX October 12-13 and online, October 14-15. This is an interesting experiment. A 4-day conference where the physical and virtual talks aren‚Äôt the same!\n\nLivebook supports collaborative editing. When you host your own instance you can invite people to join you!"
  },
  {
    "title": "flyctl postgres create",
    "url": "https://fly.io/blog/globally-distributed-postgres/",
    "content": "Fly runs apps (and databases) close to users, by taking Docker images and transforming them into Firecracker micro-vms running on our hardware around the world. You should try deploying an app: it only takes a few minutes.\n\nThis is a story about a cool hack we came up with at Fly. The hack lets you do something pretty ambitious with full-stack applications. What makes it cool is that it‚Äôs easy to get your head around, and involves just a couple moving parts, assembled in a simple but deceptively useful way. We won‚Äôt bury the lede: we‚Äôre going to talk about how you can deploy a standard CRUD application with globally-replicated Postgres, for both reads and writes, using standard tools and a simple Fly feature.\n\nIf you‚Äôve build globally distributed apps in the past, you‚Äôre probably familiar with the challenges. It‚Äôs easy to scale out a database that‚Äôs only ever read from. Database engines have features to stand up ‚Äúread replicas‚Äù, for high-availability and caching, so you can respond to incoming read requests quickly. This is great: you‚Äôre usually more sensitive to the performance of reads, and normal apps are grossly read-heavy.\n\nBut these schemes break down when users do things that update the database. It‚Äôs easy to stream updates from a single writer to a bunch of replicas. But once writes can land on multiple instances, mass hysteria! Distributed writes are hard.\n\nApplication frameworks like Rails have features that address this problem. Rails will let you automatically switch database connections, so that you can serve reads quickly from a local replica, while directing writes to a central writer. But they‚Äôre painful to set up, and deliberately simplified; the batteries aren‚Äôt included.\n\nOur read/write hack is a lot simpler, involves very little code, and it‚Äôs easy to understand. Let‚Äôs stick with the example of a standard Rails application (a difficult and common case) and dive in.\n\n\n## When Your Only Tool Is A Front-End Proxy, Every Problem Looks Like An HTTP Header\n\nYou can think of Fly.io as having two interesting big components. We have a system for transforming Docker containers into fast, secure Firecracker micro-VMs. And we have a global CDN built around our Rust front-end proxy.\n\nWe run all kinds of applications for customers here. Most interesting apps want some kind of database. We want people to run interesting apps here, and so we provide Fly Postgres: instances of Postgres, deployed automatically in read-replica cluster configurations. If your application runs in Dallas, Newark, Sydney and Frankfurt, it‚Äôs trivial to tell us to run a Postgres writer in Dallas and replicas everywhere else.\n\nFly has a lot of control over how requests are routed to instances, and little control over how instances handle those requests. We can‚Äôt reasonably pry open customer containers and enable database connection-switching features for users, nor would anyone want us to.\n\nYou can imagine an ambitious CDN trying to figure out, on behalf of its customers, which requests are reads and writes. The GETs and HEADs are reads! Serve them in the local region! The POSTs and DELETEs are writes! Send them to Dallas! Have we solved the problem? Of course not: you can‚Äôt look at an HTTP verb and assume it isn‚Äôt going to ask for a database update. Most GETs are reads, but not all of them. The platform has to work for all the requests, not just the orthodox ones.\n\nSo, short of getting in between the app and its database connection and doing something really gross, we‚Äôre in a bit of a quandary.\n\nA bit more on ‚Äúgross‚Äù here: you can get your database layer to do this kind of stuff for you directly, using something like pgpool so that the database layer itself knows where to route transactions. But there‚Äôs a problem with this: your app doesn‚Äôt expect this to happen, and isn‚Äôt built to handle it. What you see when you try routing writes at the database connection layer is something like this:\n\nA read query for data, from read replica, perhaps for validation: 0ms. ü§ò\n\nA write to the primary, in a different region: 20-400ms. üò¶\n\nA read query to the primary, for consistency, in a different region: 20-400ms. üôÄ\n\nMore read queries against primary for consistency, in a different region: 20-400ms. üò±\n\nMaybe another write to the primary: 20-400ms. üòµ\n\nRepeat ‚ò†Ô∏è.\n\nIt is much, much faster to ship the whole HTTP request where it needs to be than it is move the database away from an app instance and route database queries directly. Remember: replay is happening with Fly‚Äôs network. HTTP isn‚Äôt bouncing back and forth between the user and our edge (that would be slow); it‚Äôs happening inside our CDN.\n\nIt turns out, though, that with just a little bit of cooperation from the app, it‚Äôs easy to tell reads from writes. The answer is: every instance assumes it can handle every request, even if it‚Äôs connected to a read replica. Most of the time, it‚Äôll be right! Apps are read-heavy!\n\nWhen a write request comes in, just try to do the write, like a dummy. If the writer is in Dallas and the request lands in Frankfurt, the write fails; you can‚Äôt write to a read replica. Good! That failure will generate a predictable exception. Just catch it:\n\nIn 8 lines of code, we catch the read-only exceptions and spit out a fly-replay header, which tells our proxy to retry the same request in the writer‚Äôs region.\n\nYou could imagine taking this from 8 lines of code to 1, by packaging this logic up in a gem. That‚Äôs probably a good idea. The theme in this blog post though is that all the machinery we‚Äôre using to make this work is simple enough to keep in your head.\n\nOur proxy does the rest of the work. The user, none the wiser, has their write request served from Dallas. Everything‚Ä¶ works?\n\n\n## The Fly-Replay Header\n\nThis design isn‚Äôt why we built the fly-replay feature into our proxy.\n\nThe problem we were originally aiming at with the header was load balancing. We have clients that serve several hundred million images a day off Fly. And, some time ago, we had incidents where all their traffic would get routed to out-of-the-way places, like Tokyo, which struggled (ie: melted) to keep up with the load.\n\nThe immediate routing issues were easy to fix. But they weren‚Äôt the real problem. Even as traffic was getting sent to overloaded Tokyo servers, we had tons of spare capacity in other nearby regions. Obviously, what we want to do is spread the load to that spare capacity.\n\nBut the obvious solutions to that problem don‚Äôt work as well as you‚Äôd assume in practice. You can‚Äôt just have Tokyo notice it‚Äôs overloaded and start sending all its traffic to Singapore. Now Singapore is melting! What‚Äôs worse, traffic load is an eventual consistency problem, and the farther away an unloaded region is from Tokyo, the less likely it is that Tokyo can accurately and instantaneously predict its load.\n\nHere‚Äôs the thing, though: HTTP is cheap, especially when you‚Äôve got persistent connections (the backhaul between our edges and the workers where apps actually run is HTTP2).\n\nAnd so, fly-replay. When a request hits Tokyo, we estimate which process has capacity based on gossiped load data. We then send the request to the server running that process. Then we check to see if the process still has capacity. If it does, great, we dump the request into the user process. If it doesn‚Äôt, we send a reply to the edge server saying ‚Äúhey, this process is full, try another‚Äù. The effect is something we call latency shedding: if we can try every instance of an app process quickly enough we‚Äôll always be able to service a request.\n\n\n## You can see this in action.\n\nIf this stuff is interesting to you, check out this simple Rails app we put together (maybe we‚Äôve mentioned that it‚Äôs ridiculously easy to boot apps up on Fly?). What you‚Äôre looking at is a globally deployed stock Rails app that explicitly steers the database with fly-replay. You‚Äôre landing on an instance of the application because our BGP Anycast routes took you there; for me, my connecting region is Chicago. But you can tell the app to replay your request in a bunch of other regions; click the tabs.\n\n\n## 80% Of The Time It Works Every Time\n\nWhether you understand it in your bones or not, a big part of why you‚Äôre using a database like Postgres is that you want strong consistency. Strongly consistent databases are easy to work with. You issue a write to a consistent database, and then a read, and the read sees the result of a write. When you lose this feature, things get complicated fast, which is part of why people often prefer to scale up single database servers rather than scaling them out.\n\nObviously, once you start routing database (or HTTP) requests to different servers based on local conditions, you‚Äôve given up some of that consistency. We can‚Äôt bend the laws of physics! When you read from Frankfurt and then write to Dallas, Dallas will quickly replicate the altered rows to Frankfurt, but not instantly. There‚Äôs a window of inconsistency.\n\nWe have a couple of responses to this.\n\nFirst, in a lot of cases, it might not matter. There are large classes of applications where short inconsistencies between writes and subsequent reads aren‚Äôt a big problem. Data replicates fast enough that your users probably won‚Äôt see inconsistencies. If displaying stale data doesn‚Äôt cause real problems, maybe worry about it another time.\n\nSecond, if you want ‚Äúread-your-own-writes-between-requests‚Äù behavior, you can implement that with the same header. When you set fly-replay to the writer, set a timestamp in the user‚Äôs session, during which you fly-replay all the requests. HTTP is cheap!\n\nThird, you could simulate synchronous replication. Actual synchronous replication doesn‚Äôt work well on cross-geo clusters, but Postgres does let you see query replica freshness. We run these kinds of queries for health checks. You could build a little logic into your app to check the replication lag on the user‚Äôs region and delay the HTTP response until it‚Äôs ready.\n\nFinally, though, it‚Äôs just the case that this pattern won‚Äôt work for every application. It‚Äôs neat, and it makes Postgres read-replicas much easier to use, but it isn‚Äôt a cure-all.\n\nThat‚Äôs it. That‚Äôs the tweet. One command gets you a multi-database-capable Postgres cluster, with high-availability read replicas, in whichever regions you want us to run it in.\n\n\n## Maybe Your App is Actually Write Heavy\n\nThere are database engines designed handle distributed write scenarios, some with native geographic partitioning ‚Äì and you can use them on Fly, too.\n\nHave you seen CockroachDB? It‚Äôs amazing. CockroachDB gives developers all the tools they need to model geographic distribution into their schemas. With the right schema, Cockroach moves the chunks of data close to the most active users and keep writes wicked fast. You‚Äôll need the Enterprise edition to really take advantage of this, but it‚Äôs legit.\n\nGenerally, if you‚Äôre prepared to manage your own database, you can deploy it on Fly by attaching volumes to your instances; volumes are arbitrarily-sized block devices and some rules that we enforce in the background to pin your app to worker hosts where those volumes live. Volumes are the storage fabric on which our Postgres is built, and you can just build directly on them.\n\nSo you can see the outline of two big approaches here: you can configure Postgres (or, rather, we can configure Postgres for you) to deploy in a configuration where you‚Äôre distributed enough to get 90% of the benefit of distribution, and, if that works for you, you can stick with Postgres, which is great because everybody loves Postgres, and because Postgres is what your existing apps already do.\n\nOr, you can deploy on distributed-by-design databases like Cockroach, where databases are split into geographically distributed regions and concurrency is managed by Raft consensus.\n\n\n## And That‚Äôs It\n\nMulti-reader, single-writer Postgres configurations aren‚Äôt a new thing. Lots of people use them. But they‚Äôre annoying to get working, especially if you didn‚Äôt start building your application knowing you were going to need them.\n\nIf you‚Äôre scaling out Postgres instead of scaling it up, there‚Äôs a good chance you‚Äôre doing it because you want to scale your application geographically. That‚Äôs why people use Fly.io; it‚Äôs our whole premise. We do some lifting to make sure that running an app close to your users on Fly.io doesn‚Äôt involve a lot of code changes. This strategy, of exploiting our proxy to steer database writes, is sort of in keeping with that idea.\n\nIt should work with other databases too! There‚Äôs no reason we can see why you can‚Äôt use the same trick to get MySQL read replicas working this way. If you play around with doing that on Fly.io, please tell us about it at our community site.\n\nMeanwhile, if you‚Äôre averse to science projects, we think we‚Äôre on track to become the simplest conceivable way to hook a full-stack application up to a scalable Postgres backend. Give it a try.\n\nWant to know more? Join the discussion."
  },
  {
    "title": "You can launch an observable Elixir app on Fly.io in minutes",
    "url": "https://fly.io/blog/monitoring-your-fly-io-apps-with-prometheus/",
    "content": "Fly.io is a platform that makes deploying and running your Elixir applications fun again. You can do advanced monitoring with Prometheus on Fly.io without installing anything! Deploy your Elixir application and try it out!\n\nFly.io takes Docker containers and converts them into fleets of Firecracker micro-vms running in racks around the world. If you have a working Docker container, you can run it close to your users, whether they‚Äôre in Singapore or Amsterdam, with just a couple of commands. Fly.io is particularly nice for Elixir applications, because Elixir‚Äôs first-class support for distributed computing meshes perfectly with Fly.io‚Äôs first-class support for clusters of applications.\n\nThis post is about another cool Fly.io feature ‚Äî built-in Prometheus metrics ‚Äî and how easy it is to take advantage of them in an Elixir application. I wrote and maintain an Elixir library, PromEx, that makes it a snap to export all sorts of metrics from your Elixir applications and get them on dashboards in Grafana. Let‚Äôs explore some of the concepts surrounding Prometheus and see how we can leverage the Fly.io monitoring tools in an Elixir application to get slick looking dashboards like this one:\n\n\n## Why Application Monitoring is Important\n\nWhen customers are paying for your application or service, they expect it to work every time they reach for it. When things break or errors occur, your customers will not be happy. If you are lucky, your customers send you an email letting you know that things are not working as expected. Unfortunately, many of these occurrences go unreported.\n\nKnowing exactly when things are going wrong is key to keeping your customers happy. This is the problem that monitoring tools solve. They keep an eye on your application, and let you know exactly when things are behaving suboptimally.\n\nImagine for example that you have an HTTP JSON API. You deploy a new version that changes a bunch of endpoints. Assume it‚Äôs infeasible to go through every single route of your application every time you deploy, or to test each endpoint individually with every permutation of input data. That would take far too much time, and it doesn‚Äôt scale from an organizational perspective: it would keep engineers constantly context switching between feature work and testing new deployments.\n\nA more scalable solution: briefly smoke test the application after a deployment (as a sanity check), and then use monitoring tooling to pick up on and report on any errors. If your monitoring solution reports that your HTTP JSON API is now responding with 400 or 500 errors, you know you have a problem and you can either rollback the application, or stop it from propagating to across the cluster. The key point is that you can proactively address issues as opposed to being blind to them, and at the same time you can avoid sinking precious engineer time into testing all the things.\n\nWhile ensuring that production users are not experiencing issues is a huge benefit of application monitoring, there are lots of other benefits. They include:\n\nLet‚Äôs dig into how Prometheus achieves these goals at the technical level.\n\n\n## How Does Prometheus Work?\n\nAt its core, Prometheus is a time-series database that enables you to persist metrics in an efficient and performant manner. Once your metrics are in the Prometheus time-series database, you can create alerting rules in Grafana. Those alerts can then be triggered once certain thresholds and criteria are met, letting you know that something has gone wrong.\n\n‚ÄúBut how exactly do my application metrics end up in Prometheus?‚Äù Well, your Prometheus instance is configured to scrape all of your configured applications. At a regular interval, each of their instances is queried for metrics data, which is stored in a database. Specifically, it makes a GET HTTP call to /metrics (or wherever your metrics are exposed) and that endpoint will contain a snapshot in time of the state of your application. Once your metrics are in Prometheus, you can query the time-series database with Grafana to plot the data over time; Grafana uses PromQL to refresh data and update its panels.\n\nGiven that Prometheus scrapes your applications at a regular interval, the resolution of your time-series data is bound to that interval. In other words, if you get 1,000 requests in the span of 10 seconds, you don‚Äôt know exactly at what timestamps those 1,000 requests came in, you just know that you got 1,000 requests in a 10 second time window. While this may seem limiting, it is actually a benefit in disguise. Since Prometheus doesn‚Äôt need to keep track of every single timestamp, it is able to store all the time-series data very efficiently.\n\nLuckily with Fly.io, the administration and management of Prometheus can be taken care of for you!\n\n\n## Turning On Prometheus On Fly\n\nManaging, configuring and administering your own Prometheus instance can be a bit of a tall order if you have never worked with Prometheus before. Fortunately, all you need to do to enable Prometheus metrics for your application is add a couple of lines to your fly.toml manifest file. All Fly.io needs to know is what port and path your metrics will be available at. For the TODO List Elixir application for example, the following configuration was all that was needed:\n\nIn order to visualize your Prometheus metrics, you‚Äôll need to have an instance of Grafana running somewhere. You could deploy your own Grafana instance on Fly.io by following this guide, but you can also use Grafana Cloud (it has a free plan) ‚Äî Grafana Cloud works fine with Fly. Which ever route you take, all you then need to do is configure Grafana to communicate with the Fly.io managed Prometheus instance and you are good to go!\n\nNow that we‚Äôve got Prometheus hooked up, we need to get our Elixir application to start providing metrics.\n\n\n## Monitoring Elixir with PromEx\n\nWhenever I write a production-grade Elixir application that needs monitoring, I reach for PromEx.\n\nI wrote PromEx and maintain it because I wanted something that made it easy to manage both the collection of metrics and the lifecycle of a a bunch of Grafana dashboards. That‚Äôs to say: PromEx doesn‚Äôt just export Prometheus metrics; it also provides you with dashboards you can import into Grafana to immediately get value out of those metrics. I think this is a pretty ambitious goal and I‚Äôm happy with how it turned out. Let‚Äôs dig in.\n\nAt a library design level, PromEx is a plugin style library, where you enable a plugin for whatever library you want to monitor. For example, PromEx has plugins to capture metrics for Phoenix, Ecto, the Erlang VM itself, Phoenix LiveView and several more. Each of these plugins also has a dashboard to present all the captured metrics for you. In addition, PromEx can communicate with Grafana using the Grafana HTTP API, so it will upload the dashboards automatically for you on application start (if you configure it that is). What this means is that you can go from zero to complete application metrics and dashboards in less that 10 minutes!\n\nIn the Elixir example application, you can see that the PromEx module definition specifies what plugins PromEx should initialize, and what dashboards should be uploaded to Grafana:\n\nWith a little bit of configuration in runtime.exs PromEx can communicate with Grafana to take care of the graph annotations and dashboard uploads:\n\nWith the managed Prometheus instance from Fly.io, and the metrics collection from PromEx, you have an instrumented application in record time! Here are some snapshots from the auto generated dashboards for the Todo List application:\n\n\n## And That‚Äôs It!\n\nElixir makes it easy to run ambitious, modern applications that take advantage of distributed computing. It should be just easy easy to see what those applications are actually doing, and to have alerts go off when they misbehave. Between Fly.io‚Äôs built-in Prometheus and the PromEx library, it‚Äôs easy to get this kind of visibility. Your application can be instrumented with dashboards and annotations in a coffee break‚Äôs worth of time.\n\nBe sure to check out the Todo List application Repo for more technical details and all the code necessary to do this yourself. What used to take a few days to set up and run, now only takes a few hours, so be sure to give it a test drive!\n\nFly.io is one of the easiest ways to take advantage of Elixir community libraries like prom_ex, so you can run your application and watch what it‚Äôs actually doing, with pretty graphs to impress people looking over your shoulder in the cafe!"
  },
  {
    "title": "Fly makes using Observer easy",
    "url": "https://fly.io/blog/observing-elixir-in-production/",
    "content": "Fly networking lets you VPN in and run Observer directly in production. Deploy your Elixir application and try it out!\n\nElixir, Erlang, and really just the BEAM has a feature called ‚ÄúObserver‚Äù. It‚Äôs fun showing it to people new to Elixir because it‚Äôs just so cool! It‚Äôs a WxWidgets graphical interface that connects in realtime to a running Erlang node and lets you ‚Äúobserve‚Äù what‚Äôs going on. It has some limited ability to modify things as well, most notably you can kill running processes. This can help when something is misbehaving or you just want to play ‚Äúchaos monkey‚Äù and kill parts of the system to see how it recovers.\n\nThis picture shows a process tree for the application. Using this I can inspect individual processes or even kill them!\n\nOne very cool way to run Observer is to run it on your local machine (which has the ability to display the UI) and connect to a production server (with no windowing UI available) and ‚Äúobserve‚Äù it from a distance. So yeah‚Ä¶ have a problem in production? Not sure what‚Äôs going on? You can tunnel in, crack the lid and poke, prod, and peek around to see what‚Äôs going on.\n\nThe Fly platform makes it easy to do this for your own applications!\n\n\n## What We Will Do\n\nFly.io natively supports WireGuard, Jason Donenfeld‚Äôs amazing VPN protocol. If you‚Äôve ever lost hours of your life trying to set up an IPSec VPN, you‚Äôll be blown away by how easy WireGuard is. It‚Äôs so flexible and performant that Fly uses it as our network fabric. And it‚Äôs supported on every major platform, including macOS, iOS, Windows, and Linux. What that means for you is that if your app runs on Fly, you can open a secure, private, direct connection from your dev machine to your production network, in less time than it took me to write this paragraph. Cool, right?\n\nThis is what we‚Äôre going to do.\n\nWe will bring up a secure WireGuard tunnel that links to your servers on Fly. In this graphic, there are two my_app Elixir nodes clustered together running on Fly.\n\nFrom the local machine, we can open an IEx terminal configured to join that cluster of remote Elixir nodes. Our local machine supports running Observer and drawing the UI. We use our local observer to talk to the remote nodes in the cluster!\n\n\n## Making It Happen\n\nTo test this out, I follow this guide and apply the changes to the multi-region Tic-Tac-Toe game created here. The github repo for the project is here.\n\nHere‚Äôs what we do:\n\nAgain, follow the guide here for a step-by-step breakdown of how to do it for your project.\n\n\n## Multi-Region Support?\n\nWhen Elixir nodes are clustered together and running in different regions, Observer can connect to any node in the cluster.\n\nAfter making the changes to the TicTac project and deploying it to multiple regions, let‚Äôs see what it looks like.\n\nI have the game scaled out to two regions. One is running in fra (Frankfurt, Germany) and the other is running in lax (Los Angeles, California (US)).\n\nWhen I open Observer locally, I see two remote instances of tictactoe!\n\n\n## Exploring My App\n\nI start playing a game on the web and use Observer to browse around and find the game. It‚Äôs highlighted in blue and linked from GameRegistry.\n\nUsing Observer, I can double-click the selected game process and even view the GenServer‚Äôs state. This gives a snapshot of the state at the time I double-clicked it. I highlighted in yellow some interesting parts of the game state.\n\nIf you‚Äôre wondering why the state data looks strange (at least different from Elixir), it‚Äôs because that‚Äôs the Erlang representation of those data types.\n\n\n## Playing Chaos Monkey\n\nSomething fun you can do with Observer is identify processes, inspect them, and even kill them. This can help when something is misbehaving and you want to see more about what‚Äôs going on. This can also help you test how your system recovers from unexpected failures.\n\nI‚Äôve already identified the running game process. By right-clicking it I see I have the option to ‚Äúkill‚Äù it. What will happen when a game server dies?\n\nAfter killing the game process, I see that a new process was immediately started. How can I tell? The PID (Process ID) value is different.\n\nSo, it looks like my system isolates the damage in that no other running games are impacted when one crashes. Yay! My system stays up and running!\n\nFrom the user‚Äôs perspective playing the game, it‚Äôs not so graceful. The player is able to recover but it requires them to reload their page or restart the game joining processing. I see that my UX can be improved to make crash recovery better for the user.\n\nThat was a productive experiment!\n\n\n## Now You Try\n\nDeploy a Phoenix application to Fly, setup your WireGuard VPN, and start observing your app in production!\n\nWith Elixir you can build resilient systems! Fly makes observing them easy. What will you find digging around in the state pile?"
  },
  {
    "title": "Livebook works awesome on Fly",
    "url": "https://fly.io/blog/livebook-for-app-documentation/",
    "content": "Fly runs apps close to users, you can run pretty much any Docker image on Fly. So we‚Äôve been playing with Livebook.\n\nEvery application has that core, most important thing that it does. It is the reason the application exists. It‚Äôs that central idea that everything else is there to support.\n\nA major hurdle for new developers joining a project can be understanding and becoming comfortable with the code that implements that central idea. So you document the code and may even have 100% code coverage for it.\n\nCode documentation and tests are valuable here. Now with Livebook, there may be a new way to provide documentation, interactive exploration, and expose the app logic in a way we couldn‚Äôt before.\n\n\n## Livebook for App Logic?\n\nWhat is Livebook?\n\nLivebook is Elixir‚Äôs answer to something like Jupyter notebooks. It‚Äôs an exciting project created to help use Nx, Axon and machine learning with Elixir. Livebook is still early days, but it‚Äôs rapidly progressing, it‚Äôs already solid, and has some awesome features.\n\nWhile it was created for the machine learning space, it isn‚Äôt limited to it. Livebook is actually really powerful even when used on a regular Phoenix web project.\n\nBefore I go any further, let me show you what I mean. Previously, I created the game TicTac that let‚Äôs you play distributed Tic-Tac-Toe with a friend from across the world on Fly.io.\n\nUsing Livebook, I documented the central piece that is the GameState. It is a module and struct used to model a game‚Äôs state. This is a glimpse of what it looks like.\n\nLivebook accesses the project‚Äôs code and let‚Äôs me simultaneously document it and execute the code showing the results!\n\n\n## Why Livebook?\n\nThe important features are:\n\nThis means we can do the following:\n\nLivebook makes it easy to experiment with our code, execute it, and demonstrate the point we‚Äôre making.\n\nThis lets developers new to a project experiment and learn that central idea. It also works for experimenting and exploring changes.\n\nBecause the notebook files are simple markdown files, they are easily added to source control and become part of the project if desired. They diff and merge well too.\n\nLet‚Äôs get started!\n\n\n## Livebook Setup\n\nWhen Elixir 1.12 was released, Livebook was updated to require that version. It simplified things for the project and let them safely make certain assumptions. This means you need to have Elixir 1.12 installed.\n\nOnce installed, installing Livebook is this easy.\n\nStarting Livebook is\n\nIt gives us a URL with a randomly generated token we use to connect to our local instance. Go ahead and open it up.\n\nYay! We‚Äôre connected to our Livebook running locally.\n\n\n## Starting Tip\n\nSince we are playing with Livebook on an existing Elixir project, start Livebook from the directory of your project. It works fine without that, starting here just makes it easier for navigating and starting the runtime. More on that later.\n\nYou‚Äôll want your project to be using Elixir 1.12 as well.\n\n\n## Start a Project Notebook\n\nStart a new notebook and name it for your project or some significant part of your project. Don‚Äôt worry though, it‚Äôs just a Markdown file and you can rename it later if you want.\n\nSave it. When you save, select to save it to a file. Navigate to your project directory. I suggest adding a notebook directory to the name before adding your filename.\n\nIt may look something like this:\n\nIn this example, overview would end up named as overview.livemd.\n\nThe ‚Äúnotebook‚Äù directory just gives the project a home where multiple notebook files can be stored.\n\n\n## Connect to your Project\n\nTo connect Livebook to your project, use the ‚ÄúRuntime‚Äù menu item.\n\nUsing the ‚ÄúRuntime Settings‚Äù, select ‚ÄúMix standalone‚Äù. Navigate to your Elixir project and ‚ÄúConnect‚Äù.\n\nIt compiles your project and starts it.\n\nIf everything goes smoothly, when you return to your notebook you can add an Elixir card and as you type, you should have code completion available using CTRL+Space or Cmd+Space.\n\nWith your code loaded, you have complete access to your application running in Livebook! In fact, you can execute Ecto queries, call contexts, start GenServers, anything!\n\n\n## Document Important App Logic\n\nNow that you have Livebook setup with a new notebook and it‚Äôs connected to your your project source code, you are ready to start documenting app logic and showing what‚Äôs important!\n\nThis is what I showed earlier with the documentation and execution of the GameState module. I loved using the code completion to explore it as I documented it! It was a lot of fun writing documentation that can be interactively executed and still have all the benefits of using markdown too!\n\n\n## Run and Interact with GenServers\n\nLivebook also lets you run GenServers and interact with them too!\n\nThe other main part of the TicTac application is how a GenServer runs the GameState. The running server is managed by Horde and I was able to both describe that and show it!\n\nLivebook Tip: When working with a Livebook notebook, the keyboard shortcut you want to know is ea to ‚ÄúEvaluate All‚Äù. After opening a notebook, type ea and everything executes so you can see the commands and the results.\n\n\n## The Case for Livebook with your Project\n\nLivebook is exciting because of what it does for Elixir in the Machine Learning space. However, Livebook can also help your regular-old Phoenix applications!\n\nI hope you can see how Livebook can be used to document central pieces of your application in ways we never could before. Whether you‚Äôre documenting commission calculations, game logic, credit scoring, mortgage payments, or whatever. Livebook can help you do it in a new and awesome way.\n\nPotential benefits:\n\nGive Livebook a try on your project and see how it helps your team!\n\nLivebook supports collaborative editing. When you host your own instance you can invite people to join you!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/hooking-up-fly-metrics/",
    "content": "Fly apps include built in Prometheus instrumentation ‚Äì¬†monitor performance, create alerts, and even export your own metrics. If you haven‚Äôt taken Fly for a spin, now‚Äôs a good time: if you‚Äôve got a Docker container, it can be running on Fly in single-digit minutes.\n\nWe‚Äôve written a bit, for a general audience, about how Fly collects and manages metrics. If you‚Äôre just sort of generally interested in metrics and observability, go read that first.\n\nMeanwhile, if you‚Äôre a Fly user, or considering becoming such a user, here‚Äôs the deal:\n\nAll Fly apps now include built-in Prometheus metrics. We give you a bunch of useful metrics automatically, and we‚Äôll walk through some of them here. More importantly: you can export your own custom metrics. And you should export your own custom metrics, because metrics are supremely useful. We‚Äôll talk a bit about that, too.\n\n\n## A Quick Overview Of Metrics At Fly\n\nThis is one of those ‚Äúpictures worth a thousand words‚Äù scenarios, so, without further ado, here‚Äôs some metrics:\n\nOver the last 30 minutes, for each HTTP response code our app generated, these are the requests per second we‚Äôre seeing.\n\nFly tracks a bunch of metrics like this for you, and you can add your own. We use a Prometheus-style system to do this, and a Prometheus endpoint is part of our public API.\n\nThere‚Äôs not a lot you need to know about Prometheus before diving in. It‚Äôs enough to know that Prometheus metrics have names and labels, and that there are for the most part 3 kinds:\n\nYou query Prometheus using its query language, PromQL. If you‚Äôve used the command-line tool jq, you have a bit of the flavor of PromQL; just think of a jq that specializes in being a programmable calculator. You can read PromQL tutorials, but also like jq, you can get by with just a small subset of it, which is how we‚Äôre going to tackle this walkthrough.\n\nMost people use Prometheus with a graphical front-end, and by far the most popular front-end for Prometheus metrics is Grafana, which is what we‚Äôre going to use here. You can run your own instance of Grafana (it works fine as a Fly app), or you can sign up for a free Grafana Cloud account.\n\n\n## Getting Fly Metrics Into Grafana\n\nEasy! Get a working Grafana somewhere. Now, run flyctl auth token to retrieve a Fly token. In Grafana‚Äôs ‚ÄúConfiguration‚Äù menu, the first option is ‚ÄúData Sources‚Äù. We‚Äôre one of those.\n\n‚ÄúAdd data source‚Äù, and pick Prometheus; it should be the first option. The URL for Fly‚Äôs Prometheus is per organization. Your personal organization is https://api.fly.io/prometheus/personal, other organizations are https://api.fly.io/prometheus/\u003cname\u003e. Turn off all the authentication options, and then add a ‚ÄúCustom Header‚Äù; make it Authorization, and make the value Bearer \u003ctoken\u003e.\n\nSave the data source; Prometheus will check it and make sure it works for you. If you get the green light, you‚Äôre good to go.\n\n\n## A Basic Fly Dashboard\n\nWe set up a simple, clean dashboard with a bunch of graphs on it, and you can pull it directly into your Grafana instance. Go to ‚ÄúManage Dashboards‚Äù, click ‚ÄúImport‚Äù, and then paste in the JSON from this Github link.\n\nGrafana has a whole community site for dashboard JSONs like this, and when we figure out how it works, we‚Äôll try to get ours on it.\n\nNow, let‚Äôs take a tour of the dashboard and see how it works.\n\nThe first thing we want you to notice is the app picker at the top left of the screen. The picker fetches the fly_app_concurrency metric from our API to get a list of your apps, which is stored in the $app variable exposed to all the queries on this dashboard.\n\nIf you can‚Äôt tell, we‚Äôre assuming you‚Äôre not super familiar with Grafana, so the other thing we‚Äôll point out is that there‚Äôs a picker on the other side of the screen that determines the time scale of the metrics we‚Äôre looking at. You‚Äôre probably looking at 15 minutes worth of data, and you should know you can dial that up or down.\n\nNow, a simple graph:\n\nIf you click next to the title on this graph, there‚Äôs a drop-down you can use to edit it. Do that and you‚Äôll see the queries we‚Äôre using:\n\nfly_instance_net_(recv|sent)_bytes is a counter and works the way you‚Äôd think it would. app=‚Äú$app‚Äù narrows us down to just the metrics matching our current app. You could add , region=‚Äúord\" to the expression to narrow us down to metrics from Chicago, and now you basically get the idea for how names and labels work.\n\nWe add [5m] to the query to fetch a window of 5 minutes worth of metrics from, and then irate() to get the rates of increase from the raw counter values (you‚Äôll see irate, rate, and increase in almost every Prometheus counter query, and they all do roughly the same thing in slightly different ways). Remember, Fly‚Äôs systems are scraping your metrics at some interval and generating vectors of counters, and PromQL‚Äôs job is generally to turn those raw values into intelligible time-series information.\n\nFinally, we sum() across all the available metrics we have that match this query, and convert to bits.\n\nOn the right hand pane in the Grafana pane editor you can see that we have this set up as a line graph, and you can click around and muck with those settings if you like. Try switching ‚Äústack‚Äù on to see TX/RX superimposed. Have fun. Meanwhile, we‚Äôre moving on.\n\nConnections aren‚Äôt much more complicated.\n\nfly_app_tcp_(connects|disconnects)_count do what they say on the tin. We use increase here because Jerome is fussy (‚ÄúI always use increase for bar charts‚Äù, he tells me); $__interval is a Grafana-ism for, roughly, ‚Äúhave Grafana figure out the right interval‚Äù.\n\nNow, let‚Äôs get ambitious:\n\nCheck out all its majesty.\n\nThe query here is actually very simple:\n\nYou‚Äôve seen almost all of this before. Note that instead of summarizing all available metrics onto one line, we instead break them out by (region) (and set the legend for each data point accordingly).\n\nWe use the ‚ÄúWorld Map‚Äù visualizer in Grafana to render this. The visualizer accepts a URL for keyed map location data, and, wouldn‚Äôt you know it, our public API has just such a URL endpoint, at https://api.fly.io/meta/regions.json. The map visualizer matches the region in our metrics to the key in each row of the JSON our endpoint returns, and that‚Äôs basically it.\n\nOne more graph we want to walk through; this is a good one:\n\nI don‚Äôt know about you, but I feel smarter just having this on my screen.\n\nAnd the query:\n\nfly_edge_http_response_time_seconds_bucket is a histogram, which is a bucketed stat; the buckets here are indicated by le.\n\nAs the Fly Proxy tracks this metric, it bumps one of a collection of counters; we break them out as 5ms, 25ms, 50ms, 100ms, 200ms, 500ms, 1s, 5s, 15s, and, god help you, 30s, 60s, and 120s. This is fussier than the typical Prometheus metric, which is a simple global counter, but it supports a graph pretty much every wants: ‚Äúhow scary have my response times looked over the last N hours‚Äù.\n\nGrafana has a nice heatmap that you can drive with Prometheus histograms, and we‚Äôre not doing anything interesting with its settings.\n\n\n## Generate Your Own Metrics\n\nFly provides a bunch of built-in metrics, but you don‚Äôt have to (and shouldn‚Äôt) stick with just ours. If you export metrics from your application in Prometheus format, we‚Äôll pick them up, index them in our TSDB, and serve them to things like Grafana over our public API.\n\nFor instance, let‚Äôs say we have a Go application running on Fly. We can:\n\nYou can export stats on the default Go HTTP handler (they‚Äôll be exposed) or on a private handler on a different port or address; either way, you tell us about your metrics in fly.toml:\n\nWhen you deploy, we‚Äôll start scraping, and whatever stats you‚Äôve defined will show up in Grafana.\n\nNot a Golang person? Here‚Äôs what it looks like in Python Flask:\n\nThe prometheus_flask_exporter gets you a couple useful metrics by default, without you doing anything, and, also by default, listens on /metrics.\n\nNode.js? No problem:\n\nHopefully we‚Äôre beginning to see a theme here. If you‚Äôre using any mainstream framework, adding metrics is going to be a very short project for you.\n\nRemember, part of the idea behind Prometheus‚Äôs counter-first mentality is that counters are super cheap. They‚Äôre cheaper both to generate and to track than log lines are; think about them the same way, and instrument your application so you have a decent idea of what it‚Äôs getting up to.\n\n\n## And There You Go\n\nLike everything else on our platform, we want reasonable metrics to be so simple that it‚Äôs boring. They should just work. Even if you do nothing at all, we‚Äôre generating metrics you can pull into Grafana, watch graphs for, and alert on.\n\nPrometheus metrics are free right now. Some tier of them will be free forever. You should know that if you get super ambitious with metrics, we‚Äôll eventually have a pricing plan for them ‚Äî the TSDB that backs metrics costs us operationally to manage and scale. But we‚Äôll keep things simple and cost-effective, because we want all of you to instrument your applications."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/measuring-fly/",
    "content": "Fly.io transforms container images into fleets of micro-VMs running around the world on our hardware. It‚Äôs bananas easy to try it out; if you‚Äôve got a Docker container, it can be running on Fly in single-digit minutes.\n\nWe should talk a bit about metrics and measurement and stuff, because they‚Äôre how we all know what‚Äôs going on.\n\nThere‚Äôs two reasons we‚Äôve written this post. The first is just that we think this stuff is interesting, and that the world can always use another detailed case study. The second, though, is that the work we‚Äôre going to describe is now part of our public API, and you can piggyback your own application metrics on top of ours. Our hope is, the more we describe this stuff, the more likely you are to get value out of it.\n\n\n## Counting\n\nHere‚Äôs where we‚Äôll start: an HTTP request arrives for an app running on Fly. It lands on one of our ‚Äúedge nodes‚Äù, at fly-proxy, our Rust request router. The proxy‚Äôs job is to deliver the request to the closest unloaded ‚Äúworker node‚Äù, which hosts Firecracker VMs running user applications.\n\nSomewhere in fly-proxy‚Äôs memory there‚Äôs a counter, for the number of incoming bytes the proxy has handled. While routing the incoming request, the proxy bumps the counter.\n\nBumping a counter is cheap. So there‚Äôs counters, and things that act on them, all over the edge node. The operating system counts packets processed, TCP SYN segments seen, bytes processed per interface. CPU load. Free memory. Naturally, the proxy counts a bunch of things: a histogram of the times taken to complete TLS handshakes, number of in-flight connections, and so on. Some counts are global, others per-user.\n\nThe proxy directs the request over a WireGuard interface (more counters) to the worker node, which also runs fly-proxy (still more). The worker‚Äôs proxy finds the tap interface associated with the application and routes the request to its address (you guessed it). Inside the VM, where the user‚Äôs app is running, the guest OS counts a series of arriving Ethernet frames, a newly opened TCP connection, memory used, CPU cycles burned. Your app running in that VM might count a new HTTP connection.\n\nWe count everything we can think to count, because computers like counting stuff, so it‚Äôs cheap. You‚Äôd do the same thing even if you were building on a potato-powered MSP430 microcontroller.\n\n\n## Those who cannot remember the Borgmon are doomed to repeat it\n\nTo do anything with this information we need to collect it. There was once in our industry a lively debate about how to accomplish this, but Google settled it. Services expose HTTP endpoints that dump the stats; if your service can‚Äôt do that, it arranges to have something else do it for them.\n\nGoogle called these web pages /varz, and filled them with a line-by-line human-readable format (apocryphally: intended for human readers) that Google‚Äôs own tools learned to parse. In the real world, we‚Äôve kept the text format, and call the pages /metrics. Here‚Äôs what it looks like:\n\nIf you‚Äôre an Advent of Code kind of person and you haven‚Äôt already written a parser for this format, you‚Äôre probably starting to feel a twitch in your left eyelid. Go ahead and write the parser; it‚Äôll take you 15 minutes and the world can‚Äôt have too many implementations of this exposition format. There‚Äôs a lesson about virality among programmers buried in here somewhere.\n\nTo collect metrics, you scrape all the /metrics pages you know about, on an interval, from a collector with some kind of indexed storage. Give that system a query interface and now it‚Äôs it‚Äôs a time-series database (a TSDB). We can now generate graphs and alerts.\n\nWhat we‚Äôre describing is, obviously, Prometheus, which is an escaped implementation of Borgmon, Google‚Äôs cluster monitoring system. Google SREs have a complicated relationship with Borgmon, which was once said to have been disclosed in an ‚Äúindustry-disabling paper‚Äù. A lot of people hate it.\n\nNone of us have ever worked for Google, let alone as SREs. So we‚Äôre going out on a limb: this is a good design; it‚Äôs effective, so easy to implement that every serious programming environment supports it, and neatly separates concerns: things that generate metrics just need to keep counters and barf them out into an HTTP response, which makes it easy to generate lots of metrics.\n\n\n## Metrics Databases\n\nThe division of labor between components in this design is profoundly helpful on the database side as well. What makes general-purpose database servers hard to optimize is that they‚Äôre general-purpose: it‚Äôs hard to predict workloads. TSDBs don‚Äôt have this problem. Every Prometheus store shares a bunch of exploitable characteristics:\n\nKnowing this stuff ahead of time means you can optimize. You‚Äôll use something like an LSM tree to address the bias towards writes, jamming updates into a write-ahead log, indexing them in memory, gradually flushing them to immutable disk chunks. You can compress; timestamps recorded on a relatively consistent interval are especially amenable to compression. You can downsample older data. You can use a column store instead of a row store.\n\nAnd, of course, you can scale this stuff out in a bunch of different ways; you can have Prometheus servers scrape and aggregate other Prometheus servers, or spill old chunks of data out to S3, or shard and hash incoming data into to a pool of servers.\n\nPoint being, keeping an indefinite number of time-series metrics available for queries is a tractable problem. Even a single server will probably hold up longer than you think. And when you you need to, you can scale it out to keep up.\n\nWhich is what we‚Äôve done at Fly. And, since we‚Äôve built this, there‚Äôs no sense in keeping it to ourselves; if you‚Äôre running an app on Fly, you can hand us your metrics too. Let‚Äôs describe how that works.\n\n\n## Our Metrics Stack\n\nA typical Fly.io POP will run some small number of lightweight edge nodes, a larger number of beefier worker nodes, and, in some cases, an infra host or two. All of these nodes ‚Äî within and between POPs ‚Äî are connected with a WireGuard mesh.\n\nOur metrics stack ‚Äî which is built around Prometheus-style metrics ‚Äî features the following cast of characters:\n\nSteve here\n\nFor metrics nerds, the only interesting thing about this stack is Vicky. The story there is probably boring. Like everyone else, we started with a simple Prometheus server. That worked until it didn‚Äôt. We spent some time scaling it with Thanos, and Thanos was a lot, as far as ops hassle goes. We‚Äôd dabbled with Vicky just as a long-term storage engine for vanilla Prometheus, with promxy set up to deduplicate metrics.\n\nVicky grew into a more ambitious offering, and added its own Prometheus scraper; we adopted it and scaled it as far as we reasonably could in a single-node configuration. Scaling requirements ultimately pushed us into a clustered deployment; we run an HA cluster (fronted by haproxy). Current Vicky has a really straightforward multi-tenant API ‚Äî it‚Äôs easy to namespace metrics for customers ‚Äî and it chugs along for us without too much minding.\n\nAssume, in the abstract, an arbitrarily-scaling central Vicky cluster. Here‚Äôs a simplified view of an edge node:\n\nSo far so simple. Edge nodes take traffic from the Internet and route it to worker nodes. Our edge fly-proxy exports Prometheus stats, and so does a standard Prometheus node-exporter , to provide system stats (along with a couple other exporters that you can sort of mentally bucket into the node-exporter).\n\nJerome here\n\nFly-proxy uses the excellent metrics Rust crate with its sibling metrics-exporter-prometheus.\n\nThe proxy hosts further logic for transforming internal metrics into user-relevant metrics. We built our own metrics::Recorder that sends metrics through default prometheus Recorder, but also tees some of them off for users, rewriting labels in the process.\n\nHere‚Äôs a worker node:\n\nThere‚Äôs more going on here!\n\nA worker node hosts user app instances. We use HashiCorp Nomad to orchestrate Firecrackers; one of the three biggest components of our system is driver we wrote for Nomad that manages Firecrackers. Naturally, Nomad exports a bunch of metrics to Telegraf.\n\nReal quick though, one more level of detail:\n\nIn each Firecracker instance, we run our custom init, which launches the user‚Äôs application. Our Nomad driver, our init, and Firecracker conspire to establish a vsock ‚Äî a host Unix domain socket that presents as a synthetic Virtio device in the guest ‚Äî that allows init to communicate with the host; we bundle node-exporter-type JSON stats over the vsock for Nomad to collect and relay to Vicky.\n\nThe Firecracker picture includes one of the more important details in our system. These VMs are all IP-addressable. If you like (and you should!), you can expose a Prometheus exporter in your app, and then tell us about it in your fly.toml; it might look like this:\n\nWhen you deploy, our system notices those directives, and will arrange for your metrics to end up in our Vicky cluster.\n\nZooming out, our metrics architecture logically looks something like this:\n\n\n## Metrics \u003e Checks\n\nWhen it comes to automated monitoring, there are two big philosophies, ‚Äúchecks‚Äù and ‚Äúmetrics‚Äù. ‚ÄúChecks‚Äù is what many of us grew up with: you write a script that probes the system and makes sure it‚Äôs responding the way you expect it to. Simple.\n\n‚ÄúMetrics‚Äù is subtler. Instead of running scripts, you export metrics. Metrics are so much cheaper than checks that you end up tracking lots more things. You write rules over metrics to detect anomalies, like ‚Äúmore 502 responses than expected‚Äù, or ‚Äúsudden drop in requests processed‚Äù, or ‚Äú95th percentile latency just spiked‚Äù. Because you‚Äôve got a historical record of metrics, you don‚Äôt even need to know in advance what rules you need. You just write them as you think of them.\n\nThat‚Äôs ‚Äúwhite box monitoring‚Äù. Use metrics to open up your systems as much as you can, then build monitoring on top, so you can ramp up the monitoring over time without constantly redeploying new versions of your applications.\n\nWhat‚Äôs more, you‚Äôre not just getting alerting; the same data flow gives you trending and planning and pretty graphs to put on LCD status boards on your wall.\n\nOnce again, the emerging standard for how to accomplish this appears to be Prometheus. It‚Äôs easy to add Prometheus exporters to your own applications, and you should do so, whether or not you run your app on Fly. Something will eventually collect those metrics and put them to good use!\n\n\n## At Any Rate\n\nSo that‚Äôs the metrics infrastructure we built, and the observability ideas we shoplifted to do it.\n\nThe neat thing about all of this is that Prometheus metrics are part of our public API. You can take them out for a spin with the free-plan Grafana Cloud right now; sign up with your Github account and you can add our API as a Prometheus data source. The URL will be https://api.fly.io/prometheus/$(your-org) ‚Äî if you don‚Äôt know your org, personal works ‚Äî and you‚Äôll authenticate with a customer Authorization header Bearer $(flyctl auth token).\n\nAdd a dashboard (the plus sign on the left menu), add an empty panel, click ‚ÄúMetrics‚Äù, and you‚Äôll get a dropdown of all the current available metrics. For instance:\n\nYou can display any metric as a table to see the available labels; you can generally count on ‚Äúapp‚Äù, ‚Äúregion‚Äù, ‚Äúhost‚Äù (an identifier for the worker node your app is running on) and ‚Äúinstance‚Äù. Poke around and see what‚Äôs there!\n\nRemember, again, that if you‚Äôre exporting your own Prometheus metrics and you‚Äôve told us about it in fly.toml, those metrics will show up in Grafana as well; we‚Äôll index and store and handle queries on them for you, with the networking and access control stuff taken care of automatically."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/building-a-distributed-turn-based-game-system-in-elixir/",
    "content": "We‚Äôre Fly.io, and we run full-stack apps on our hardware around the world. This, it turns out, makes us one of the easiest places in the world to run clustered and distributed Elixir apps, and if you‚Äôre into Elixir, you should give us a try; it‚Äôll take just a couple minutes to get started.\n\nOne of the best things about building web applications in Elixir is LiveView, the Phoenix Framework feature that makes it easy to create live and responsive web pages without all the layers people normally build.\n\nMany great Phoenix LiveView examples exist. They often show the ease and power of LiveView but stop at multiple browsers talking to a single web server. I wanted to go further and create a fully clustered, globally distributed, privately networked, secure application. What‚Äôs more, I wanted to have fun doing it.\n\nSo I set out to see if I could create a fully distributed, clustered, privately networked, global game server system. Spoiler Alert: I did.\n\n\n## What I didn‚Äôt have to build\n\nWhat I find remarkable is what I didn‚Äôt need to build.\n\nI didn‚Äôt build a Javascript front end using something like React.js or Vue.js. That is the typical approach. Building a JS front-end means I need JS components, a front-end router, a way to model the state in the browser, a way to transfer player actions to the server and a way to receive state updates from the server.\n\nOn the server, I didn‚Äôt build an API. Typically that would be REST or GraphQL with a JSON structure for transferring data to and from the front-end.\n\nI didn‚Äôt need other external systems like Amazon SQS, Kafka, or even just Redis to pass state between servers. This means the entire system requires less cross-technology knowledge or specialized skills to build and maintain it. I used Phoenix.PubSub which is built on technology already in Elixir‚Äôs VM, called the BEAM. I used the Horde library to provide a distributed process registry for finding and interacting with GameServers.\n\nAs for Fly.io‚Äôs WireGuard connected private network between geographically distant regions and data centers? I don‚Äôt even know how I would have done that in AWS, which is why I‚Äôve always given up on the idea.\n\n\n## What I did build\n\nWhat I built was just a proof of concept, but I‚Äôm surprised at how it came together. I ended up with a platform that can host many different types of games, all of which:\n\nAnd, just one little extra detail: the platform supports multiple connected servers operating together in clusters. Elixir for the win!\n\nI created this as an open source project on Github, so you can check it out yourself.\n\nhttps://github.com/fly-apps/tictac\n\n\n## Technology\n\nI‚Äôve worked with enough companies and teams to imagine several different approaches to build a system like this. Those approaches would all require large multi-disciplinary teams like a front-end JS team, a backend team, a DevOps team, and more. In contrast, I set out to do this by myself, in my spare time, and with a whole lot of ‚Äúlife‚Äù happening too.\n\nHere‚Äôs what I chose to use:\n\n\n## Application Architecture\n\nThere are many guides to getting started with LiveView, I‚Äôm not focusing on that here. However, for context, this demonstrates the application architecture when running on a local machine.\n\nThe ‚ÄúABCD‚Äù in the graphic is a running game identified by the 4-letter code ‚ÄúABCD‚Äù.\n\nLet‚Äôs walk it through.\n\n\n## We need a game\n\nI needed a simple game to play and model for this game system. I chose Tic-Tac-Toe. Why?\n\nI want to emphasize that this system can be used to build many turn-based, multi-user games! This simple Tic-Tac-Toe game covers all of the basics we will need. Besides, Tic-Tac-Toe was even made into a TV Show!\n\nThis is what the game looks like with 2 players.\n\nThe game system works great locally. Let‚Äôs get it deployed!\n\n\n## Hosting on Fly.io\n\nFollowing the Fly.io Getting Started Guide for Elixir, I created a Dockerfile to generate a release for my application. Check out the repo here:\n\nhttps://github.com/fly-apps/tictac\n\nThe README file outlines both how to run it locally and deploy it globally on Fly.io.\n\nWhat is special about hosting it on Fly.io? Fly makes it easy to deploy a server geographically closer to the users I want to reach. When a user goes to my website, they are directed to my nearest server. This means any responsive LiveView updates and interactions will be even faster and smoother because the regular TCP and websocket connections are just that much physically closer.\n\nBut for the game, I wanted there to be a single source of truth. That GameServer can only exist in one place. Supporting a private, networked, and fully clustered environment means my server in the EU can communicate with the GameServer that might be running in the US. But my EU players have a fast and responsive UI connection close to them. This provides a better user experience!\n\nHere is what I find compelling about Fly.io for hosting Elixir applications:\n\n\n## Conclusion\n\nFor a proof-of-concept, I couldn‚Äôt be happier! In a short time, by myself, I created a working, clustered, distributed, multi-player, globe-spanning gaming system!\n\nThe pairing of Elixir + LiveView + Fly.io is excellent. Using Elixir and LiveView, I built a powerful, resilient, and distributed system in orders of magnitude shorter time and effort. Deploying it on Fly.io let let me easily do something I would never have even tried before, namely, deploying servers in regions around the globe while keeping the application privately networked and clustered together.\n\nWhenever I‚Äôve thought of creating a service with a global audience, I‚Äôd usually scapegoat the idea saying, ‚ÄúWell I don‚Äôt know how I‚Äôd get the translations, so I‚Äôll just stick with the US. It‚Äôs a huge market anyway.‚Äù In short, I‚Äôve never even considered a globally connected application because it would be ‚Äúway too hard‚Äù.\n\nBut here, with Elixir + LiveView + Fly.io, I did something by myself in my spare time that larger teams using more technologies struggle to deliver. I‚Äôm still mind blown by it!\n\n\n## What will you build?\n\nTic-Tac-Toe is a simple game and doesn‚Äôt provide ‚Äúhours of fun‚Äù. I know you can think of a much cooler and more interesting multi-player, turn-based game that you could build on a system like this. What do you have in mind?\n\nDiscuss this project (and more) in our community forums."
  },
  {
    "title": "This was all Andrew Dunham‚Äôs idea.You can play with this right now.",
    "url": "https://fly.io/blog/docker-without-docker/",
    "content": "We‚Äôre Fly.io. We take container images and run them on our hardware around the world. It‚Äôs pretty neat, and you should check it out; with an already-working Docker container, you can be up and running on Fly in well under 10 minutes.\n\nEven though most of our users deliver software to us as Docker containers, we don‚Äôt use Docker to run them. Docker is great, but we‚Äôre high-density multitenant, and despite strides, Docker‚Äôs isolation isn‚Äôt strong enough for that. So, instead, we transmogrify container images into Firecracker micro-VMs.\n\nLet‚Äôs demystify that.\n\n\n## What‚Äôs An OCI Image?\n\nThey do their best to make it look a lot more complicated, but OCI images ‚Äî OCI is the standardized container format used by Docker ‚Äî are pretty simple. An OCI image is just a stack of tarballs.\n\nBacking up: most people build images from Dockerfiles. A useful way to look at a Dockerfile is as a series of shell commands, each generating a tarball; we call these ‚Äúlayers‚Äù. To rehydrate a container from its image, we just start the the first layer and unpack one on top of the next.\n\nYou can write a shell script to pull a Docker container from its registry, and that might clarify. Start with some configuration; by default, we‚Äôll grab the base image for golang:\n\nWe need to authenticate to pull public images from a Docker registry ‚Äì this is boring but relevant to the next section ‚Äì and that‚Äôs easy:\n\nThat token will allow us to grab the ‚Äúmanifest‚Äù for the container, which is a JSON index of the parts of a container.\n\nThe first query we make gives us the ‚Äúmanifest list‚Äù, which gives us pointers to images for each supported architecture:\n\nPull the digest out of the matching architecture entry and perform the same fetch again with it as an argument, and we get the manifest: JSON pointers to each of the layer tarballs:\n\nIt‚Äôs as easy to grab the actual data associated with these entries as you‚Äôd hope:\n\nAnd with those pieces in place, pulling an image is simply:\n\nUnpack the tarballs in order and you‚Äôve got the filesystem layout the container expects to run in. Pull the ‚Äúconfig‚Äù JSON and you‚Äôve got the entrypoint to run for the container; you could, I guess, pull and run a Docker container with nothing but a shell script, which I‚Äôm probably the 1,000th person to point out. At any rate, here‚Äôs the whole thing.\n\nYou‚Äôre likely of one of two mindsets about this: (1) that it‚Äôs extremely Unixy and thus excellent, or (2) that it‚Äôs extremely Unixy and thus horrifying.\n\nUnix tar is problematic. Summing up Aleksa Sarai: tar isn‚Äôt well standardized, can be unpredictable, and is bad at random access and incremental updates. Tiny changes to large files between layers pointlessly duplicate those files; the poor job tar does managing container storage is part of why people burn so much time optimizing container image sizes.\n\nAnother fun detail is that OCI containers share a security footgun with git repositories: it‚Äôs easy to accidentally build a secret into a public container, and then inadvertently hide it with an update in a later image.\n\nWe‚Äôre of a third mindset regarding OCI images, which is that they are horrifying, and that‚Äôs liberating. They work pretty well in practice! Look how far they‚Äôve taken us! Relax and make crappier designs; they‚Äôre all you probably need.\n\nSpeaking of which:\n\n\n## Multi-Tenant Repositories\n\nBack to Fly.io. Our users need to give us OCI containers, so that we can unpack and run them. There‚Äôs standard Docker tooling to do that, and we use it: we host a Docker registry our users push to.\n\nRunning an instance of the Docker registry is very easy. You can do it right now; docker pull registry \u0026\u0026 docker run registry. But our needs are a little more complicated than the standard Docker registry: we need multi-tenancy, and authorization that wraps around our API. This turns out not to be hard, and we can walk you through it.\n\nA thing to know off the bat: our users drive Fly.io with a command line utility called flyctl. flyctl is a Go program (with public source) that runs on Linux, macOS, and Windows. A nice thing about working in Go in a container environment is that the whole ecosystem is built in the same language, and you can get a lot of stuff working quickly just by importing it. So, for instance, we can drive our Docker repository clientside from flyctl just by calling into Docker‚Äôs clientside library.\n\nIf you‚Äôre building your own platform and you have the means, I highly recommend the CLI-first tack we took. It is so choice. flyctl made it very easy to add new features, like databases, private networks, volumes, and our bonkers SSH access system.\n\nOn the serverside, we started out simple: we ran an instance of the standard Docker registry with an authorizing proxy in front of it. flyctl manages a bearer token and uses the Docker APIs to initiate Docker pushes that pass that token; the token authorizes repositories serverside using calls into our API.\n\nWhat we do now isn‚Äôt much more complicated than that. Instead of running a vanilla Docker registry, we built a custom repository server. As with the client, we get a Docker registry implementation just by importing Docker‚Äôs registry code as a Go dependency.\n\nWe‚Äôve extracted and simplified some of the Go code we used to build this here, just in case anyone wants to play around with the same idea. This isn‚Äôt our production code (in particular, all the actual authentication is ripped out), but it‚Äôs not far from it, and as you can see, there‚Äôs not much to it.\n\nOur custom server isn‚Äôt architecturally that different from the vanilla registry/proxy system we had before. We wrap the Docker registry API handlers with authorizer middleware that checks tokens, references, and rewrites repository names. There are some very minor gotchas:\n\nIn both cases, the source of truth for who has which repositories and where is the database that backs our API server. Your push carries a bearer token that we resolve to an organization ID, and the name of the repository you‚Äôre pushing to, and, well, our design is what you‚Äôd probably come up with to make that work. I suppose my point here is that it‚Äôs pretty easy to slide into the Docker ecosystem.\n\n\n## Building And Running VMs\n\nThe pieces are on the board:\n\nWhat we need to do now is arrange those pieces so that we can run containers as Firecracker VMs.\n\nAs far as we‚Äôre concerned, a container image is just a stack of tarballs and a blob of configuration (we layer additional configuration in as well). The tarballs expand to a directory tree for the VM to run in, and the configuration tells us what binary in that filesystem to run when the VM starts.\n\nMeanwhile, what Firecracker wants is a set of block devices that Linux will mount as it boots up.\n\nThere‚Äôs an easy way on Linux to take a directory tree and turn it into a block device: create a file-backed loop device, and copy the directory tree into it. And that‚Äôs how we used to do things. When our orchestrator asked to boot up a VM on one of our servers, we would:\n\nThis is all a few thousand lines of Go.\n\nThis system worked, but wasn‚Äôt especially fast. Part of the point of Firecracker is to boot so quickly that you (or AWS) can host Lambda functions in it and not just long-running programs. A big problem for us was caching; a server in, say, Dallas that‚Äôs asked to run a VM for a customer is very likely to be asked to run more instances of that server (Fly.io apps scale trivially; if you‚Äôve got 1 of something running and would be happier with 10 of them, you just run flyctl scale count 10). We did some caching to try to make this faster, but it was of dubious effectiveness.\n\nThe system we‚Äôd been running was, as far as container filesystems are concerned, not a whole lot more sophisticated than the shell script at the top of this post. So Jerome replaced it.\n\nWe asked if he wanted credit and he hesitated and said maybe and we said we‚Äôd keep it subtle, so here you go. He doesn‚Äôt work for us, he‚Äôs just awesome.\n\nWhat we do now is run, on each of our servers, an instance of containerd. containerd does a whole bunch of stuff, but we use it as as a cache.\n\nIf you‚Äôre a Unix person from the 1990s like I am, and you just recently started paying attention to how Linux storage works again, you‚Äôve probably noticed that a lot has changed. Sometime over the last 20 years, the block device layer in Linux got interesting. LVM2 can pool raw block devices and create synthetic block devices on top of them. It can treat block device sizes as an abstraction, chopping a 1TB block device into 1,000 5GB synthetic devices (so long as you don‚Äôt actually use 5GB on all those devices!). And it can create snapshots, preserving the blocks on a device in another synthetic device, and sharing those blocks among related devices with copy-on-write semantics.\n\ncontainerd knows how to drive all this LVM2 stuff, and while I guess it‚Äôs out of fashion to use the devmapper backend these days, it works beautifully for our purposes. So now, to get an image, we pull it from the registry into our server-local containerd, configured to run on an LVM2 thin pool. containerd manages snapshots for every instance of a VM/container that we run. Its API provides a simple ‚Äúlease‚Äù-based garbage collection scheme; when we boot a VM, we take out a lease on a container snapshot (which synthesizes a new block device based on the image, which containerd unpacks for us); LVM2 COW means multiple containers don‚Äôt step on each other. When a VM terminates, we surrender the lease, and containerd eventually GCs.\n\nThe first deployment of a VM/container on one of our servers does some lifting, but subsequent deployments are lightning fast (the VM build-and-boot process on a second deployment is faster than the logging that we do).\n\n\n## Some Words About Init\n\nJerome wrote our init in Rust, and, after being cajoled by Josh Triplett, we released the code, which you can go read.\n\nThe filesystem that Firecracker is mounting on the snapshot checkout we create is pretty raw. The first job our init has is to fill in the blanks to fully populate the root filesystem with the mounts that Linux needs to run normal programs.\n\nWe inject a configuration file into each VM that carries the user, network, and entrypoint information needed to run the image. init reads that and configures the system. We use our own DNS server for private networking, so init overrides resolv.conf. We run a tiny SSH server for user logins over WireGuard; init spawns and monitors that process. We spawn and monitor the entry point program. That‚Äôs it; that‚Äôs an init.\n\n\n## Putting It All Together\n\nSo, that‚Äôs about half the idea behind Fly.io. We run server hardware in racks around the world; those servers are tied together with an orchestration system that plugs into our API. Our CLI, flyctl, uses Docker‚Äôs tooling to push OCI images to us. Our orchestration system sends messages to servers to convert those OCI images to VMs. It‚Äôs all pretty neato, but I hope also kind of easy to get your head wrapped around.\n\nThe other ‚Äúhalf‚Äù of Fly is our Anycast network, which is a CDN built in Rust that uses BGP4 Anycast routing to direct traffic to the nearest instance of your application. About which: more later.\n\nIt‚Äôll take less than 10 minutes to get almost any container you‚Äôve got running globally on our Rust-powered anycast proxy network."
  },
  {
    "title": "That‚Äôs not how we‚Äôd do it",
    "url": "https://fly.io/blog/the-5-hour-content-delivery-network/",
    "content": "The term ‚ÄúCDN‚Äù (‚Äúcontent delivery network‚Äù) conjures Google-scale companies managing huge racks of hardware, wrangling hundreds of gigabits per second. But CDNs are just web applications. That‚Äôs not how we tend to think of them, but that‚Äôs all they are. You can build a functional CDN on an 8-year-old laptop while you‚Äôre sitting at a coffee shop. I‚Äôm going to talk about what you might come up with if you spend the next five hours building a CDN.\n\nIt‚Äôs useful to define exactly what a CDN does. A CDN hoovers up files from a central repository (called an origin) and stores copies close to users. Back in the dark ages, the origin was a CDN‚Äôs FTP server. These days, origins are just web apps and the CDN functions as a proxy server. So that‚Äôs what we‚Äôre building: a distributed caching proxy.\n\n\n## Caching proxies\n\nHTTP defines a whole infrastructure of intricate and fussy caching features. It‚Äôs all very intimidating and complex. So we‚Äôre going to resist the urge to build from scratch and use the work other people have done for us.\n\nWe have choices. We could use Varnish (scripting! edge side includes! PHK blog posts!). We could use Apache Traffic Server (being the only new team this year to use ATS!). Or we could use NGINX (we‚Äôre already running it!). The only certainty is that you‚Äôll come to hate whichever one you pick. Try them all and pick the one you hate the least.\n\n(We kid! Netlify is built on ATS. Cloudflare uses NGINX. Fastly uses Varnish.)\n\nWhat we‚Äôre talking about building is not basic. But it‚Äôs not so bad. All we have to do is take our antique Rails setup and run it in multiple cities. If we can figure out how to get people in Australia to our server in Sydney and people in Chile to our server in Santiago, we‚Äôll have something we could reasonably call a CDN.\n\n\n## Traffic direction\n\nRouting people to nearby servers is a solved problem. You basically have three choices:\n\nYou‚Äôre probably going to use a little of (1) and a little of (2). DNS load balancing is pretty simple. You don‚Äôt really even have to build it yourself; you can host DNS on companies like DNSimple, and then define rules for returning addresses. Off you go!\n\nAnycast is more difficult. We have more to say about this ‚Äî but not here. In the meantime, you can use us, and deploy an app with an Anycast address in about 2 minutes. This is bias. But also: true.\n\nBoom, CDN. Put an NGINX in each of a bunch of cities, run DNS or Anycast for traffic direction, and you‚Äôre 90% done. The remaining 10% will take you months.\n\n\n## The Internet is breaking\n\nThe briny deeps are filled with undersea cables, crying out constantly to nearby ships: ‚Äúdrive through me‚Äù! Land isn‚Äôt much better, as the old networkers shanty goes: ‚Äúbackhoe, backhoe, digging deep ‚Äî make the backbone go to sleep‚Äù. When you run a server in a single location, you don‚Äôt so much notice this. Run two servers and you‚Äôll start to notice. Run servers around the world and you‚Äôll notice it to death.\n\nWhat‚Äôs cool is: running a single NGINX in multiple cities gives you a lot of ready-to-use redundancy. If one of them dies for some reason, there are bunch more to send traffic to. When one of your servers goes offline, the rest are still there serving most of your users.\n\nIt‚Äôs tedious but straightforward to make this work. You have health checks (aside: when CDN regions break, they usually break by being slow, so you‚Äôd hope your health checks catch that too). They tell you when your NGINX servers fail. You script DNS changes or withdraw BGP routes (perhaps just by stopping your BGP4 service on those regions) in response.\n\nThat‚Äôs server failure, and it‚Äôs easy to spot. Internet burps are harder to detect. You‚Äôll need to run external health checks, from multiple locations. It‚Äôs easy to get basic, multi-perspective monitoring ‚Äì¬†we use Datadog and updown.io, and we‚Äôre building out our own half-built home grown service. You‚Äôre not asking for much more than what cURL will tell you. Again: the thing you‚Äôre super wary about in a CDN is a region getting slow, not falling off the Internet completely.\n\nQuick aside: notice that all those monitoring options work from someone else‚Äôs data center to your data center. DC-DC traffic is a good start, enough for a lot of jobs. But it isn‚Äôt representative. Your users aren‚Äôt in data centers (I hope). When you‚Äôre really popular, what you want is monitoring from the vantage point of actual clients. For this, you can find hundreds of companies selling RUM (real user monitoring), which usually takes the form of surreptitiously embedded Javascript bugs. There‚Äôs one rum we like. It‚Äôs sold by a company called Plantation and it‚Äôs aged in wine casks. Drink a bunch of it, and then do your own instrumentation with Honeycomb.\n\nRidiculous Internet problems are the worst. But the good news about them is, everyone is making up the solutions as they go along, so we don‚Äôt have to talk about them so much. Caching is more interesting. So let‚Äôs talk about onions.\n\n\n## The Golden Cache Hit Ratio\n\nThe figure of merit in cache measurement is ‚Äúcache ratio‚Äù. Cache ratio measures how often we‚Äôre able to server from our cache, versus the origin.\n\nA cache ratio of 80% just means ‚Äúwhen we get a request, we can serve it from cache 80% of the time, and the remaining 20% of the time we have to proxy the request to the origin‚Äù. If you‚Äôre building something that wants a CDN, high cache ratios are good, and low cache ratios are bad.\n\nIf you followed the link earlier in the post to the Github repository, you might‚Äôve noticed that our na√Øve NGINX setup is an isolated single server. Deploying it in twenty places gives us twenty individual servers. It‚Äôs dead simple. But the simplicity has a cost ‚Äì there‚Äôs no per-region redundancy. All twenty servers will need to make requests to the origin. This is brittle, and cache ratios will suffer. We can do better.\n\nThe simple way to increase redundancy is to add a second server in each region. But doing that might wreck cache ratios. The single server has the benefit of hosting a single cache for all users; with two, you‚Äôve got twice the number of requests per origin, and twice the number of cache misses.\n\nWhat you want to do is teach your servers to talk to each other, and make them ask their friends for cache content. The simplest way to do this is to create cache shards ‚Äì¬†split the data up so each server is responsible for a chunk of it, and everyone else routes requests to the cache shard that owns the right chunk.\n\nThat sounds complicated, but NGINX‚Äôs built in load balancer supports hash based load balancing. It hashes requests, and forwards the ‚Äúsame request‚Äù to same server, assuming that server is available. If you‚Äôre playing the home version of this blog post, here‚Äôs a ready to go example of an NGINX cluster that discovers its peers, hashes the URL, and serves requests through available servers.\n\nWhen requests for a.jpg hit our NGINX instances, they will all forward the request to the same server in the cluster. Same for b.jpg. This setup has servers serve as both the load balancing proxy and the storage shard. You can separate these layers, and you might want to if you‚Äôre building more advanced features into your CDN.\n\n\n## A small, financially motivated aside\n\nOur clustered NGINX example uses Fly-features we think are really cool. Persistent volumes help keep cache ratios high between NGINX upgrades. Encrypted private networking makes secure NGINX to NGINX communications simple and keeps you from having to do complicated mTLS gymnastics. Built in DNS service discovery helps keep the clusters up to date when we add and remove servers. If it sounds a little too perfectly matched, it‚Äôs because we built these features specifically for CDN-like-workloads.\n\nBut of course, you can do all this stuff anywhere, not just on Fly. But it‚Äôs easy on Fly.\n\n\n## Onions have layers\n\nTwo truths: a high cache ratio is good, the Internet is bad. If you like killing birds and conserving stones, you‚Äôll really enjoy solving for cache ratios and garbage Internet. The answer to both of those problems involves getting the Internet‚Äôs grubby hands off our HTTP requests. A simple way to increase cache ratios: bypass the out-of-control Internet and proxy origin requests through networks you trust to behave themselves.\n\nCDNs typically have servers in regions close to their customers‚Äô origins. If you put our NGINX example in Virginia, you suddenly have servers close to AWS‚Äôs largest region. And you definitely have customers on AWS. That‚Äôs the advantage of existing alongside a giant powerful monopoly!\n\nYou can, with a little NGINX and proxy magic, send all requests through Virginia on their way to the origin servers. This is good. There are fewer Internet bear traps between your servers in Virginia and your customers‚Äô servers in us-east-1. And now you have a single, canonical set of servers to handle a specific customers‚Äô requests.\n\nGood news. This setup improves your cache ratio AND avoids bad Internet. For bonus points, it‚Äôs also the foundation for extra CDN features.\n\nIf you‚Äôve ever gone CDN shopping, you‚Äôve come across things like ‚ÄúShielding‚Äù and ‚ÄúRequest Coalescing‚Äù. Origin shielding typically just means sending all traffic through a known data center. This can minimize traffic to origin servers, and also, because you probably know the IPs your CDN regions use, you can control access with simple L4 firewall rules.\n\nReally. Firewalls are the wrong way to protect origin servers. Like WireGuard. We support WireGuard tunnels so you can talk to your origin privately.\n\nCoalescing requests also minimizes origin traffic, especially during big events when many users are trying to get at the same content. When 100,000 users request your latest cleverly written blog post at once, and it‚Äôs not yet cached, that could end up meaning 100k concurrent requests to your origin. That‚Äôs a face melting level of traffic for most origins. Solving this is a matter of ‚Äúlocking‚Äù a specific URL to ensure that if an NGINX server is making an origin request, the other clients pause until the cache is file. In our clustered NGINX example, this is a two line configuration.\n\n\n## Oh no, slow\n\nProxying through a single region to increase cache ratios is a little bit of a cheat. The entire purpose of a CDN is to speed things up for users. Sending requests from Singapore to Virginia will make things barely faster, because a set of NGINX servers with cached content is almost always faster than origin services. But, really, it‚Äôs slow and undesirable.\n\nYou can solve this with more onion layers:\n\nRequests in Australia could run through Singapore on the way to Virginia. Even light is slow over 14,624 kilometers (Australia to Virginia), so Australia to Singapore (4,300 kilometers) with a cache cuts a perceptible amount of latency. It will be a little slower on cache misses. But we‚Äôre talking about the difference between ‚Äúirritatingly slow‚Äù and ‚Äú150ms worse than irritatingly slow‚Äù.\n\nIf you are building a general purpose CDN, this is a nice way to do it. You can create a handful of super-regions that aggregate cache data for part of the world.\n\nIf you‚Äôre not building a general purpose CDN, and are instead just trying to speed up your application, this is a brittle solution. You are probably better off distributing portions of your application to multiple regions.\n\n\n## Where are we now?\n\nThe basic ideas of a CDN are old, and easy to understand. But building out a CDN has historically been an ambitious team enterprise, not a weekend project for a single developer.\n\nBut the building blocks for a capable CDN have been in tools like NGINX for a long time. If you‚Äôve been playing along at home with the Github repo, we hope you‚Äôve noticed that even the most complicated iteration of the design we‚Äôre talking about, a design that has per-region redundancy and that allows for rudimentary control of request routing between regions, is mostly just NGINX configuration ‚Äî and not an especially complicated configuration. The ‚Äúcode‚Äù we‚Äôve added is just bash sufficient to plug in addresses.\n\nSo that‚Äôs a CDN. It‚Äôll work just great for simple caching. For complicated apps, it‚Äôs only missing a few things.\n\nNotably, we didn‚Äôt address cache expiration at all. One ironclad rule of using a CDN is: you will absolutely put an embarrassing typo on a launch release, notice it too late, and discover that all your cache servers have a copy titled ‚ÄúA Better Amercia‚Äù. Distributed cache invalidation is a big, hairy problem for a CDN. Someone could write a whole article about it.\n\nThe CDN layer is also an exceptionally good place to add app features. Image optimization, WAF, API rate limiting, bot detection, we could go on. Someone could turn these into ten more articles.\n\nOne last thing. Like we mentioned earlier: this whole article is bias. We‚Äôre highlighting this CDN design because we built a platform that makes it very easy to express (you should play with it). Those same platform features that make it trivial to build a CDN on Fly also make it easy to distribute your whole application; an application designed for edge distribution may not need a CDN at all.\n\nWould you like to know more? Ask us anything."
  },
  {
    "title": "Jerome published our init sourceAlso buy our cereal!",
    "url": "https://fly.io/blog/ssh-and-user-mode-ip-wireguard/",
    "content": "Here‚Äôs a thing you‚Äôd probably want to do with an application hosted on a provider like Fly.io: pop a shell on it.\n\nBut Fly is kind of an odd duck. We run hardware in data centers around the world, connected to the Internet via Anycast and to each other with a WireGuard mesh. We take Docker-type containers from users and transmogrify them into Firecracker micro-VMs. And, when we first got started, we did all this stuff so that our customers could run ‚Äúedge applications‚Äù; generally, relatively small, self-contained bits of code that are especially sensitive to network performance, and that need to be run close to users. In that environment, being able to SSH into an app is not that important.\n\nBut that‚Äôs not all people use Fly for now. Today, you can easily run your whole app on Fly. We‚Äôve made it easy to run ensembles of services, in cluster configurations, that can talk to each other privately, store data persistently, and talk to their operators over WireGuard. And, if I keep on writing like this, I can probably tag every blog post we‚Äôve written in the last couple months.\n\nAnyways, we didn‚Äôt have an SSH feature.\n\nNow, of course, you could just build a container that ran an SSH service, and then SSH into it. Fly supports raw TCP (and UDP) networking; if you told our Anycast network about your weird SSH port in your fly.toml, we‚Äôd route SSH connections to you, and that‚Äôd work just fine.\n\nBut that‚Äôs not how people want to build containers, and we‚Äôre not asking them to. So, we built an SSH feature. It is wacky. I am here to describe it, in two parts.\n\n\n## Part The First: 6PN and Hallpass\n\nI‚Äôve written a bunch about private networking at Fly. Long story short: it‚Äôs like a simpler, IPv6 version of GCP or AWS ‚ÄúVirtual Private Clouds‚Äù; we call it ‚Äú6PN‚Äù. When an app instance (a Firecracker micro-VM) is started at Fly, we assign it a special IPv6 prefix; the prefix encodes the app‚Äôs ID, the ID of its organization, and an identifier for the Fly hardware it‚Äôs running on. We use a tiny bit of eBPF code to statically route those IPv6 packets along our internal WireGuard mesh, and to make sure that customers can‚Äôt hop into different organizations.\n\nFurther, you can bridge the private IPv6 networks we create with other networks, using WireGuard. Our API will mint new WireGuard configurations, and you can stick them on an EC2 host to proxy RDS Postgres. Or, if you like, use the Windows, Linux, or macOS WireGuard client to connect your development machine to your private network.\n\nYou can probably see where this is going.\n\nWe wrote a teeny, tiny, trivial SSH server in Go, called Hallpass. It‚Äôs practically the ‚Äúhello world‚Äù of Go‚Äôs x/crypto/ssh library. (If I was doing it over again, I‚Äôd probably just use the Gliderlabs SSH server library, where it would literally be ‚Äúhello world‚Äù). Our Firecracker ‚Äúinit‚Äù starts Hallpass on all instances, bound to the instance‚Äôs 6PN address.\n\nNormally, this big balloon thingy would be an elaborate scheme to get you to check out our product, but here it‚Äôs just pointing out some new source code we haven‚Äôt talked about elsewhere.\n\nIf you can talk to your organization‚Äôs 6PN network (say, over a WireGuard connection), you can log into your instances with Hallpass.\n\nThere‚Äôs only one interesting thing about how Hallpass works, and that‚Äôs authentication. The infrastructure in our production network doesn‚Äôt generally have direct access to our API or the databases that back it, nor, of course, do the instances themselves. This makes communicating configuration changes ‚Äî like ‚Äúwhat keys are allowed to log into instances‚Äù ‚Äî a bit of a project.\n\nWe sidestepped that work by using SSH client certificates. Rather than propagating keys every time a user wants to log in from a new host, we establish a one-time root certificate for their organization; the public key for that root certificate is hosted in our private DNS, and Hallpass consults the DNS to resolve the certificate every time it gets a login attempt. Our API signs new certificates for users they can use to log in.\n\nYou probably have questions.\n\nFirst: certificates. Decades of X.509 madness have probably left a bad taste in your mouth for ‚Äúcertificates‚Äù. I don‚Äôt blame you. But you should be using certificates for SSH, because they are great. SSH certificates aren‚Äôt X.509; they‚Äôre OpenSSH‚Äôs own format, and there isn‚Äôt much to them. Like all certificates, they have expiration dates, so you can create short-lived keys (which is almost always what you want). And, of course, they allow you to provision a single public key, on a whole bunch of servers, that can authorize an arbitrary number of private keys, without repeatedly updating those servers.\n\nNext: our API, and signing certificates. Welp! We‚Äôre pretty careful, but it‚Äôs basically as secure as your Fly access token is; it couldn‚Äôt, right now, be any more secure than that, because your access token allows you to deploy new versions of your app container. There‚Äôs a lot of ceremony involved with WebPKI X.509 CA signing; this isn‚Äôt that, and we‚Äôre pretty unceremonious.\n\nFinally, the DNS. This, I concede, seems batshit. But it is less batshit than it seems! Every host we run instances on runs a local version of our private DNS server (a small Rust program). eBPF code ensures that you can only talk to that DNS server (technically: you can only make queries of the private DNS API of the server; it‚Äôll recurse for anybody) from the 6PN address of your server. The DNS server can ‚Äî I know this is weird ‚Äî reliably discern your organization identity from source IP addresses. So that‚Äôs what we do.\n\nAll this stuff is happening behind the scenes. What you, as a user, saw was the command flyctl ssh issue -a, which would grab a new certificate from our API and insert it into your local SSH agent, at which point SSH would just sort of work. It‚Äôs been neat.\n\nBut: things can always be neater.\n\n\n## Part The Second: User-Mode TCP/IP WireGuard\n\nHere‚Äôs a problem: not everyone has WireGuard set up. They should; WireGuard is great, and it‚Äôs super useful for managing applications running on Fly. But, whatever, some people don‚Äôt.\n\nThey‚Äôd still like to SSH into their apps.\n\nAt first glance, not having WireGuard installed seems like a dealbreaker. The way WireGuard works is, you get a new network interface on your machine, either a kernel-mode WireGuard interface (on Linux) or a tunnel device with a userland WireGuard service attached (everywhere else). Without that network interface, you can‚Äôt talk over WireGuard.\n\nBut if you squint at WireGuard and tilt your head the right way, you can see that‚Äôs not technically true. You need operating system privileges to configure a new network interface. But you don‚Äôt need any privileges to send packets to 51820/udp. You can run the whole WireGuard protocol as an unprivileged userland process ‚Äì that‚Äôs how wireguard-go works.\n\nThat gets you as far as handshaking WireGuard. But you‚Äôre still not talking over a WireGuard network, because you can‚Äôt just send random strings to the other end of a WireGuard connection; your peer expects TCP/IP packets. The native socket interface on your machine won‚Äôt help you establish a TCP connection over a random connected UDP socket.\n\nHow hard could it be to put together a tiny user-mode TCP, just for the purposes of doing pure-userland WireGuard networking, so people could SSH into instances on Fly without installing WireGuard?\n\nI made the mistake of musing about this on a Slack channel I share with Jason Donenfeld. I mused about it just before I went to bed. I woke up. Jason had implemented it, using gVisor, and made it part of the WireGuard library.\n\nThe trick here is gVisor. We‚Äôve written about it before. For those who aren‚Äôt familiar, gVisor is Linux, running in userland, reimplemented in Golang, as a container runc replacement. It is a pants-are-shirts bananas crazy project and, if you run it, I think you should brag about it, because wow is it a lot. And, buried in its guts is a complete TCP/IP implementation, written in Go, whose inputs and outputs are just []byte buffers.\n\nSome tweets were twote, and, a couple hours later, I got a friendly email from Ben Burkert. Ben had done some gVisor networking work elsewhere, was interested in what we were working on, and wanted to know if we wanted to collaborate. Sounded good to us! And, long story short, we now have an implementation of certificate-based SSH, running over gVisor user-mode TCP/IP, running over userland wireguard-go, built into flyctl.\n\nTo use it, you just use flyctl to ssh:\n\nTo give you some perspective on how bananas this is: dogmatic-potato-342.internal is an internal DNS name, resolving only over private DNS on 6PN networks. It works here because, in ssh shell mode, flyctl is using gVisor‚Äôs user-mode TCP/IP stack. But gVisor isn‚Äôt providing the DNS lookup code! That‚Äôs just the Go standard library, which has been hornswoggled into using our bunko TCP/IP interface.\n\nFlyctl, by the way, is open source (it has to be; people have to run it on their dev machines) so you can just go read the code. The good code, under pkg, Ben wrote. The nightmare code, elsewhere, is me. User-mode IP WireGuard is astonishingly straightforward in Go. Like, if you‚Äôve done low-level TCP/IP work before: you probably won‚Äôt believe how simple it is: the objects gVisor‚Äôs TCP stack code hand back plug directly into the standard library‚Äôs network code.\n\nHere, let‚Äôs take a look:\n\nCreateNetTUN is part of wireguard-go; it packages up gVisor and gives us (1) a sythetic tun device we can use to read and write raw packets to drive WireGuard, and (2) a net.Dialer that wraps gVisor that we can drop into Go code to use that WireGuard network.\n\nAnd that‚Äôs‚Ä¶ it? I mean, here‚Äôs us using it for DNS:\n\nIt‚Äôs just normal Go networking code. Bananas.\n\n\n## So Obviously Everybody Should Be Doing This\n\nFor a couple hundred lines of code (not counting the entire user-mode Linux you‚Äôll be pulling in from gVisor, HEY! Dependencies! What are you gonna do!) you can bring up a new, cryptographically authenticated network, any time you want to, in practically any program.\n\nGranted, it‚Äôs substantially slower than kernel TCP/IP. But how often does that really matter? And, in particular, how often does it matter for the random tasks that you‚Äôd ordinarily build a weird, clanking TLS tunnel for? When it starts to matter, you can just swap in real WireGuard.\n\nAt any rate, it solves a huge problem for us. It‚Äôs not just SSH; we also host Postgres databases, and it‚Äôll be super handy to be able to bring up a psql shell anywhere, regardless of whether you can install macOS WireGuard right that second, with a simple command.\n\nAlso Ben Burkert is fantastic and you should all bid his consulting rates up, because he got this project done in what seemed like a few hours.\n\nOK, we‚Äôre still going to try to get you to check out our product, because it is really neat, and you can SSH into things as if they were servers from 2002.\n\nI would write more gracefully about him, and about Jason, and about gVisor, and about this whole project, except that Tailscale tweeted this morning about a gVisor TCP/IP feature they‚Äôre going to roll out, and I won‚Äôt be beaten to the punch. Not this time, Tailscale! Not this time!"
  },
  {
    "title": "Store data like on a real computer from 1992",
    "url": "https://fly.io/blog/persistent-storage-and-fast-remote-builds/",
    "content": "If you‚Äôve been keeping up with us at Fly, you may be picking up on a bit of a narrative with us.\n\nFly launched, in the long-long-ago, with a somewhat narrow use case. We took containers from our customers and transmogrified them into fleets of Firecracker micro-VMs connected to an anycast network that kept code running close to users. When we were talking to investors, we called this ‚Äúedge computing‚Äù, and sold it as a way to speed up websites. And that works great.\n\nBut it turns out though if you build a flexible platform for edge apps, you wind up with a pretty good way to run lots of other kinds of applications. And so our users have been doing that. And we‚Äôre not going to complain! Instead, we‚Äôre working to make it easier to do that.\n\n\n## The Storage Problem\n\nThe biggest question mark for people thinking about hosting whole apps on Fly has always been storage.\n\nUntil somewhat recently, micro-VMs on Fly were entirely ephemeral. A worker gets an order to run one. It builds a root filesystem for it from the container image. It runs the micro-VM, maybe for a few hours, maybe for a few weeks. The VM exits, the worker cleans it up, and that‚Äôs it.\n\nYou can get a surprising amount done with a configuration like this and little else, but if you want to run an entire app, front-to-backend, you need more.\n\nUntil now, the one really good answer we had was to use external services for storage. You can, for instance, keep your data in something like RDS, and then use a fast, secure WireGuard gateway to bridge your Fly app instances to RDS. Or you can set up S3 buckets in a bunch of regions. That works fine and for some apps might be optimal.\n\nBut, obviously, developers want persistent storage. And we‚Äôre starting to roll that out.\n\n\n## Fly Volumes\n\nYou can, on Fly today, attach persistent storage volumes to Fly apps. It‚Äôs straightforward: you run flyctl volumes create data --region ewr --size 25 to create a 25 gig volume named ‚Äúdata‚Äù in Newark. Then you tell your app about it in fly.toml:\n\nOnce connected to a volume, we‚Äôll only run as many instances of your app as you have matching volumes. You can create lots of volumes, all named ‚Äúdata‚Äù, in different regions. We modeled this UX after Docker and tried to make it predictable for people who work with containers.\n\n\n## What‚Äôs Happening With Volumes\n\nI‚Äôm going to share what‚Äôs happening under the hood when you use volumes, and then rattle off some of the limitations, because they are important.\n\nFly runs on dedicated hardware in data centers around the world. On volume-eligible servers, that hardware includes large amounts of NVME storage. We use Linux LVM to carve out a thin pool. We track the amount of space in all those pools.\n\nLVM2 thin pools are interesting. Rather that preallocating disk space for new logical volumes, thin volumes allocate as data is written. They have in effect a current size and a cap. And the caps of all the volumes in a single thin pool can overlap; you can create more thin volume space than there is disk, like a bank creates money. This was a big deal in enterprise disk storage in the 2000s, is built into LVM now, and we‚Ä¶ don‚Äôt use it for much. Our disks aren‚Äôt oversubscribed, but thin allocation makes administration and snapshot backups a little easier for us, and I just thought you‚Äôd like to know.\n\nWe encrypt volumes for users, using standard Linux block device crypto (XTS with random keys). This doesn‚Äôt mean much to you, but we do it anyways, because somebody somewhere is going to talk to a SOC 2 auditor who is going to want to know that every disk in the shop is ‚Äúencrypted‚Äù, and, OK, these ones are too.\n\nThe problem of course is that for us to manage your encrypted devices, we have to have the keys. Which means that anybody who manages to own up our devices also has those keys, and, as you‚Äôd hope, people who can‚Äôt own up our devices already don‚Äôt have access to your disk. About the only problem disk encryption solves for us is somebody forklifting our servers out of their secure data centers in some weird hardware heist.\n\nThe problem is mitigated somewhat by our orchestration system. The control plane for Fly.io is Hashicorp Nomad, about which we will be writing more in the future. Nomad is in charge of knowing which piece of our hardware is running which applications. Because we have a fairly intense TLS certificate feature, we also have a deployment of Hashicorp Vault, which is basically the the de facto standard on-prem secret storage system. Nomad knows how to talk to Vault, and we store drive secrets there; when a micro-VM is spun up on a piece of hardware, it gets a lease for the app‚Äôs associated secrets, and hardware not running that app doesn‚Äôt.\n\nThis is all sort of built in to Nomad and Vault (the secret storage and leasing, that is; the disk management is all us) and it‚Äôs neat, and might matter a little bit in the future when we start doing volume migrations between servers, but really: serverside full disk encryption is pretty much rubber chicken security, and I‚Äôm just taking this opportunity to get that take out there.\n\nAnyways.\n\nWhen you create a volume with flyctl, you talk to our API server, which finds a compatible server with sufficient space. Once a match is found, we decrement the available space on the server and push a Consul update recording the existence of the new volume.\n\nWorkers listen for Consul updates for volume changes and create/remove LVM thin volumes, with ext4 filesystems, as needed.\n\nWhen an instance of a volume-attached app is scheduled to deploy, our orchestrator treats the volume as a constraint when finding eligible servers. Deployments are routed to (and limited to) hosts with attachable volumes. Before booting up the micro-VM for those apps, we look up the logical volume we made, recreate its block device node in the jail Firecracker runs inside of, and set up mount points.\n\nAnd that‚Äôs pretty much all there is to it. You get a persistent filesystem to play with, which survives multiple reboots and deployments of your app. It‚Äôs performance-competitive (usually, a little faster) with EBS.\n\n\n## There Are Implications To This\n\nThe storage nerds in our audience are raising their hands in the air waiting to point this out: we haven‚Äôt talked about data resilience. And that‚Äôs because, right now, there isn‚Äôt much to talk about.\n\nRight now, for raw Fly volumes, resilience is your problem. There! I said it!\n\nThis is not a storage model that‚Äôs appropriate to every application, and we want to be super clear about that. We think it makes sense in two major cases.\n\nThe first is cluster storage. In the past few months, we‚Äôve made it easy to boot up clusters of things that talk to each other, and to create ensembles of services that work together. So one way to use theoretically-unreliable storage is with a replicating database cluster, where you‚Äôre effectively backing up the data in real time. You do resilience at the app layer.\n\nThe second, of course, is for the kinds of data where loss is an inconvenience and not a disaster; caches, metrics, and things you can back up on a slower-than-real-time cadence. This is a big class of applications! We really wanted disks for CDN workloads. Caches can go away, but minimizing cache churn and keeping ‚Äúwarm‚Äù caches around between deploys is handy. And big caches are great! Not having cache sizes bound by memory is great! If you‚Äôre building a CDN with Fly, go nuts with volumes.\n\nAttaching volumes to apps changes the way they scale. Once you attach a volume, you can only have as many instances of that app as you have volumes. Again, a lot of the time, volume storage will make sense for a cluster of storage/database servers you run alongside the rest of your app.\n\nAnother subtle change is deployment. By default, Fly uses ‚Äúcanary‚Äù deployments: we spin up a new VM, make sure it passes health checks, and then tear down the old one. But volumes are provisioned 1:1 with instances; if you have 3 volumes for an app, we can‚Äôt make a 4th volume appear for the canary deploy to work. So apps with volumes do rolling deploys.\n\nAn aside: we don‚Äôt currently expose snapshots or volume migration in our API. But if you‚Äôre a storage nerd: we can generally do the things you‚Äôd expect to be able to do with logical volumes, like shipping LVM snapshots, and you should totally reach out if you need something like that or want to know how it would work.\n\nThis is a starting point for us, not the final destination. We‚Äôve got more storage stuff coming. This is going to be a whole thing with us in 2021.\n\nYou‚Äôre less than 10 minutes away from having any container you can build running globally, with attached persistent storage.\n\n\n## In Other News Remote Docker Builds Got Way Faster\n\nHey, one of the things you can do now that we have volumes available is drastically improve build times.\n\nThere‚Äôs two ways flyctl will get a Dockerfile deployed on Fly. The first is easy: we‚Äôll talk to your local Docker instance, have it build the image, and then push that image to our registry. Until recently, this was the ‚Äúgood‚Äù way to do it, because it‚Äôs fast.\n\nThe second way it can work is that flyctl can do a remote build. This works by talking to a remote Docker server instead of one running locally ‚Äî which, incidentally, is how Docker works already if you‚Äôre on macOS and it‚Äôs running in a VM.\n\nFor the last couple years, to do remote builds, we used AWS CodeBuild. CodeBuild is great and all, but if you think about what Fly is, it‚Äôs a little weird that we‚Äôre using it (our internal CI/CD system runs on our hardware). And our user experience with CodeBuild has been‚Ä¶ not the best. Until now, remote build has been the ‚Äúbad‚Äù way to do it; it‚Äôs slow enough that, on our own projects, when we saw ‚Äúremote builds‚Äù kicking in, we stopped the deploy, started up our local Docker instance, and started over.\n\nNo more! If you‚Äôre running recent flyctl, you may have noticed remote builds got a lot faster.\n\nThat‚Äôs because when you run a remote build now, we first match the build to a ‚Äúbuilder‚Äù app running in an instance in our network. The builder has attached storage to cache layers and manifests and exposes a Docker server, to which flyctl authenticates and requests a Docker build, which then gets pushed to our repository.\n\nOnce you do a remote build, you‚Äôll see them in your app list. Say hi! We‚Äôre not billing you for them.\n\nYou can make a remote build happen if you deploy without a local Docker running, call deploy --remote-only, or are running on ARM.\n\nGetting this working was interesting, because it adds a new kind of instance to the mix. You don‚Äôt want a builder hanging around doing nothing when you‚Äôre not deploying. Our builder instances terminate after 10 minutes of inactivity, and are created/deployed automatically by flyctl (we added a GQL call to do this) as needed. Up until now, if your instance died, our orchestration‚Äôs job was to immediately restart it; builder jobs are ‚Äúephemeral‚Äù and aren‚Äôt restarted if they exit successfully.\n\nThis is something we get a lot of requests for. People want permanent storage and short lived VMs that boot on demand and exit when their job is done. We‚Äôve got that now and are figuring out the best way to surface it. If you‚Äôre interested, let us know.\n\nDo you have storage feature requests or questions about the new remote builders? We have a community discussion just for you."
  },
  {
    "title": "Come for the Rust esoterica, stay for the hosting",
    "url": "https://fly.io/blog/the-tokio-1-x-upgrade/",
    "content": "At Fly.io, we run a Rust-based load-balancer which handles almost all of our traffic. It stands on the shoulders of Tokio and Hyper. When the Tokio team announced 0.3 and then 1.0, we figured we‚Äôd have to upgrade sooner than later to access related crate upgrades. The Rust ecosystem moves pretty fast and we wanted to be able to keep our software up to date.\n\nAfter a few weeks of patience, most of the ecosystem had already been upgraded. I believe this was a well-coordinated move from the Tokio team, making sure a good chunk of the ecosystem was going to be available when (or soon after) they released 1.0. We use hyper, reqwest, async-compression, tokio-io-timeout, tokio-tungstenite and sentry in addition to tokio, so we waited until they were ready.\n\n\n## Differences with tokio 0.2\n\nHere are some notes about the Tokio upgrade. They‚Äôre by no means exhaustive.\n\n\n## No implementations of Stream\n\nWith Tokio 1.0, the team has decided to forego all Stream implementations until its stabilization in the stdlib.\n\nIf you still want Stream‚Äòs, then you probably should try tokio-stream. It implements Stream for TcpListener, Interval and many more.\n\nIf not, most of the time, you can get around this change by looping:\n\n\n## Mutable access requirements relaxed\n\nIn the previous code snippet, you might‚Äôve noticed I don‚Äôt need a mut-able TcpListener. You can now use more of the tokio API without mutable access! That‚Äôs a welcome change: it reduces much of the locking required in our proxy.\n\n\n## New AsyncRead and AsyncWrite traits\n\nNotably, AsyncRead::poll_read and AsyncWrite::poll_write don‚Äôt return the number of bytes read/written anymore, just Result\u003c()\u003e. A quick work around is this:\n\n\n## More pinning required\n\nFutures likes Sleep and Shutdown are now explicitly !Unpin. If you need to use them multiple times in a tokio::select! (like we do, all the time), then you‚Äôll need to either pin them on the heap with Box::pin or pin them on the stack with tokio::pin!. The documentation explains it in more detail.\n\n\n## runtime::Handle doesn‚Äôt have block_on (yet)\n\nThere are discussions around adding back Handle::block_on, but for now, it‚Äôs left unimplemented.\n\nWe needed it for the following pattern:\n\nWe ended up getting around that restriction by using Arc\u003cRuntime\u003e since block_on doesn‚Äôt require mutable access to Runtime anymore.\n\n\n## TcpListener::from_std needs to be set to nonblocking\n\n‚Ä¶ or else you‚Äôll be surprised that everything hangs when you start accepting connections.\n\nFor example, if you use socket2 to fashion a TcpListener with a few more options, you‚Äôll need to use tokio::net::TcpListener::from_std(std_listener). Before doing that, you‚Äôll want to set_nonblocking(true) on your std listener.\n\nIt‚Äôll take less than 10 minutes to get almost any container you‚Äôve got running globally on our Rust-powered anycast proxy network.\n\n\n## Miscellaneous API changes\n\n\n## Hyper WebSockets\n\nWebSockets upgrades are now on the whole request/response instance, not just Body ( body.on_upgrade() vs hyper::upgrade::on(req)). Here‚Äôs an example.\n\nHyper now relies on OnUpgrade being present on the Response‚Äôs or Request‚Äôs Extensions. We were previously replacing extensions with our own. We had to make sure we copied OnUpgrade into our new Extensions, if present, before overwriting.\n\n\n## Kudos to the Tokio team\n\nThis was a major version upgrade, but it wasn‚Äôt hard at all. Especially with the sporadic question/answer sessions on the very helpful Tokio Discord (hat tip to Alice Ryhl in particular)."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/practical-smokescreen-sanitizing-your-outbound-web-requests/",
    "content": "This is a post about the most dangerous vulnerability most web applications face, one step that we took at Fly to mitigate it, and how you can do the same.\n\nServer-side request forgery (SSRF) is application security jargon for ‚Äúattackers can get your app server to make HTTP requests on their behalf‚Äù. Compared to other high severity vulnerabilities like SQL injection, which allows attackers to take over your database, or filesystem access or remote code injection, SSRF doesn‚Äôt sound that scary. But it is, and you should be nervous about it.\n\nThe deceptive severity of SSRF is one of two factors that makes SSRF so insidious. The reason is simple: your app server is behind a security perimeter, and can usually reach things an ordinary Internet user can‚Äôt. Because HTTP is a relatively flexible protocol, and URLs are so expressive, attackers can often use SSRF to reach surprising places; in fact, leveraging HTTP SSRF to reach non-SSRF protocols has become a sport among security researchers. A meaty, complicated example of this is Joshua Maddux‚Äôs TLS SSRF trick from last August. Long story short: in serious applications, SSRF is usually a game-over vulnerability, meaning attackers can use it to gain full control over an application‚Äôs hosting environment.\n\nThe other factor that makes SSRF nerve-wracking is its prevalence. As an industry, we‚Äôve managed to drastically reduce instances of vulnerabilities like SQL injection by updating our libraries and changing best practices; for instance, it would be weird to see a mainstream SQL library that didn‚Äôt use parameterized queries to keep attacker meta-characters out of query parsing. But applications of all shapes and sizes make server-side HTTP queries; in fact, if anything, that‚Äôs becoming more common as we adopt more and more web APIs.\n\nThere are two common patterns of SSRF vulnerabilities. The first, simplest, and most dangerous comprises features that allow users to provide URLs for the web server to call directly; for instance, your app might offer ‚Äúweb hooks‚Äù to call back to customer web servers. The second pattern involves features that incorporate user data into URLs. In both cases, an attacker will try to leverage whatever control you offer over URLs to trick your server into hitting unexpected URLs.\n\nFortunately, there‚Äôs a mitigation that frustrates attackers trying to exploit either pattern of SSRF vulnerabilities: SSRF proxies.\n\n\n## You should know about Smokescreen\n\nImagine if your application code didn‚Äôt have to be relentlessly vigilant about every URL it reached out to, and could instead assume that a basic security control existed to make sure that no server-side HTTP query would be able to touch internal resources? It‚Äôs easy if you try! What you want is to run your server-side HTTP through a proxy.\n\nWe‚Äôve been putting Smokescreen to work at Fly, and it‚Äôs so useful, we thought we should share. Smokescreen is an egress proxy that was built at Stripe to help manage outgoing connections in a sensible and safe way.\n\nSmokescreen‚Äôs job is to make sure your outgoing requests are sane, sensible and safe. SmokeScreen was created by Stripe to ensure that they knew where all their outgoing requests were going. Specifically, it makes sure that the IP address requested is a publicly routed IP and that means checking any request isn‚Äôt destined for 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, or fc00::/7 (the IPv6 ‚ÄúULA‚Äù space, which includes Fly‚Äôs 6PN private addresses.\n\n\n## Out of the box\n\nThere‚Äôs more SmokeScreen can do, but before we get to that, let‚Äôs talk about how Smokescreen determines who you are. By default, Smokescreen uses the client certificate from a TLS connection, extracts the common name of the certificate and uses that as the role. There is another mechanism documented for non-TLS connections using a header but doesn‚Äôt seem to be actually wired up Smokescreen (probably because it‚Äôs way too simple to present to be another system). So you‚Äôll have to use TLS CA certs for all the systems connecting through Smokescreen and that is an administrative pain.\n\n\n## Getting Basic\n\nWe wanted Smokescreen to be simpler to enable, and with Fly we have the advantage of supporting Secrets for all applications. Rather than repurposing TLS CAs to provide a name, we can store a secret with the Smokescreen proxy and with the app that sends requests to the outside world. That secret? For the example, we‚Äôve gone with a PROXY_PASSWORD that we can distribute to all inside the Fly network.\n\nHere‚Äôs the Fly Github repository for the Fly Smokescreen.\n\n\n## I‚Äôm on the list‚Ä¶\n\nIn all cases, what Smokescreen does is turn the identity of an incoming request into a role. That role is then looked up in the acl.yaml file. Here‚Äôs the Fly example ACL:\n\nWe‚Äôve gone super simple on the roles here. There‚Äôs one and that‚Äôs authed. You‚Äôre either authed or you fall through to default. The project field is there to make logging more meaningful by associating roles with projects.\n\nThe control of what happens with requests comes from the action field; this has three settings: open lets all traffic through, report lets all traffic through but logs the request if it‚Äôs not on the list, and enforce only lets through traffic on the list. The list in this example isn‚Äôt there, so report logs all requests and enforce blocks all requests.\n\nAdding allowed-domains and a list of domains lets you fine tune these options. For a general purpose block-or-log egress proxy, this example is enough. Smokescreen has more ACL control options, including global allow and deny lists if you want to maintain simple but specfic rules but want to block a long list of sites.\n\n\n## Smokescreen inside\n\nIf you are interested in how this modified Smokescreen works, look in the main.go file. This is where the smokescreen code is loaded as a Go package. The program creates a new configuration for Smokescreen with a alternative RoleFromRequest function. It‚Äôs this function that extracts the Proxy-Authorization password and checks it against the PROXY_PASSWORD environment variable. If it passes that test, it returns authed as a role. Otherwise, it returns an empty string, denoting no role. It‚Äôs this function that you may want to customize to create your own mappings from username and password combinations to Smokescreen roles.\n\n\n## Deploy now\n\n\n## Fly\n\nThis is where we show how to deploy on Fly first:\n\nAnd that‚Äôs it for Fly; there‚Äôll be a mysmokescreen app set up with Fly‚Äôs internal private networking DNS (and external DNS if we needed that, which we don‚Äôt here), and it‚Äôll be up and running. Turn on your Fly 6PN (Private Networking) VPN and test it with:\n\nAnd that will return the Fly homepage to you. Run fly logs and you‚Äôll see entries for the opening and closing of the proxy‚Äôs connection to fly.io. What‚Äôs neat with the Fly deployment is that with just two commands you can deploy the same application globally.\n\n\n## Docker - locally\n\nIf you‚Äôre on another platform, you should be able to reuse the Dockerfile. Running locally, you just need to do:\n\nAnd to test, in another session, do:\n\nYou‚Äôll see the log output appearing in the session where you did the docker run. We leave it as an exercise to readers to deploy the application to their own Cloud.\n\n\n## Using a Proxy from an app\n\nTo wrap up this article, we present two code examples, one in Go and one in Node, that take from the environment a PROXYURL pointed at our smokescreen and a PROXYPASSWORD for that smokescreen and issue a simple GET for an https: URL.\n\nOn Fly, the PROXY_URL can be as simple as ‚Äúhttp://mysmokescreen.internal:4750/‚Äù. Fly‚Äôs 6PN network automatically maps deployed applications‚Äô names and instances into the the .internal TLD for DNS. On other platforms, you‚Äôll have to configure a hostname for your smokescreen and make sure you change it everywhere if you move your proxy.\n\n\n## Calling through an authenticated Proxy from Go\n\nThis example uses only the system libraries. There are no extra modules needed.\n\n\n## Calling through an authenticated Proxy from Node.js\n\nThis example uses the https-proxy-agent package.\n\n\n## Smokescreen summarized\n\nWe‚Äôve shown you examples of setting up a custom Smokescreen with password authentication. You‚Äôll find all the code for setting that up at the Fly Github repository for this Smokescreen. Have fun sanitizing your outgoing web requests."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/the-january-2021-fly-changelog/",
    "content": "Private networking between Fly Apps, WireGuard VPNs, updates to scaling, and backend improvements. This is what‚Äôs new at Fly in this month‚Äôs ChangeLog.\n\nIf you missed 2020, lucky you, to get you all caught up, here‚Äôs our Previously On Fly segment. Caught up? Good.\n\nAnd now here is the Fly ChangeLog, covering the end of December 2020 to January 2021.\n\n\n## 6PN Networking and IPv6 WireGuard VPNs\n\nLet your apps talk amongst themselves with 6PN networking. 6PN private networks build on top of Fly‚Äôs WireGuard mesh networking and make it easy to look up other apps in your organizations, apps in regions, or globally view all instances of an app.\n\nAnd if that wasn‚Äôt enough, IPv6 WireGuard Peering lets you create VPN tunnels into your organization so you can safely work from inside the Fly infrastructure when developing or use it to securely bridge outside networks and applications into Fly.\n\n\n## Count Scaling\n\nWe‚Äôve switched our default scaling model to a more predictable model, scaling by the count of instances, with autoscaling getting its own command.\n\n\n## Backend Improvements\n\nIt‚Äôs not all new commands and features. We‚Äôve been tuning up the Fly infrastructure too. We recently switched to containerd for pulling your images in, removing the entire building roofs stage, and letting us reuse more layers. The result is shorter subsequent boot times for larger images and a faster Fly for all.\n\nAnother backend change means that events in an app‚Äôs lifecycle‚Äîincluding being configured, starting up, and shutting down‚Äîare now being logged in the app‚Äôs own logs. This change means more information in logs about what‚Äôs happening with an app.\n\nAnd volumes, the persistent storage option on Fly, are now being lazily initialized and encrypted (by default) so that configuring them is faster.\n\nThis is the Fly ChangeLog, where we list all significant changes to the Fly platform, tooling, and websites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\n\n## 22nd January\n\nflyctl: Version 0.0.162 released\n\n\n## 15th January\n\nflyctl: Version 0.0.161 released\n\n\n## 14th January\n\nflyctl: Version 0.0.160 released\n\nflyctl: Version 0.0.159 released\n\n\n## 6th January\n\nflyctl: Version 0.0.158 released\n\n\n## 23rd December\n\nflyctl: Version 0.0.157 released\n\n\n## 17th December\n\nflyctl: Version 0.0.156 released\n\n\n## 15th December\n\nflyctl: Version 0.0.155 released\n\n\n## 10th December\n\nflyctl: Version 0.0.154 released\n\n\n## 3rd December\n\nflyctl: Version 0.0.153 released"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-in-2020/",
    "content": "TL:DR; Launch, Turboku, Custom Domains, Scaling, One-Click Apps, Persistent Storage, Private Networking and WireGuard VPNs.\n\nFly‚Äôs had an unprecedented year, along with everyone else. For us, it was bringing our Application Platform to the world, and much more. Here‚Äôs a run down of the 12 months‚Ä¶\n\nJanuary - Fly Global Application Platform launches and we declare the command line is king, especially for driving. Search parties are sent out to look for a snappier name.\n\nFebruary - We take Fly to Heroku apps and make them faster with Turboku. It gets Heroku apps right up to the edge of a global network, boosting TLS response times. Also that month, we roll out Cloud Native Buildpack support.\n\nMarch - Custom Domains arrive, complete with a GraphQL API for managing them. We also introduce new Scaling controls for Fly applications and the Datasette starts supporting Fly.\n\nApril - One-Click Imaginary launches on Fly, letting you click and deploy an image processing service. It‚Äôs the first of a range of ‚Äúone-click‚Äù services on Fly. Also in April, as Deno heads to version 1.0, Fly adds support for Deno.\n\nMay - Pausing and resuming Fly apps is now an option after we tuned up the Scale system.\n\nJune - Thomas writes about How CDNs generate Certificates and Flyctl starts speaking JSON for easier automation.\n\nJuly - Static Websites with small servers are now a breeze with GoStatic and Fly.io, and Thomas explains how sandboxes and workloads can be isolated and how we do it at Fly (spoiler: Firecracker VMs).\n\nAugust - We add simple ‚Äúbuiltin‚Äù builders to Flyctl for Node, Ruby, Deno, Go, and static websites.\n\nSeptember - Heroku databases are in this month, How to Turboku with them and How to just use them. And Flyctl gets easier to update.\n\nOctober - It‚Äôs a busy month:\n\nNovember - A month of examples as we show how to deploy Redis, MinIO, Gogs, and MQTT along with Node-Red and Redis with TLS. Nearly all of the examples make use of the persistent volumes of Fly.\n\nDecember - It‚Äôs time for a festive feature-filled end of the year:\n\nAnd that was 2020. 2021? Ssssh! Spoilers! Sign up to community.fly.io to be among the first to know."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-to-build-a-global-message-service-with-nats/",
    "content": "We‚Äôre looking at new example applications that show you how to make the best use of the latest Fly features with the applications you want to run.\n\nThere are actually two examples that make up this example. One example is a Websocket-based chat that relays its messages through that NATS cluster. The other part is that NATS cluster, configured as a global messaging cluster that you can install inside your Fly organization‚Äôs network.\n\nChat: A 6PN Example with NATS builds on a previous Fly example messaging application using WebSockets to talk to users, and using NATS powered messaging to run in any region.\n\nGlobal NATS Cluster shows to configure a three-node cluster of NATS servers that automatically discover and communicate with each other thanks to Fly‚Äôs 6PN networking.\n\nBoth examples use Fly‚Äôs 6PN and its DNS features like .internal addresses that make locating running instances of apps in an organization or region as simple as a DNS lookup.\n\nFind more Fly examples and demonstration applications in Flight Plans - Guides and Examples."
  },
  {
    "title": "Easily run clusters on Fly",
    "url": "https://fly.io/blog/building-clusters-with-serf/",
    "content": "Assume for a second we‚Äôd like to see what happens when a web page loads in a browser in Singapore. Easy enough; Fly.io will take a container image you throw at it, transform it into a Firecracker VM, and run it in Singapore.\n\n\n## Getting Up And Running\n\nWe want a container that loads a web page in a browser; that sounds like a job for Headless Chromium. Here‚Äôs a Dockerfile; actually, don‚Äôt bother, it‚Äôs just one of those Dockerfiles that installs the right apt-get packages and then downloads the right Chromium distribution; the entrypoint runs Chromium with the right arguments.\n\nDeploy the app on Fly:\n\nAnd this will pretty much just work. Say we named the Fly app ichabod-chrome. When flyctl deploy finishes, our image will be running as VM somewhere near Singapore, and reachable from around the world as ichabod-chrome.fly.dev. You could drive the Chrome instance running in Singapore using the Chrome debug protocol, which has implementations in a ton of languages; for instance, if we want to screenshot a page in Ruby, we could just install the ferrum gem, and then:\n\nSuper boring! Neat that it works, though! But there‚Äôs, like, an obvious problem here: Chrome Debug Protocol isn‚Äôt authenticated, so we‚Äôre just kind of hanging out on the Internet hoping nobody does something dumb with the browser proxy we‚Äôve created on this public URL.\n\nLet‚Äôs fix that. We‚Äôll run our Chrome as a 6PN application, and talk to it over WireGuard. We crack open the fly.toml that flyctl generated for us, and add:\n\nWe also yank out the whole [[services]] section, because we‚Äôre not exposing any public services to health-check anymore. And we change our entrypoint to bind to its private IPv6 address.\n\nA flyctl deploy run loads our ‚Äúnew‚Äù application, which speaks CDP only over private IPv6 addresses. But: now we can‚Äôt talk to it! We‚Äôre not on the private IPv6 network.\n\nThat‚Äôs easy to fix: install WireGuard (it runs everywhere). Then run flyctl wireguard create, which will generate a WireGuard configuration for us that we can load in our client. Hit the connect button, and we‚Äôre good to go again, this time with a cryptographically secure channel to run CDP over. On our internal DNS, which is included in the WireGuard configuration we generate, our app is now reachable at ichabod-chrome.internal.\n\n\n## Clusters and DNS\n\nLet‚Äôs say we want a bunch of Headless Chromiums, in a bunch of different locations. Maybe we want to screenshot the CNN front page from different countries, or run Lighthouse tests from around the world. I‚Äôm not here to judge your life decisions.\n\nGetting those Chromium instances up and running is mercifully boring. Let‚Äôs say we want roughly to run in Singapore, Sydney, Paris, and Chile:\n\n‚Ä¶ and that‚Äôs it; Fly will figure out how to satisfy those constraints and deploy appropriately (we‚Äôre asking now for 4 instances, and Fly will try to spread those instances around as many data centers as it can).\n\nNow, we want to drive these new instances, and do so selectively. To do that, we have to be able to find them. We can use the DNS to do that:\n\nAnd this pretty much works, and you can probably get a long ways just using DNS for instance discovery, especially if your cluster is simple.\n\nBut for me, for this app, this is kind of an annoying place to leave off. I could pick a bunch of nits, but the big one is that there isn‚Äôt a good way to automatically get updates when the DNS changes. I can get a pretty good picture of the world when an instance starts up, but I have to go through contortions to update that picture as time ticks on.\n\nWhen we were putting DNS together at Fly, we had the same thoughts. And yet we did nothing about them! We quickly concluded that if people wanted ‚Äúinteresting‚Äù service discovery, they could B.Y.O.\n\nLet‚Äôs see how that plays out with this cluster. I‚Äôm going set up HashiCorp Serf to make all the components of this cluster aware of each other.\n\n\n## Running HashiCorp Serf\n\nThey do somewhat similar things, but Serf gets less attention than its HashiCorp sibling Consul. Which is a shame, because Serf is a simpler, more approachable system that does 80% of what a lot of people use Consul for.\n\nA reasonable mental model of Consul is that it‚Äôs a distributed system that solves 3 problems:\n\nUnlike Consul, Serf handles just one of these problems, #1. In fact, Consul uses Serf under the hood to solve that problem for itself. But Consul is much more complicated to set up. Serf runs without leaders (cluster members come and go, and everybody just figures things out) and no storage requirements.\n\nSerf is easy. In a conventional configuration ‚Äî one where we run Serf as a program and not as a library embedded into our application ‚Äî every node in the cluster runs a Serf agent, which, after installing Serf, is just the serf agent command. All the configuration can get passed in command line arguments:\n\nThere‚Äôs not much to it. We give every node a unique name. Serf by default assumes we‚Äôre running on a LAN and sets timers accordingly; we switch that to WAN mode. Importantly, we bind Serf to our 6PN private address. Then we set some tags, for our convenience later when selecting members.\n\nTo help Serf find other members in the cluster and converge on the complete picture of its membership, can make some quick introductions:\n\nHere we‚Äôre just dumping the current snapshot of the cluster from DNS and using serf join to introduce those members. Now, if we have nodes Alice, Bob, and Chuck, and Alice introduces herself to Bob and Bob introduces herself to Chuck, Bob will make sure Alice knows about Chuck as well. We‚Äôll talk about how that works in a second.\n\nI wrap these two actions, running the agent and making introductions, up in a little shell script. Because I‚Äôm now running multiple thingies in my Docker image, I use overmind as my new entrypoint, which drives a Procfile. Here‚Äôs the whole Dockerfile.\n\nWhat did this get me? Well, from now on, if I‚Äôm on the private IPv6 network for my organization, I can find any node and instantly get a map of all the other nodes:\n\nI can integrate this information with a shell script, but I can also just bring it into my application code directly (here with the relatively simple serfx gem:\n\nI could easily filter this down by location (via the ‚Äúregion‚Äù) tag, role, or, as we‚Äôll see in a sec, network closeness. This interface is simpler than DNS, it‚Äôs lightning fast, and it‚Äôs always up-to-date.\n\n\n## A Quick Word About Security\n\nSerf has a security feature: you can key your Serf communications statically, so rogue nodes without the key can‚Äôt participate or read messages.\n\nIt‚Äôs fine, I guess. I‚Äôd be nervous if I was deploying Serf in an environment where I was really depending on Serf‚Äôs encryption for security. But, frankly, it doesn‚Äôt matter to us here, because we‚Äôre already running on a private network, and our external connectivity to that network takes advantage of the vastly more sophisticated cryptography in WireGuard.\n\n\n## What Serf Is Doing\n\nThe first bit of distributed systems jargon that comes up when people describe Serf is SWIM, the ‚ÄúScalable Weakly-Consistent Infection Membership‚Äù protocol. Distributed systems are full of protocols with acronymical names that are hard to get your head around, and SWIM is not one of those; I don‚Äôt think you even need a diagram to grok it.\n\nYou can imagine the simplest possible membership protocol, where you make introductions (like we did in the last section) and every member simply relays messages and tries to connect to every new host it learns about. That‚Äôs probably what you‚Äôd come up with if you ran into the membership problem unexpectedly in a project and just needed to bang something out to solve it, and it works fine to a point.\n\nSWIM is just a couple heuristic steps forward from that naive protocol, and those steps make the protocol (1) scale better, so you can handle many thousands of nodes, and (2) quickly detect failed nodes.\n\nFirst, instead of spamming every host we learn about with heartbeats on an interval, we instead select a random subset of them. We essentially just ping each host in that subset; if we get an ACK, we‚Äôve confirmed they‚Äôre still members (and, when new nodes connect up to us, we can share our total picture of the world with them to quickly bring them up to date). If we don‚Äôt get an ACK, we know something‚Äôs hinky.\n\nNow, to keep the group membership picture from flapping every time a ping fails anywhere in the network, we add one more transaction to the protocol: we mark the node we couldn‚Äôt ping as SUS, we pick another random subset of nodes, and we ask them to ping the SUS node for us. If they succeed, they tell us, and the node is no longer SUS. If nobody can ping the node, we finally conclude that the node is the impostor, and eject them from the ship.\n\nSerf‚Äôs SWIM implementation has some CS grace notes, but you could bang the basic protocol out in an hour or two if you had to.\n\nSerf isn‚Äôt just a SWIM implementation, and SWIM isn‚Äôt the most interesting part of it. That honor would have to go to the network mapping algorithm Vivaldi. Vivaldi, which was authored by a collection of my MIT CSAIL heroes including Russ Cox, Frans Kaashoek, and (yes, that) Robert Morris, computes an all-points pairwise network distance map for a cluster. Here‚Äôs a funny thread where Russ Cox finds out, years later, that HashiCorp implemented his paper for Serf.\n\nHere‚Äôs roughly how Vivaldi works:\n\nWe model the members of our cluster as existing in some space. To get your head around it, think of them as having Cartesian 3D coordinates. These coordinates are abstract; they have no relation to real 3D space.\n\nTo assign nodes coordinates in this space, we attach them to each other with springs of varying (and, to start with, indeterminate) lengths. Our job will be to learn those lengths, which we‚Äôll do by sampling network latency measurements.\n\nTo begin with, we‚Äôll take our collection of spring-connected nodes and squish them down to the origin. The nodes are, to begin with, all sitting on top of each other.\n\nThen, as we collect measurements from other nodes, we‚Äôll measure error, comparing our distance in the model to the distance reflected by the measurement. We‚Äôll push ourselves away from the nodes we‚Äôre measuring in some random direction (by generating a random unit vector), scaled by the error and a sensitivity factor. That sensitivity factor will itself change based on the history of our error measurements, so that we update the model more or less confidently based on the quality of our measurements.\n\nOur cluster converges on a set of network coordinates for all the nodes that, we hope, relatively accurately represents the true network distance between the nodes.\n\nThis all sounds complicated, and I guess it is, but it‚Äôs complicated in the same sense that TCP congestion control (which was originally also based on a physical model) is complicated, not in the sense that, say, Paxos is: the complexity is mostly not exposed to us and isn‚Äôt costing meaningful performance. Serf sneaks Vivaldi data into its member updates, so we get them practically for free.\n\nWe can now ask Serf to give us the RTT‚Äôs between any two points on the network:\n\nIf you‚Äôre like me, you read Serf‚Äôs description of their Vivaldi implementation and have a record scratch moment when they say they‚Äôre using an 8-dimensional coordinate system. What do those coordinates possibly represent? But you can sort of intuitively get your head around it this way:\n\nImagine that network performance was entirely dictated by physical distance, so that by sampling RTTs and updating a model what we were effectively doing was recapitulating the physical map of where nodes where. Then, a 2D or 3D coordinate space might effectively model network distance. But we know there are many more factors besides physical distance that impact network distance! We don‚Äôt know what they are, but they‚Äôre embedded somehow in the measurement data we‚Äôre collecting. We want enough dimensionality in our coordinates so that by iteratively and randomly sproinging away from other nodes, we‚Äôre capturing all the factors that determine our RTTs, but not so much that the data that we‚Äôre collecting is redundant. Anyways, 8 coordinates, plus (again) some grace notes.\n\nArmon Dadger, the CTO of HashiCorp, has a really excellent talk on Vivaldi that you should just watch if this stuff is interesting to you.\n\nFrankly, I‚Äôm writing about Vivaldi because it‚Äôs neat, not because I get a huge amount of value out of it. In theory, Serf‚Äôs Vivaldi implementation powers ‚Äúnearness‚Äù metrics in Consul, which in our experience have been good but not great; I‚Äôd trust relative distances and orders of magnitude. But RTT‚Äôs aside, you could also theoretically take the 8D coordinates themselves and use them to do more interesting modeling, like automatically creating clusters of nearby or otherwise similar nodes.\n\nA last Serf thing to point out: membership is interesting, but if you have a membership protocol, you‚Äôre epsilon away from having a messaging system, and Serf does indeed have one of those. You can send events to a Serf cluster and tell your agent to react to them, and you can define queries, which are events that generate replies. So, I can set up Serf this way:\n\nAnd now we can query the load on all our nodes:\n\nUnder the hood, Serf is using logical timestamps to distribute these messages somewhat (but imperfectly) reliably. I love logical timestamps so much.\n\nPainlessly boot up clusters on Fly and link them with WireGuard to your computer, AWS, or GCP.\n\n\n## \u0026 Scene!\n\nSo anyways, my point here is, you can do a lot better than DNS for service discovery in a 6PN setup on Fly. Also my point is that Serf is really useful and much, much easier to set up and run than Consul or Zookeeper; you can bake it into a Dockerfile and forget about it.\n\nAlso, my point is that Fly is a pretty easy way to take distributed systems tools like this out for a spin, which you should do! You can boot a Consul cluster up on Fly, or, if you‚Äôre an Elixir person, you could use Partisan instead of Serf, which does roughly the same kind of thing."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/get-fly-with-your-fly-command-line/",
    "content": "The flyctl command is your route to harnessing the immense power of the global Fly network‚Ä¶ well, it‚Äôs a pretty cool command. There are plenty of features in it that not everyone knows. With that in mind, here are some tips for some higher Flying:\n\nThe command‚Ä¶ It started off as flyctl but we kept being asked when would the command become fly. ‚ÄúWhy not both?‚Äù we said and now, on most platforms, you can use flyctl or fly interchangeably. Boom! Three keystrokes saved!\n\nThe current app‚Ä¶ You already know that Fly looks for a fly.toml file in the current directory to work out what the app you are working with is called. But for many commands you can also add -a appname to your Fly command and it‚Äôll use that appname instead. If there‚Äôs a fly.toml in your current directory, it will ask if you are sure that‚Äôs the name you want to use to be on the safe side. So now you don‚Äôt have to change directory to, say, get a status on mygreatapp99 - just do fly status -a mygreatapp99.\n\nBuilding from outside the directory‚Ä¶ The fly deploy command can work with that. Say you want to build the app in a child directory ‚Äúmygreatchildapp‚Äù. Then just point the deploy at it. fly deploy ./mygreatchildapp and Fly will find the fly.toml in there and get to deploying.\n\nDeploy different‚Ä¶ What should you do if you have a Fly app that you want to deploy a couple of different ways. If, for each way, you‚Äôve built a fly.toml file how do you pull that all of them together under one directory? Rename the fly.toml files to something meaningful for your app test.toml, preview.toml, production.toml and then leverage the -c aka --config flag on fly deploy. That gets you the ability to run fly deploy -c preview.toml and away it goes deploying with that config. Oh, and you can combine that with the previous item on this list and then build out of your mono repo tree without changing directory.\n\nEasy opening‚Ä¶ Are you working out what your application‚Äôs ‚Äú.fly.dev‚Äù host name is? Save time with the fly open command that works out the app‚Äôs URL for you and opens your browser on that page.\n\nGet Metrics‚Ä¶ Some things look better rendered in your browser, like metrics. Now you may know the fly dash command which opens up your Fly web dashboard in your browser. Well, you can go one better with fly dash metrics which takes you straight to the dashboard metrics page for some graphical delight and a lot of information.\n\nGo Docs‚Ä¶ If you want to look something up in the docs and you are at the command line, try fly docs which opens your browser straight into the documentation‚Äôs top page."
  },
  {
    "title": "Run apps with long lived connections",
    "url": "https://fly.io/blog/graceful-vm-exits-some-dials/",
    "content": "Fly.io transforms containers into swarms of fast-booting VMs and runs them close to users. You can now delay VM shutdowns up to 24 hours to let the overly attached clients finish their work.\n\nFly apps are typically fast to boot, and it‚Äôs relatively easy to boot new VMs. We start them up, do some health checks, and then add them to our load balancer and DNS service discovery. But what comes up must go down. We shut VMs down for any number of reasons ‚Äì new deploys, or scaling, or for maintenance on underlying hardware.\n\nBy default, we send a SIGINT to tell a VM it‚Äôs time to go away. Then we wait 5 seconds and, if the VM is still running, we forcefully terminate it. This works fine most of the time, especially for application servers, but some work takes longer to clean up. A live video streaming service may a have users (often teenagers) connected for hours at a time. Or database servers might have in flight transactions to commit before terminating.\n\nKeeping processes alive longer is a boring, simple way to solve these kinds of problems. So you can now tell us to keep VMs for up to 24 hours after we send a shutdown signal. And you can also specify what signal we send (because signal handling is wildly inconsistent). Just add these options to your fly.toml configuration file:\n\nLaunch your Docker apps on Fly and we‚Äôll keep them alive while your users finish what they‚Äôre doing.\n\n\n## Example: drain long lived TCP connections\n\nHAProxy is an open source project for load balancing TCP and HTTP connections. Most HTTP requests are fast, but you might also run HAProxy to handle large user uploads or load balance across databases.\n\nBy default, a SIGINT causes an HAProxy server to immediately close all connections and shut down, aka ‚Äúhard stop‚Äù. If you‚Äôd rather cleanly drain connections instead of serving errors, you can use the ‚Äúsoft stop‚Äù mode and specify a long kill timeout.\n\n\n## Example: gracefully shutdown a database server\n\nPostgres responds to the SIGINT signal (our default) by immediately aborting open transactions and closing all connections. This is called the ‚Äúfast shutdown‚Äù mode and results in discarding data and causing application errors. Instead, you can now use the ‚Äúsmart shutdown‚Äù mode by sending SIGTERM and giving it five minutes to commit transactions.\n\n\n## Shared infrastructure and long lived connections\n\nModern cloud infrastructure forces a lot of application compromises, especially when you‚Äôre sharing infrastructure. Most cloud function and container hosting products sit behind layers of shared services, each needing frequent releases to keep them humming along. Releases are disruptive, especially for software that proxies user connections to arbitrary containers.\n\nProviders simplify their lives by limiting what customer containers can do ‚Äì¬†they might only serve HTTP, for example, or have to implement a custom event handler. If an app can only speak HTTP, has to complete every requests within 30 seconds, it‚Äôs very simple to roll out new proxy releases.\n\nThis is silly, but we‚Äôre not immune. We run a global load balancer service in front of Fly apps. When you use it for HTTP or TCP connections, our releases can disrupt in flight connections. We do as much as we can to minimize the impact and drain connections over a period of minutes when necessary.\n\nSome apps need us to get the heck out of the way. We‚Äôve built our plumbing specifically to allow this. You can opt out of our HTTP and TLS handlers, for example. And if you run a UDP service, our load balancing is entirely stateless. And we have experimental stateless TCP load balancing! If you have an app that needs to keep connections alive as long as possible, let us know, we‚Äôll help you how to try it out.\n\nDo you want to know more? Or have an idea? We‚Äôve got a community forum just for you."
  },
  {
    "title": "Connect your containers with WireGuard",
    "url": "https://fly.io/blog/ipv6-wireguard-peering/",
    "content": "Fly.io transforms containers into swarms of fast-booting VMs and runs them close to users. Now you can connect those swarms privately to other networks with WireGuard.\n\nThey say that when you‚Äôre starting a product company, it‚Äôs a better plan to chase down something a bunch of people will really love a lot than it is to try to build something that everyone will just like a little bit. So when Fly.io launched, it had a pretty simple use case: taking conventional web applications ‚Äì applications built for platforms outside of Fly.io ‚Äì and speeding them up, by converting them into Firecracker MicroVMs that we can run close to users on a network of servers around the world.\n\nThis works great, and you should try it; for instance, if you‚Äôve got an application running on Heroku, we have it down to just a button click on a web page. Or, our Fly.io speed-run can get a Dockerized application deployed everywhere from Chile to Singapore in just a couple commands. It‚Äôs easier than figuring out how to use rsync; so easy, it‚Äôs boring.\n\nBut, predictably, people have wanted to launch other stuff on Fly, besides making existing applications go fast. And we want to help them do that. We like stuff! We like talking to people about stuff! And we‚Äôve gotten pretty good at getting stuff working on Fly. But stuff hasn‚Äôt always been as boring as we‚Äôd like.\n\nWe‚Äôre striking blows for the forces of boredom, by making it straightforward to get just about anything running on Fly.io. One way we‚Äôre doing that is by making it easy to peer networks with Fly, using WireGuard and IPv6.\n\nHere‚Äôs the TL;DR:\n\n\n## IPv6 Private Networking at Fly\n\nApps on Fly.io belong to accounts, which are associated with organizations. You don‚Äôt have to grok this; you just have an ‚Äúorganization‚Äù, trust us. Every Fly.io organization has its own private IPv6 network; we call them ‚Äú6PNs‚Äù, or, at least I do; I‚Äôm trying to make it a thing.\n\nEvery app in your organization is connected to the same 6PN. Every app instance has a ‚Äú6PN address‚Äù. Bind services to that address and they‚Äôre available other apps in your private network; bind a service only to it, and it‚Äôs only reachable privately.\n\nYou don‚Äôt need to understand the rest of this section, but in case you‚Äôre interested:\n\nWe carve up IPv6 addresses and embed information in them. We start with the IPv6 ULA prefix fdaa::/16 (the ULA space in IPv6 is analogous to the 10-net space in IPv4, except there‚Äôs a lot more of it).\n\nThen, for every instance of every app we run, we collect a bit of information: a ‚Äúnetwork ID‚Äù associated with the app‚Äôs organization, an identifier for the hardware the instance is running on, and an identifier for the instance itself, and come up with this gem of an IPv6 address:\n\nTechnically, what we end up delegating to each instance is a /112, which is the IPv6 equivalent of an IPv4 Class B address; you can address 65,000 (and change) different things inside of an instance if you wanted to. I haven‚Äôt come up with any kind of use for this, but, why not?\n\nMeanwhile, an organization has effectively a /48, or ‚Äúmind-bogglingly huge‚Äù, 6PN prefix.\n\nThe core design idea of this system is pretty simple: we control IPv6 address assignments and routing inside our network. To lock an instance into a 6PN network, all we really need is a trivial BPF program that enforces the ‚Äúdon‚Äôt cross the streams‚Äù rule: you can‚Äôt send packets between different 6PN prefixes. We‚Äôre already BPF‚Äôing all our interfaces to make UDP work, so this is an easy change.\n\n\n## 6PN DNS\n\nHaving all these IPv6 addresses doesn‚Äôt help much if your apps can‚Äôt find each other, so we run an internal DNS service for our 6PN networks. So in reality, you never think about 6PN at all. You just need the names of your apps.\n\nOur DNS service is a small Tokio Rust program backed by sqlite databases that our service discovery system builds on all our hosts. It accepts packets only from 6PN addresses, and, because of its network position, can trust source addresses, which it uses to determine the answers to questions.\n\nAlso, it forwards external DNS queries, so it can stand in as the sole nameserver in resolv.conf. That was another 20 lines of code.\n\nI wish the server was interesting enough to talk about more, but it‚Äôs not; it‚Äôs practically the ‚Äúhello world‚Äù of the NLNet ‚Äúdomain‚Äù crate. If you don‚Äôt use Fly.io, my message to you in this section is mostly ‚Äúgo forth and build ye a Rust DNS server, for lo, it is pretty easy to do‚Äù. Also, deploy it on Fly.io, it‚Äôs great.\n\nIf you do use Fly.io, first, thanks and congratulations. Also you might be interested in our naming scheme. Assume your app is fearsome-bagel-43, and has a sibling app serf-43. Then:\n\nFor instances of apps running in Fly, DNS is available on fdaa::3 (the one exception to the ‚Äúno crossing the streams‚Äù 6PN access rule).\n\n\n## Internal WireGuard at Fly\n\nThe basic architecture of Fly.io is that we have hardware colocated in datacenters around the world. We direct global traffic to the edge of our network with BGP-driven Anycast, and we route it to nearby worker servers over Jason Donenfeld‚Äôs WireGuard.\n\nWireGuard is amazing. It will likely replace all other VPN protocols. But it‚Äôs so lightweight and performant that I think it‚Äôs going to change the role VPNs have. It‚Äôs just as easy to set up a WireGuard connection as it is an SSH account. And you pay practically no performance penalty for using it. So you end up using VPNs for new things.\n\nWhat makes WireGuard so interesting?\n\nWe run an internal WireGuard mesh, about which we‚Äôll write more in the future. All you need to know here is that connectivity between hosts in our network happens entirely over WireGuard.\n\nI come from old-breed ISP stock, from a time when Cisco AGS+‚Äôs roamed the land, and what I was taught very early on is that the best routing protocol is static routing, if you can get away with it. With the information embedded in our addresses, we can route 6PN statically.\n\nBut there‚Äôs a catch. A central part of WireGuard‚Äôs design is the notion of ‚Äúcryptokey routing‚Äù. WireGuard peers are identified by a Curve25519 key (a short Base64 string), and each peering connection is tagged with a set of ‚ÄúAllowed IPs‚Äù. When the OS wants to send traffic over a WireGuard link, it routes packets to the WireGuard device, and WireGuard checks its table of peers to see which ‚Äúallows‚Äù that destination. For that to work, ‚ÄúAllowed IPs‚Äù can‚Äôt overlap between links.\n\nThat‚Äôs a problem for our 6PN design, because a 6PN prefix obviously has to run across a bunch of hosts, and there‚Äôs no way to wildcard a chunk out of the middle of an address.\n\nThe solution is straightforward, though: we just use BPF to temporarily swap the ‚Äúhost‚Äù and ‚Äúnetwork‚Äù chunks of the address before and after routing through WireGuard. Our WireGuard mesh sees IPv6 addresses that look like fdaa:host:host::/48 but the rest of our system sees fdaa:net:net::48. This turns out to be an extremely simple transform, since swapping bytes in an IPv6 header doesn‚Äôt alter checksums.\n\n\n## WireGuard Peering\n\nHere‚Äôs a thing you might want to do with an app running on Fly: connect it to to a database managed by AWS RDS.\n\nHere‚Äôs a way to do that: boot up a WireGuard gateway in AWS (here, with a few dozen lines of Terraform, but use whatever you like; if Fly.io stands for anything, it‚Äôs ‚Äúnot having to know Terraform‚Äù) that peers into your 6PN network and exposes a Postgres proxy like PgBouncer. It‚Äôs a pretty boring configuration, which is the kind we like.\n\nThis works today at Fly.io because of WireGuard Peering. We will generate WireGuard configurations for you that will work in APAC, North America, and Europe. To do that, just run flyctl wireguard create. We‚Äôll spit out a config that will drop into Linux, macOS, or Windows WireGuard.\n\nWireGuard peers get /120 delegations (the equivalent of an IPv4 class C), and an organization-specific DNS endpoint baked into the config. When you add a WireGuard peer, we update DNS across the fleet, so your peer is available by its name; if we called this peer rds-us-east-1, our apps could reach it at rds-us-east-1._peer.internal. We can get a list of peers by looking up the TXT at _peer.internal.\n\nA nice thing about this design is that it doesn‚Äôt require you to expose any management services on the AWS side; your AWS WireGuard gateway connects out to us, and the default security rules for your VPC should keep everything hermetically sealed inside (you obviously want to verify this part of your configuration; we‚Äôre just saying, we‚Äôre not asking you to open up any ports).\n\nOf course, you can also use WireGuard 6PN peering to manage your app instances directly; for example, the config we generate drags-and-drops into macOS WireGuard.\n\nLaunch your Docker apps on Fly and we‚Äôll seamlessly connect them to any network you‚Äôd like using WireGuard\n\n\n## Service Discovery In Fly Private Networks\n\nI think you can get pretty far designing applications with the DNS we expose right now, but we‚Äôve deliberately kept it boring, because we assume different people will want different things.\n\nBut nothing stops you from making service discovery exciting! I have, for instance, a multi-perspective DNS resolver example I‚Äôll publish shortly that uses Hashicorp Serf to auto-discover new nodes. The important thing to know about 6PN networking is that it‚Äôs direct between nodes; we don‚Äôt proxy it or meddle with it in any way. Anything you want to run, including your own full Consul cluster, should just work\n\nI‚Äôm pretty happy with how this design is turning out and optimistic that it achieves ‚Äúboring‚Äù for connecting arbitrary services to Fly applications. So you can use Fly not only to make existing applications run faster, but also as a core component of new applications. I‚Äôm kind of in love with the ergonomics of our dev UX (I can say that because, as a late arrival to the Fly team, I had no hand in designing it), and anything that lets me use flyctl for more stuff is a win in my book.\n\nDo you want to know more? Or have an idea? We‚Äôve got a community forum just for you."
  },
  {
    "title": "New VMs, delivered fresh",
    "url": "https://fly.io/blog/new-vms-more-ram-extra-cpu-and-a-dollar-menu/",
    "content": "Fly.io turns your container apps into swarms of fast-booting VMs and runs them close to your users. Bless your users with 2GB of RAM for about ten bucks per month. Or shop the dollar menu for tiny VMs *.\n\nWe first showed Fly.io VMs to developers in early 2020. They were most interested in running CPU intensive apps doing image processing, machine learning predictions, and even video transcoding (despite what were, until recently, offensive bandwidth prices). So when we launched, most of the available VMs were thicc like oatmeal, but weak on the RAM.\n\nThe requests changed when we got in front of more developers. We knew we‚Äôd have to solve databases ‚Äúat some point‚Äù, we just didn‚Äôt expect devs to ask day one. Databases ‚Äì and apps that look like databases when you squint ‚Äì need a higher RAM to CPU ratio. Small databases hoover up RAM, but largely leave the CPU alone.\n\nOur new VMs come in two flavors, shared-cpu with up to 2GB of RAM for lightweight apps, and dedicated-cpu with up to 64GB of RAM for not-quite-big-data-but-it-should-be-fast. Here‚Äôs the price breakdown (and if you‚Äôre a future reader, you might see even more VM types):\n\n\n## Cloud VM pricing, an abridged guide\n\nWe like working on Fly.io, and want to continue hacking away. Sustaining the company means getting to 70% margins on VMs ‚Äì¬†a number that comes up all the time.\n\nThe dirty secret of Virtual Machine pricing is ‚Ä¶ the machines are virtual. The specs we promise are only loosely constrained by the underlying hardware. And there‚Äôs no reason to tell customers the host hardware specs [1]. We can even sell the same RAM several times over. Yay margins!\n\nFortunately, we don‚Äôt want to make money on VMs. We do, however, want to help happy customers and minimize our own operational headaches. Oversubscribing host hardware is a great way to piss off customers and keep us hoppin‚Äô while we‚Äôre on call. A few unexpected OOM errors will ruin everyone‚Äôs day.\n\n\n## To the slide rule!\n\nIf you do the math, you‚Äôll estimate that our cost to run a dedicated-cpu-1x VM is a little under $10 per month. That‚Äôs a ridiculous simplification, but good for hasty math.\n\nThe actual cost is a function of hardware + colocation + power. And we commit to the hardware in yearly increments, while we bill you in seconds. For additional fun, we have to buy servers before we get customers. So there‚Äôs dead time before we‚Äôre even covering the expense, much less making margins.\n\nReading that back, it actually sounds pretty terrible. But we‚Äôre lucky to have levers that make it work.\n\nShared CPU instances let us fill in the gaps with a lower priced product that‚Äôs decoupled from margins. We can load large hosts up with shared VMs, cover our costs pretty quickly, and avoid nasty surprises.\n\n\n## Shared CPU VMs\n\nOver subscribing CPUs is less fraught than over subscribing RAM since there‚Äôs enough oomf on any given server to move things around and avoid contention. We‚Äôve run micro VMs for the last 6 months to get an idea of how to price these, and the results have been favorable for pricing.\n\nWe can pool a bunch of CPUs together, and share each with an average of 12 VMs. This works great for bursty apps, the big pool of CPUs lets us mix and match busy and idle VMs to produce vast quantities of heat maximise CPU utilization.\n\nThis works 100% of the time until it doesn‚Äôt. CPU contention is a given. We have safeguards in place to ensure that VMs trying to eat a whole CPU have to give it up to your polite VM (cpuset.priority if you‚Äôre a cgroups nerd). And when that doesn‚Äôt solve the contention, we can move VMs to different hardware. Sometimes in different regions!\n\nThe best part? These VMs cost you $1.94 per month to run full time. That‚Äôs cheaper than most dollar menus, these days [^].\n\nLaunch your Docker apps on Fly and we‚Äôll run them in Firecracker VMs for even cheaper than you expected.\n\n\n## But running databases on ephemeral VMs is silly\n\nDatabase apps also need persistent storage. VMs that boot up, then go away and take all their storage with them are not a good place to run DBs. We don‚Äôt sell persistent storage. But, you know, we definitely should."
  },
  {
    "title": "We got private networking, y‚Äôall",
    "url": "https://fly.io/blog/incoming-6pn-private-networks/",
    "content": "More often than not, modern applications are really ensembles of cooperating services, running independently and transacting with each other over the network. At Fly.io, we‚Äôd like it to be not just possible to express these kinds of applications, but pleasant, perhaps even boring.\n\nUp till now, that‚Äôs been a hard promise for us to fulfill, because services deployed on Fly.io ran as strangers to each other. You could arrange to have a front-end cache service talk to a backend app service, but they‚Äôd need to rendezvous through public IP addresses. More frustratingly, you‚Äôd need to secure their connection somehow, and the best answer to that is usually mTLS and certificates. Ack! Thbhtt!\n\nIt shouldn‚Äôt be this hard. Fly.io is fully connected through a WireGuard mesh joining every point in our network where services can run. We already promise a secure transport for your packets. You might derive satisfaction from running your own CA, and if that‚Äôs your thing, we‚Äôre not here to judge. But you shouldn‚Äôt have to. And now you don‚Äôt.\n\n\n## Introducing 6PN\n\n6PN (for IPv[6] [P]rivate [N]etworking) is our answer to the basic ‚ÄúVPC‚Äù feature most cloud providers offer. It‚Äôll soon be on by default and requires no additional configuration. A 6PN network connects all the applications in a Fly.io organization.\n\nEvery instance of every application in your organization now has an additional IPv6 address ‚Äî its ‚Äú6PN address‚Äù, in /etc/hosts as fly-local-6pn. That address is reachable only within your organization. Bind services to it that you want to run privately.\n\nIt‚Äôs pretty inefficient to connect two IPv6 endpoints by randomly guessing IPv6 addresses, so we use the DNS to make some introductions. Each of your Fly apps now has an internal DNS zone. If your application is fearsome-bagel-43, its DNS zone is fearsome-bagel-43.internal ‚Äî that DNS resolves to all the IPv6 6PN addresses deployed for the application. You can find hosts by region: nrt.fearsome-bagel-43.internal are your instances in Japan. You can find all the regions for your application: the TXT record at regions.fearsom-bagel-43.internal. And you can find the ‚Äúsibling‚Äù apps in your organization with the TXT record at _apps.internal.\n\nTo fully enable this feature, you need to add a snippet of config to your fly.toml:\n\n\n## Some Examples\n\n\n## Caching services\n\nLet‚Äôs say we want to run a high-capacity sharded nginx cache. We can create an almost-vanilla nginx.conf with an upstream for our cache nodes:\n\nIn the Dockerfile for this application, we can include a script that looks up the 6PN addresses for our application ‚Äî in bash, you can just use dig aaaa fearsome-bagel-43.internal +short to get them. Substitute them into nginx.conf as server lines:\n\nserver [fdaa:0:1:a01:a0a:dead:beef:2]:8080 ;\n\n‚Ä¶ and then reload nginx. The consistent hashing feature in nginx balances traffic across your shards, and minimizes disruption as instances join and leave.\n\n\n## Databases\n\nHere‚Äôs a simpler example: let‚Äôs run a Redis server for the apps in our organization. We can start from the standard Redis Dockerfile ‚Äî FROM redis, and write a trivial start.sh:\n\nCreate a Fly app, like redis-bagel-43 (I don‚Äôt know what it is with me today). The rest of your apps will see it once it‚Äôs deployed, as redis-bagel-43.internal.\n\nThe nice thing about this is that Redis is default-locked to your organization, just as a consequence of how 6PN works. You could set up TLS certificates to authenticate clients, but if you‚Äôre not doing something elaborate with your organization, there‚Äôs probably no need.\n\n\n## Messaging with NATS\n\nOr, how about linking up all your applications with a global messaging fabric? You could use that to build a chat app, or to ship logs, or to build event-driven applications. Once again: we can use an almost-verbatim vendor Dockerfile:\n\nAnd the only interesting part of that configuration:\n\nNATS will configure itself with available peers, and your other applications can get to it at nats-bagel-43.internal.\n\n\n## Behind The Scenes\n\nI‚Äôll take a second to explain a bit about how this works. Skip ahead if you don‚Äôt care!\n\nIPv6 addresses are big. You just won‚Äôt believe how vastly, hugely, mind-boggling big they are. Actually, no, you will; they‚Äôre 16 bytes wide. So, just ‚Äúpretty big‚Äù.\n\nWe use that space to embed routing and access information; an identifier for your organization, an identifier for the Fly host that your app is running on, and an identifier for the individual instance of your app. These addresses are assigned directly by our orchestration system. You‚Äôll see them in your instance, on eth0; they‚Äôre the addresses starting in fdaa.\n\nWe route with a sequence of small BPF programs; they enforce access control (you can‚Äôt talk to one 6PN network from another), and do some silly address rewriting footwork so that we can use WireGuard‚Äôs cryptokey routing to get packets from one host to another, without running a dynamic routing protocol.\n\nIt‚Äôs a boring detail, but in case you‚Äôre wondering: our service discovery system populates a database on each host that we run a Rust DNS server off of, to serve the ‚Äúinternal‚Äù domain. We inject the IP of that DNS server into your resolv.conf ‚Äî the IP address of that server is always fdaa::3.\n\n\n## Where Our Heads Are At\n\nThere‚Äôs a theme to the way we build things at Fly (or at least, Kurt has a theme that he keeps hitting us over the head with). We like interesting internals ‚Äî WireGuard, Firecracker, Rust, eBPF ‚Äî but boring, simple UX. Things should just work, in the manner you‚Äôd hope they would. Managing Fly.io app ensembles shouldn‚Äôt even be close to anyone‚Äôs full-time job.\n\nSo we‚Äôve kept 6PN as boring as we can. You can make things interesting and weird if you want! Run a Serf cluster between all your apps! Boot up Consul or etcd. Set up a CA. You can make service discovery and security as interesting as you want. We‚Äôre going to try to stay out of the way.\n\nYou might be able to guess what our next steps are: we‚Äôre going to make it boring to connect other networks and services to your private network. Follow us on community.fly.io for early announcements of new networking features.\n\nLaunch your Docker apps on Fly and get baked in, secure private networking between instances."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/we-cut-bandwidth-prices-go-nuts/",
    "content": "Fly.io turns your Docker apps into Firecracker VMs and runs them all over the world. We‚Äôre now charging $0.02 per GB for outbound data transfer from North America and Europe, and $0.04 per GB almost everywhere else.\n\nWhen we launched Fly way back in March, we charged $0.085 per GB to send data out from North America and Europe, and $0.14 per GB for Asia Pacific. Our volume has increased dramatically and pushed our unit costs down, so we lowered our prices as of December 1st 2020. The net result is 75% cheaper bandwidth for apps running on Fly.io [1].\n\n\n## The joy of bandwidth pricing\n\nIf you shop around, you‚Äôll find book stores-turned-hosting-providers who charge their captives exorbitant fees to move data around. It‚Äôs high margin and locks people in. What more could you ask for?\n\nYou‚Äôll also find companies who don‚Äôt charge for bandwidth ‚Ä¶ directly. They do, but it‚Äôs not a line item on anyone‚Äôs bill.\n\nWe‚Äôve gone back and forth on the right way to price bandwidth into our services. Pricing projects are my favorite kind of scope creep. It‚Äôs easy to go from ‚Äúwhat should this cost?‚Äù to ‚Äúhow do we want people to behave?‚Äù to ‚Äúwhat even is consciousness?‚Äù\n\nUltimately, we want to accomplish two things:\n\nThese are at odds with each other! So we‚Äôve made a few trade offs.\n\n\n## Making it work for everyone\n\nNormally, scaling bandwidth pricing means graduated tiers or minimum commits with overage. Those are complex, though, and hard to wrap your head around (and, depending on who you‚Äôre buying from, hard to even get in writing). So we decided to charge everyone the same price. Customers with intense, high volumes apps will pay the same bandwidth rates as individual devs on Fly.io.\n\nThose high volume tiers are important for attracting the right customers, though. People who build video communications apps, game servers, and photo services need to keep bandwidth prices low so they can make money. Normal cloud bandwidth pricing (including our previous pricing) is much too expensive for these kinds of apps.\n\nWhat we‚Äôve really done is make a one tier pricing scheme with the lowest price we can charge and still make reasonable margins. If you‚Äôve ever wondered what reasonable margins are, the answer is 70%. We need to make a little more than three times our costs to keep doing what we‚Äôre doing.\n\n\n## Some light math\n\nIf you do the margin math, you‚Äôll estimate that our cost to deliver 1GB of data in North America is about $0.006 per GB. In Singapore it‚Äôs about $0.012 per GB. These are pretty close, but they aren‚Äôt exact. We have roughly 11 different bandwidth rates, depending on the facility and region we‚Äôre running servers in. We also pay for bandwidth between servers and regions. One GB of data from Singapore to an end-client in New York City has two different ‚Äúcosts‚Äù. [2]\n\nInflicting that complexity on you all would help with margin control, but ugh. What we‚Äôve done instead is set a blended price that fits most apps running on Fly.io, and decided to just eat the extra cost from outliers. If you want to exploit that, run an app in Sydney with a whole bunch of users in India. We‚Äôll lose money on your app and you will win one round of capitalism.\n\nSince we tend to favor transparency and predictability over ‚Äúprice that scales well‚Äù, we‚Äôve ended up with pricing that will be too high for some customers. Which is fine, we‚Äôre growing, but we have a plan for that. If these bandwidth prices don‚Äôt work for your use case, and you can commit to a large amount of data transfer each month, we‚Äôll lower these prices for everyone.\n\n\n## The many meanings of the word ‚Äúfree‚Äù\n\nWe should also talk about free bandwidth. Free can be more complicated than transparent pricing. Companies that pitch free bandwidth typically mean free for a narrow, low volume use case. When your volume goes up, either because you‚Äôre building a YouTube competitor or because your self hosted community was featured on CNN, a friendly sales person will ring you up and offer an enterprise package to ensure continued service quality. Or they shift traffic to lower cost (and less reliable) networks. Either way, it‚Äôs not really my favorite surprise.\n\nThat said, when I run side projects, or apps that don‚Äôt serve much data, I don‚Äôt really want to think about how much bandwidth is costing me. We created a free tier to cover this, you get 160GB to use each month before you start paying us. We hope that‚Äôs enough for your side projects (and if it‚Äôs not, tell us what you‚Äôre working on).\n\n\n## We don‚Äôt really want to make money on bandwidth\n\nGo build yourself a CDN. Or the next YouTube. Or just make a little family photo album app in Elixir. We‚Äôll take it, launch it in Firecracker VMs, and charge you very little for bandwidth."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/the-november-fly-changelog/",
    "content": "More example applications, enhancements to the Fly command line and more. This is what‚Äôs new on Fly in November 2020.\n\n\n## Example Applications\n\nWe‚Äôve been focussing on getting more example applications for Fly into your hands. Each guide is complete with a walkthrough of how they were put together so you can see the power of Fly in action. Or you can hop straight to the Github repositories for each and use the files there to deploy immediately. There are guides for:\n\nOne thing this set of Guides do is make use of the - currently in preview - persistent Volumes support for Fly apps, aka writable disk space. They are used here for everything from database persistence and data storage, to preserving state in programming environments and queuing up messages. And more example applications are on the way.\n\n\n## Importable secrets\n\nWe‚Äôve added an import command to fly secrets to let you import secrets in bulk into an app. Just create a ‚Äúkey=value‚Äù file of settings and pipe it into fly secrets import and they will be transferred to the app in one block. As an added bonus, you can create multiline secrets by surrounding that value in two pairs of three double quotes:\n\n\n## Other changes\n\nThere‚Äôs a whole bunch of other enhancements and bug fixes in the Fly CLI. More options in the fly list apps command to help filter and sort through your applications. More JSON output to assist developers building automation. The Deno builtin now supports a Version setting which defaults to 1.5.4; as newer Deno release appear it makes it easier to either pin to an older version or track the latest developments. Read the full changelog below for more details.\n\nThis is the Fly Changelog where we list all significant changes to the Fly platform, tooling, and websites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\n\n## Over November\n\nFly Platform\n\nFly Web\n\n\n## 30th November\n\nflyctl: Version 0.0.151 released\n\n\n## 16th November\n\nflyctl: Version 0.0.150 released\n\n\n## 12th November\n\nflyctl: Version 0.0.149 released\n\nflyctl: Version 0.0.148 released\n\n\n## 11th November\n\nflyctl: Version 0.0.147 released\n\n\n## 4th November\n\nflyctl: Version 0.0.146 released"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/more-appkata-examples-to-try-on-fly/",
    "content": "There are two new additions to our Appkata collection of example apps, one‚Äôs a secured Redis and the other is Node-RED, a powerfule online application construction kit. Try them out today!\n\nThe latest additions to our Appkata collection of example apps have landed and they cover how to fit Fly to your selected applications and are some super useful apps anyway. So let‚Äôs dive in.\n\n\n## Node-RED\n\nEver wanted to be able to dive into your cloud application and graphically wire up a dashboard or data transfer? You can with Node-RED, a GUI flow programming environment that hails from the world of IoT. It‚Äôs built on top of Node and has a vast library of modules so you can create flows which bridge MQTT, Redis, Graphana, and more with your own logic. Or you can turn the data into a web dashboard. It‚Äôs a powerful tool and the new Appkata guide will get it up and running quickly.\n\n\n## Redis with TLS\n\nOur first set of examples included Redis as a simple service, but we wanted to give you a more solid version. Enter Redis with TLS. This uses mkcert to make it as quick and simple as possible to create TLS certificates so you can secure your Redis. It uses techniques I wrote about on the developer site dev.to.\n\n## Community.fly.io\n\nIf you follow the links to community.fly.io, you will see that our new community site is a great place to ask questions, look for hints and tips and get your Fly apps running at their best. If you have a Fly account, sign in today. It‚Äôs easy as we‚Äôve enabled single sign-on between Fly and the community site making it simple to participate."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/appkata-example-apps-on-fly/",
    "content": "Learn by example with our new example apps for Fly. We‚Äôre starting off with databases, object storage, Git services, and event messaging and in the process we‚Äôll learn how to make any app Fly.\n\nTo build a modern application, you need to know how to take down your technical challenges in style. It‚Äôs a style we call Appkata! Over the coming weeks, we‚Äôll be sharing with you the moves you need to deploy apps onto Fly that let you build faster. This week we kick off with four apps for your backend that give you a fast key/value database, an accessible S3 storage node, your own private Git server and a lightweight, powerful event-driven messaging server.\n\n\n## Redis\n\nWe already have a Redis on Fly, but this Redis is all your own to run in whatever region or regions you need it in. It gives you complete control of the server and makes features like pubs available. This first Redis example shows how you build on existing Docker images and can customize them for better performance of Fly. It also shows how to use the Volumes feature of Fly to add persistent storage to your Redis so it can be restarted and recover like nothing had happened.\n\n\n## MinIO\n\nTalking about the persistent storage that Volumes offer got us to thinking that we should expose that directly to the world, and what better way than turning it into an S3-compatible object store (yes, yes, there are better ways, but everyone knows how to use S3). For this we reached for MinIO, a free software implementation of the S3 platform. In this Appkata example, we set up a single node for MinIO with an attached persistent volume and access it over Fly‚Äôs TLS edge connections, through the web for admin and through the MinIO client.\n\n\n## Gogs\n\nAnother way to expose persistent storage, if you wave your hands abstractly enough, is Git which is more typically thought of some sort of source code control system. One of the lightest ways to get a useful Git service is with the open-sourced Gogs server, designed to be as painless as possible. The Appkata example for Gogs shows how you can deploy it, using its SQLite3 database option and Fly‚Äôs volumes, to have your own private Git server, with issue tracker, web hooks and other modern development essentials, up and running in no time.\n\n\n## MQTT\n\nMessaging between apps and devices makes your Appkata style flow smoothly. Now, you can use Redis to pass messages, but there‚Äôs plenty of other messaging platforms about which have easily accessible pubs and quality of service features. One of those is MQTT, a protocol developed for Internet of Things applications which is also at home connecting apps. We‚Äôve put together an Appkata example which uses Mosquitto, an open source MQTT broker, and brings it online with TLS-encrypted connections and user/password authentication, all backed by a Fly Volume so the broker can persist messages when things are busy or slow.\n\nSo, there are four example apps, each of which can be used as part of your app fighting style. We‚Äôll be adding new examples and building on these examples over the coming weeks.\n\n\n## Community.fly.io\n\nIf you follow the links to community.fly.io, you will see that our new community site is a great place to ask questions, look for hints and tips and get your Fly apps running at their best. If you have a Fly account, sign in today. It‚Äôs easy as we‚Äôve enabled single sign-on between Fly and the community site making it simple to participate."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-answers-questions-suspend-resume-restart-and-redis/",
    "content": "We get asked questions about Fly in a lot of places on the web which we answer. But, not everyone is everywhere on the web, so with Fly Answers Questions, we bring those answers to you. If you have questions about Fly, why not ask @flydotio on Twitter, or drop a query in the Fly Community.\n\nQ: I‚Äôd like to park my Fly application for a little while so it isn‚Äôt consuming resources. How can I do this without destroying the application and redeploying it later?\n\nA: As you‚Äôll have noticed, Fly applications stay running all the time ready to service your traffic. You can, as you say, destroy the application and redeploy it, but you‚Äôll lose configuration with that process.\n\nWhich is why the fly suspend command exists. Rather than destroy the app, it turns the number of instances ‚Äî VMs running the app ‚Äî down to zero. At this point the application is effectively not running, but all its status and settings are intact.\n\nBringing it back is a case of running fly resume which initially brings back one instance to ensure all is well and then fully redeploys the application using its previous configuration.\n\nQ: Followup question: what‚Äôs the restart command then?\n\nA: Well spotted. The fly restart command is nothing to do with suspended applications. It doesn‚Äôt trigger any deployments either. What it does is go to each instance of your application and restart them in place. It‚Äôs useful for when your app may have got into an indeterminate state and you‚Äôd like to start as fresh as possible but without redeploying.\n\nQ: I was looking at using Fly‚Äôs Redis support but got an error when I went to use the subscribe command. What‚Äôs up with that?\n\nA: Publish/Subscribe is not supported on Fly‚Äôs Redis implementation. It joins a small set of unsupported Redis commands. Most unsupported commands are related to server or client management. The PubSub commands, along with Redis scripting commands and the Geo-based Redis commands are currently not available.\n\nQ: Can I set the minimum number of instances for an app to 0?\n\nA: No, Fly applications will always have one or more instances running. If you want to go to 0, you have to suspend your application, see the first question for more on that.\n\nQ: Isn‚Äôt the Fly CLI called flyctl? You‚Äôve been using the fly command in these answers.\n\nA: The answer to that is ‚ÄúWhy not both?‚Äù. We‚Äôre slowly migrating the flyctl command to the quicker - to type and say - fly command. When you install flyctl now, it creates a symlink from flyctl to fly and you can use either to run the command. Over time, we‚Äôll move completely to the shorter command, with an eye to back-compatibility for your scripts.\n\nThat‚Äôs it for this Fly Answers Questions. Get your questions in on community.fly.io or on Twitter.\n\n\n## Community.fly.io\n\nIf you follow the links to community.fly.io, you will see that our new community site is a great place to ask questions, look for hints and tips and get your Fly apps running at their best. If you have a Fly account, sign in today. It‚Äôs easy as we‚Äôve enabled single sign-on between Fly and the community site making it simple to participate."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/the-october-fly-changelog-preview-disks-and-dns-and-better-builtins/",
    "content": "Persistent storage is now available, in preview, for Fly applications. Builtin builders are more configurable and there‚Äôs a new watch mode for automatically updating app status. All this and more in the Fly Changelog for October.\n\n\n## Disks in Preview\n\nIf your application needs to have persistent storage, then the currently in preview volumes feature is for you. The volumes command allows you to create persistent disks for your application. These persist between restarts, deployments and even the app being suspended. Further details about the preview and how to use it are in the disks preview thread in the fly community.\n\n\n## Env Variables Support\n\nYou can now set environment variables in your fly.toml file, rather than overloading the secrets feature of Fly. It‚Äôs an extra section in the fly.toml file and that‚Äôs documented in the Fly configuration reference.\n\n\n## Other Changes\n\nThe latest release of Fly‚Äôs CLI tool has a whole range of other improvements:\n\n\n## Better Builtins\n\nIf you use builtins and have found them not quite flexible enough for you, we‚Äôve now introduced builtin settings that allow you to control selected features in the builtins. Examples of this include the ability to set permissions in the Deno builtin builder and turn on HTTPS auto-upgrading and logging in the static web server.\n\n\n## Status Watching\n\nIf you‚Äôre tired of repeatedly typing in fly status to check your apps status (or hitting cursor up and return), the new --watch flag will make your life even easier. It switches to poll the status every 5 seconds and provides you with a regular update.\n\nThis is the Fly Changelog where we list all significant changes to the Fly platform, tooling, and websites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\n\n## 27th October\n\nflyctl: Version 0.0.145 released\n\n\n## 22nd October\n\nFly Platform/Web"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/bpf-xdp-packet-filters-and-udp/",
    "content": "Imagine for a moment that you run a content distribution network for Docker containers. You take arbitrary applications, unmodified, and get them to run on servers close to their users around the world, knitting those servers together with WireGuard. If you like, imagine that content delivery network has an easy-to-type name, perhaps like ‚Äúfly.io‚Äù, and, if you really want to run with this daydream, that people can sign up for this service in like 2 minutes, and have a Docker container deployed globally in less than 5. Dream big, is what I‚Äôm saying.\n\nIt‚Äôs easy to get your head around how this would work for web applications. Your worker servers run Firecracker instances for your customer applications; your edge servers advertise anycast addresses and run a proxy server that routes requests to the appropriate workers. There are a lot of details hidden there, but the design is straightforward, because web applications are meant to be proxied; almost every web application deployed at scale runs behind a proxy of some sort.\n\nBesides running over TCP, HTTP is proxy-friendly because its requests and responses carry arbitrary metadata. So, an HTTP request arrives at an edge server from an address in Santiago, Chile; the proxy on that edge server reads the request, slaps an X-Forwarded-For on it, makes its own HTTP request to the right worker server, and forwards the request over it, and this works fine; if the worker cares, it can find out where the request came from, and most workers don‚Äôt have to care.\n\nOther protocols ‚Äì really, all the non-HTTP protocols ‚Äì aren‚Äôt friendly to proxies. There‚Äôs a sort of standard answer to this problem: HAProxy‚Äôs PROXY protocol, which essentially just encapsulates messages in a header that ferries the original source and destination socket addresses. But remember, our job is to get as close to unmodified Docker containers as we can, and making an application PROXY-protocol-aware is a big modification.\n\nYou can make any protocol work with a custom proxy. Take DNS: your edge servers listen for UDP packets, slap PROXY headers on them, relay the packets to worker servers, unwrap them, and deliver them to containers. You can intercept all of UDP with AF_PACKET sockets, and write the last hop packet that way too to fake addresses out. And at first, that‚Äôs how I implemented this for Fly.\n\nBut there‚Äôs a problem with this approach. Two, really. First, to deliver this in userland, you‚Äôre adding a service to all the edge and worker servers on your network. All that service does is deliver a feature you really wish the Linux kernel would just do for you. And services go down! You have to watch them! Next: it‚Äôs slow ‚Äî no, that‚Äôs not true, modern AF_PACKET is super fast ‚Äî but it‚Äôs not fun. That‚Äôs the real problem.\n\n\n## Packet filters, more than you wanted to know:\n\nPacket filters have a long and super-interesting history. They go back much further than the ‚Äúfirewall‚Äù features the term conjures today; at least all the way back to the Xerox Alto. Here follows an opinionated and inaccurate recitation of that history.\n\nFor most of the last 20 years, the goal of packet filtering was observability (tcpdump and Wireshark) and access control. But that wasn‚Äôt their motivating use case! They date back to operating systems where the ‚Äúkernel networking stack‚Äù was just a glorified ethernet driver. Network protocols were changing quickly, nobody wanted to keep hacking up the kernel, and there was a hope that a single extensible networking framework could be built to support every protocol.\n\nSo, all the way back in the mid-1980s, you had CSPF: a port of the Alto‚Äôs ‚Äúpacket filter‚Äù, based on a stack-based virtual machine (the Alto had a single address space and just used native code) that evaluated filter programs to determine which 4.3BSD userland program would receive which Ethernet frame. The kernel divided packet reception up into slots (‚Äúports‚Äù) represented by devices in /dev; a process claimed a port and loaded a filter with an ioctl. The idea was, that‚Äôs how you‚Äôd claim a TCP port for a daemon.\n\nThe CSPF VM is extremely simple: you can push literals, constants, or data from the incoming packet onto a stack, you can compare the top two values on the stack, and you can AND, OR, and XOR the top two values. You get a few instructions to return from a filter immediately; otherwise, the filter passes a packet if the top value on the stack is zero when the program ends. This scaled‚Ä¶ sort of‚Ä¶ for rates of up to a million packets per day. You took a 3-6x performance hit for using the filter instead of native kernel IP code.\n\nFast forward 4 years, to McCanne, Van Jacobsen and tcpdump. Kernel VMs for filtering are a good idea, but CSPF is too simplistic to go fast in 1991. So, swap the stack for a pair of registers, scratch memory, and packet memory. Execute general-purpose instructions ‚Äì loads, stores, conditional jumps, and ALU operations ‚Äì over that memory; the filter ends when a RET instruction is hit, which returns the packet outcome. You‚Äôve got the Berkeley Packet Filter.\n\nIf you‚Äôre loading arbitrary programs from userland into the kernel, you‚Äôve got two problems: keeping the program from mucking up kernel memory, and keeping the program from locking up the kernel in an infinite loop. BPF mitigates the first problem by allowing programs access only to a small amount of bounds-checked memory. The latter problem BPF solves by disallowing backwards jumps: you can‚Äôt write a loop in BPF at all.\n\nThe most interesting thing about BPF isn‚Äôt the virtual machine (which, even in the kernel, is like a page or two of code; just a for loop and a switch statement). It‚Äôs tcpdump, which is a no-fooling optimizing compiler for a high-level language that compiles down to BPF. In the early 2000s, I had the pleasure of trying to extend that compiler to add demultiplexing, and can attest: it‚Äôs good code, and it isn‚Äôt simple. And you barely notice it when you run tcpdump (and Wireshark, which pulls in that compiler via libpcap).\n\nBPF and libpcap were successful (at least in the network observability domain they were designed for), and, for the next 20 years, this is pretty much the state of the art for packet filtering. Like, a year or two after BPF, you get the invention of firewalls and iptables-like filters. But those filters are boring: linear search over a predefined set of parameterized rules that selectively drop packets. Zzz.\n\nSome stuff does happen. In ‚Äò94, Mach tries to use BPF as its microkernel packet dispatcher, to route packets to userland services that each have their own TCP/IP stack. Sequentially evaluating hundreds of filters for each packet isn‚Äôt going to work, so Mach‚Äôs ‚ÄúMPF‚Äù variant of BPF (note: that paper is an actual tfile) lets you encode a lookup table into the instruction stream, so you only decode TCP or UDP once, and then dispatch from a table.\n\nMcCanne‚Äôs back in the late ‚Äò90s, with BPF+. Out with the accumulator register, in with a serious 32-bit register file. Otherwise, you have to squint to see how the BPF+ VM differs from BPF. The compiler, though, is radically different; now it‚Äôs SSA-form, like LLVM (hold that thought). BPF+ does with SSA optimization passes what MPF does with lookup tables. Then it JITs down to native code. It‚Äôs neat work, and it goes nowhere, at least, not under the name BPF+.\n\nMeanwhile, Linux things happen. To efficiently drive things like tcpdump, Linux has poached BPF from FreeBSD. Some packet access extensions get added.\n\nThen, around 2011, the Linux kernel BPF JIT lands. BPF is so simple, the JIT is actually a pretty small change.\n\nThen, a couple years later, BPF becomes eBPF. And all hell breaks loose.\n\n\n## eBPF\n\nIt‚Äôs 2014. You‚Äôre the Linux kernel. If virtually every BPF evaluation of a packet is going to happen in JIT‚Äôd 64 bit code, you might as well work from a VM that‚Äôs fast on 64-bit machines. So:\n\nAn aside about these virtual machines: I‚Äôm struck by how similar they all are ‚Äî BPF, BPF+, eBPF, throw in DTrace while you‚Äôre at it. General register file, load/store (maybe with some special memories and addressing modes, but less and less so), ALU, conditional branches, call it a day.\n\nA bunch of years ago, I was looking for the simplest instruction set I could find that GCC would compile down to, and ended up banging out an emulator for the MSP430, which ended up becoming a site called Microcorruption. Like eBPF, the whole MSP430 instruction set fits on a page of Wikipedia text. And they‚Äôre not that dissimilar! If you threw compat out the window ‚Äî which we basically did anyways ‚Äî and, I guess, made it 64 bits, you could have used MSP430 as the ‚Äúenhanced‚Äù BPF: weirdly, eBPF had essentially the same goal I did: be easy to compile down to.\n\nEmphatically: if you‚Äôre still reading and haven‚Äôt written an emulator, do it. It‚Äôs not a hard project! I wrote one for eBPF, in Rust (a language I suck at) in about a day. For a simple architecture, an emulator is just a loop that decodes instructions (just like any file format parser would) and then feeds them through a switch statement that operates on the machine‚Äôs registers (a small array) and memory (a big array). Take a whack at it! I‚Äôll post my terrible potato eBPF emulator as encouragement.\n\nThe eBPF VM bears a family resemblance to BPF, but the execution model is radically different, and terrifying: programs written in userland can now grovel through kernel memory. Ordinarily, the technical term for this facility would be ‚Äúkernel LPE vulnerability‚Äù.\n\nWhat makes this all tenable is the new eBPF verifier. Where BPF had a simple ‚Äúno backsies‚Äù rule about jumps, the kernel now does a graph traversal over the CFG to find loops and dead code. Where BPF had a fixed scratch memory, eBPF now does constraint propagation, tracking the values of registers to make sure your memory accesses are in bounds.\n\nAnd where BPF had the tcpdump compiler, eBPF has LLVM. You just write C. It‚Äôs compiled down to SSA form, optimized, emitted in a simple modern register VM, and JIT‚Äôd to x64. In other words: it‚Äôs BPF+, with the MPF idea tacked on. It‚Äôs funny reading the 90‚Äôs papers on scaling filters, with all the attention they paid to eliminating common subexpressions to merge filters. Turned out the answer all along was just to have a serious optimizing compiler do the lifting.\n\nLinux kernel developers quickly come to the same conclusion the DTrace people came to 15 years ago: if you‚Äôre going to have a compiler and a kernel-resident VM, you might as well use it for everything. So, the seccomp system call filter gets eBPF. Kprobes get eBPF. Kernel tracepoints gets eBPF. Userland tracing gets eBPF. If it‚Äôs in the Linux kernel and it‚Äôs going to be programmable (even if it shouldn‚Äôt be), it‚Äôs going to be programmed with eBPF soon. If you‚Äôre a Unix C programmer like I am, you‚Äôre kind of a pig in shit.\n\n\n## XDP\n\nIn astronomy, a revolution means a celestial object that comes full circle.‚Äú Mike Milligan\n\nRemember that packet filters weren‚Äôt originally designed as an observability tool; researchers thought they‚Äôd be what you build TCP/IP stacks out of. You couldn‚Äôt make this work when your file transfer protocol ran at 1/6th speed under a packet filter, but packet filters today are optimized and JIT‚Äôd. Why not try again?\n\nIn 2015, developers added eBPF to TC, the Linux traffic classifier system. You could now theoretically intercept a packet just after it hit the socket subsystem, make decisions about it, modify the packet, and pick an interface or bound socket to route the packet to. The kernel socket subsystem becomes programmable.\n\nA little less than a year later, we got XDP, which is eBPF running right off the driver DMA rings. JIT‚Äôd eBPF is now practically the first code that touches an incoming packet, and that eBPF code can make decisions, modify the packet, and bounce it to another interface - XDP can route packets without the TCP/IP stack seeing them at all.\n\nXDP developers are a little obsessed with the link-saturating performance you can get out of using eBPF to bypass the kernel, and that‚Äôs neat. But for us, the issue isn‚Äôt performance. It‚Äôs that there‚Äôs something we want the Linux kernel networking stack to do for us ‚Äî shuttle UDP packets to the right firecracker VM ‚Äî and a programming interface that Linux gives us to do that. Why bother keeping a daemon alive to bounce packets in and out of the kernel?\n\nFly.io users register the ports they want their apps to listen on in a simple configuration file. Those configurations are fed into distributed service discovery; our servers listen on changes and, when they occur, they update a routing map ‚Äì a simple table of addresses to actions and next-hops; the Linux bpf(2) system call lets you update these maps on the fly.\n\nA UDP packet arrives and our XDP code checks the destination address in the routing table and, if it‚Äôs the anycast address of an app listening for UDP, slaps a proxy header on the packet and shuttles it to the next-hop WireGuard interface for the closest worker.\n\nOn the worker side, we‚Äôre lucky in one direction and unlucky in the other.\n\nRight now, XDP works only for ingress packets; you can‚Äôt use XDP to intercept or alter a packet you‚Äôre sending, which we need to do to proxy replies back to the right edge. This would be a problem, except that Firecracker VMs connect to their host OS with tap(4) devices ‚Äì fake ethernet devices. Firecrackers transmitting reply packets translates to ingress events on the host tap device, so XDP works fine.\n\nThe unlucky bit is WireGuard. XDP doesn‚Äôt really work on WireGuard; it only pretends to (with the \"xdpgeneric‚Äù interface that runs in the TCP/IP stack, after socket buffers are allocated). Among the problems: WireGuard doesn‚Äôt have link-layer headers, and XDP wants it to; the discrepancy jams up the socket code if you try to pass a packet with XDP_OK. We janked our way around this with XDP_REDIRECT, and Jason Donenfeld even wrote a patch, but the XDP developers were not enthused, just about the concept of XDP running on WireGuard at all, and so we ended up implementing the worker side of this in TC BPF.\n\n\n## Some programming advice\n\nIt‚Äôs a little hard to articulate how weird it is writing eBPF code. You‚Äôre in a little wrestling match with the verifier: any memory you touch, you need to precede with an ‚Äúif‚Äù statement that rules out an out-of-bounds access; if the right conditionals are there, the verifier ‚Äúproves‚Äù your code is safe. You wonder what all the Rust fuss was about. (At some point later, you remember loops, but as I‚Äôll talk about in a bit, you can get surprisingly far without them). The verifier‚Äôs error messages are not great, in that they‚Äôre symbolic assembly dumps. So my advice about writing BPF C is, you probably forgot to initialize a struct field.\n\n(If you‚Äôre just looking to play around with this stuff, by the way, I can give you a Dockerfile that will get you a janky build environment, which is how I did my BPF development before I started using perf, which I couldn‚Äôt get working under macOS Docker).\n\nThe huge win of kernel BPF is that you‚Äôre very unlikely to crash the kernel with it. The big downside is, you‚Äôre not going to get much feedback from the TCP/IP stack, because you‚Äôre sidestepping it. I spent a lot of time fighting with iptables (my iptables debugging tip: iptables -Z resets the counters on each rule, and iptables -n -v -L prints those counters, which you can watch tick) and watching SNMP counters.\n\nI got a hugely useful tip from Julia Evans‚Äô blog, which is a treasure: there‚Äôs a ‚Äúdropwatch‚Äù subsystem in the kernel, and a userland ‚Äúdropwatch‚Äù program to monitor it. I extended Dropwatch to exclude noisy sources, lock in on specific interfaces, and select packets by size, which made it easy to isolate my test packets; Dropwatch diagnosed about half my bugs, and I recommend it.\n\nMy biggest XDP/BPF breakthrough came from switching from printk() debugging to using perf. Forget about the original purpose of perf and just think of it as a performant message passing system between the kernel and userland. printk is slow and janky, and perf is fast enough to feed raw packets through. A bunch of people have written perf-map-driven tcpdumps, and you don‚Äôt want to use mine, but here it is (and a taste of the XDP code that drives it) just so you have an idea how easy this turns out to be to build with the Cilium libraries. In development, my XDP and TC programs have trace points that snapshot packets to perf, and that‚Äôs all the debugging I‚Äôve needed since.\n\nTo sum up this up the way Hannibal Buress would: I am terrible at ending blog posts, and you can now, in a beta sort of way, deploy UDP applications on Fly.io. So, maybe give that a try. Or write an emulator or play with BPF and XDP in a Docker container; we didn‚Äôt invent that but you can give me some credit anyways."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-behind-the-scenes-fresh-logging/",
    "content": "Fly users are noticing faster, more reliable logs. Find out what happened behind the scenes to make that happen in this article.\n\nSince Fly launched, we‚Äôve been collecting and managing logs for all the applications running on the Fly platform. It‚Äôs a critical but often rarely noted function of the platform. When you type flyctl logs, behind the scenes, there is a lot of computing power and storage being brought to bear. Over the last weeks, and transparently to users, the entire logging platform has been replaced with a whole new logging platform and a new approach to working with logs. We talked to the person who drove the change, Steve Berryman.\n\nDj: How did this project begin?\n\nSteve: The previous logging system was built around seven fairly large, centralized Graylog servers. Even with all of them running, the volume of logs we got at various times of the day couldn‚Äôt be processed fast enough, and that meant things were dropped.\n\nDj: What kind of volume are we talking about?\n\nSteve: Between 20,000 and 30,000 logs per second.\n\nIn theory, we could have just expanded the servers and added more power to manage things. The way logs get to customers, though, was by polling the Graylog API and polling APIs isn‚Äôt really the nicest way to work with an API. But then there wasn‚Äôt any other way to get that information out other than through the API.\n\nDj: What were the options?\n\nSteve: We considered Kafka and other message streaming services, but they would have been another big tool in the chain. It was pointed out to me by Jerome that there was a new Log processing tool called Vector. It‚Äôs written in Rust and it‚Äôs very efficient. Like Logstash, it does all the capture, process, and transform of logs. We initially thought of using that to send things to Graylog, with Vector running on each server, but we found that Vector didn‚Äôt support the various Graylog protocols. It was then we had an idea.\n\nDj: Which was?\n\nSteve: Why even bother sending the logs to Graylog if we‚Äôre processing the logs in Vector at each server. Send the logs straight to Elasticsearch and take out the Graylog middleman. All Elasticsearch has to do then is index the logs and retrieve them.\n\nVector runs on every server and logs go straight from journald, or other applications, into Vector. There it parses them and runs a number of transforms on them. For example, this includes taking fields out from journald logs where we aren‚Äôt interested in them, some regex parsing, transforming the names of things into slightly nicer things for the new schema. It then ships the results to Elasticsearch which happily takes them in.\n\nDj: A new schema?\n\nSteve: Yes, although people can‚Äôt see it externally, I decided to move us to using ECS, the Elasticsearch Common Schema. It‚Äôs a general log schema they‚Äôve defined for various purposes, with a lot of common fields already defined - there‚Äôs file fields, log fields, network source and destination fields, geo fields, HTTP fields, TLS fields and more. It also lets us add our own fields, so we have fields for Fly app names, Fly alloc id, Fly regions and other Fly-related things.\n\nThe good part is that, with all the apps feeding logs in according to the schema, it makes searching across apps much easier. Searching for say a source IP address across different apps may have meant searching for ‚Äúsrc.IP‚Äù, ‚Äúsource-IP‚Äù, ‚ÄúIP.source‚Äù and any other variation. With a common schema, we know if there is a source IP address, it‚Äôll be in the source field. It‚Äôs nice to know what you‚Äôre looking for regardless of application.\n\nObviously, not everything will follow this schema, but then as soon as you find something you want to parse or transform, you can add a bit of config into the config management system for Vector. The Vector configuration language is pretty simple, it‚Äôs a bunch of TOML that defines sources, transforms, and sinks. Logs come in through the sources. Transforms then modify the log‚Äôs structure, adding or removing fields, or making other changes. The result is then sent on to one or more sinks. The important sinks for us is Elasticsearch.\n\nDeploy those Vector config changes and the configuration management system will distribute to all the servers and from then on, all the logs will reflect that change. That‚Äôs a bit more work than before with Graylog.\n\nDj: Why‚Äôs that?\n\nSteve: Graylog‚Äôs rules and transformations are centralized on Graylog servers, so it was one place to change things. I like Graylog a lot, but it is a big, heavy, Java, enterprise app that does a lot and where centralization makes sense. But it also centralizes the work needed to be performed on logs.\n\nWith Vector, we‚Äôve distributed that work out to all the servers where it barely registers as load and we get to manage that with our own configuration system. We can also add the hardware that was servicing Graylog to the Elasticsearch fleet to make Elasticsearch perform better.\n\nAs an aside, one cool feature of Vector is that it allows us to unit test configurations locally. Along with the validate command for checking configurations, it means we have the tools to efficiently check configurations before deployment, giving us a lot more confidence.\n\nDj: How long did this take?\n\nSteve: In all, about two weeks alongside other work. The actual implementation of getting Vector onto servers and feeding Elasticsearch didn‚Äôt take long at all. The bulk of the work was in the snagging, getting all the little issues handled, from configuring mapping and schemas on Elasticsearch and getting the encryption support working right, to fixing up various fields‚Äô content and making it all run smoothly. Also, the Vector Discord channel was very helpful too with Timber.io devs participating in the chat.\n\nDj: So we‚Äôve got a distributed log collection and processing platform with Vector. What about getting that data to users?\n\nSteve: Currently, we have the API servers picking up the data from Elasticsearch using the same polling mechanism as before. Now, though, we can optimize that and make it more searchable and flexible.\n\n\n## How `flyctl` Displays Logs\n\nWe normally tail logs by polling a cursor against a REST endpoint. When a deployment fails, we also want to show logs for the failed allocations. So we query our GraphQL API for the failed allocations, and for each one of them, the GraphQL resolver gets the last N lines of log entries with that allocation‚Äôs id. The problem was with the old logging system, there was a good chance the logs had not been processed when this query was made. With the new system, it‚Äôs fast enough that the logs are already available.\n\nOne of the cool things we‚Äôll be able to do with the new logging platform - we‚Äôve not done it yet - is being able to point logs at different log service endpoints. Eventually, we hope to be able to send logs to Papertrail, Honeycomb, Kafka, possibly anything with a Vector sink component.\n\nDj: Beyond that, any specific plans?\n\nSteve: Probably incorporating internal metrics into the platform so we can see how efficiently we are handing logs and optimizing all our feeds into Vector, especially with Firecracker logs.\n\nWe are better set up for the future with this new architecture, so who knows what‚Äôs next.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/stuff-your-pi-hole-from-anywhere/",
    "content": "How do you take the ad-scrubbing Pi-Hole and turn it into a globally available app? Quickly and simply with Fly and in this article, we‚Äôll show you how.\n\nA Pi-Hole could be the hero of your web connection. It blocks advertising at the roots; the DNS roots that is. By setting up your own Pi-Hole, you can tip all your local network‚Äôs requests for known advertising domains down it.\n\n\n## What is Pi-Hole?\n\nThink of Pi-Hole as a firewall for popups, overlays, banners and other mental focus-stealing and bandwidth-eating advertising. Unlike a firewall which filters through all your traffic, Pi-Hole does its work by acting as your DNS server with a difference, a database of advertising and tracking hosts and domains.\n\nWhen a machine queries the Pi-Hole DNS for something that matches one of the entries in the database, Pi-Hole politely doesn‚Äôt resolve it and returns a dummy address. Ads that are served from ad providers just can‚Äôt load, while embedded ads are blocked from downloading their assets. Even dynamically inserted ads get caught by this technique.\n\nOnce you start stuffing things in your Pi-Hole, you‚Äôll wish you could take it with you everywhere.\n\n\n## Pi-Hole at Home\n\nThe typical Pi-Hole is running on the eponymous single-board computer, tucked away somewhere on the home network. There are a lot of good reasons to do that. For example, a Pi-Hole can also be a DHCP server for your network. That means it can automatically give out its address for DNS resolution to all your laptops, phones, aging desktops, smart TVs, and Amazon surveillance devices, practically configuring all your home devices to use it.\n\nBut then that makes it somewhat hard to take on the road with you, even if you are the most organized road warrior to ride the roads. You stop at a coffee shop and then the next hour is spent connecting the Pi-Hole to the diner Wi-Fi and reconfiguring it for its new environment and, well, you aren‚Äôt going to get a lot of browsing done.\n\nOf course, you only need to point your DNS at the Pi-Hole and if you have a server at home which is running Pi-Hole, you could just point your mobile devices at that and vanish those ads. To make that easier, the diggers of the Pi-Hole have packaged it up as a Docker image that you can run on a machine on the edge of your network. There‚Äôs also a guide for configuring it with a VPN to make things easier.\n\n\n## Pi-Hole The Planet!\n\nRemember traveling? We do, and we also remember that the further you go from home, the further you are from your home server. And that means latency. And we hate latency at Fly. Physics dictates though that distance means latency and given how many DNS lookups a modern browser or app could be doing, that all mounts up.\n\nAh, you say, I have a cunning plan; I will run my Pi-Hole in a datacenter somewhere on a singular server. And so you might, but all you are changing then is that the latency depends on the distance between you and the datacenter, rather than the distance between you and your home.\n\n\n## What we need is a Pi-Hole that is everywhere.\n\nA global Pi-Hole to keep you free from adverts and your connection free from the bandwidth hogs would be wonderful but how could you build it?\n\nWell, imagine you had a global edge network connected to dozens of data centers around the world. You could deploy Pi-Hole as an edge application to that network and get the benefits of automatically low latency and global availability. Fly‚Äôs network is a global edge network with data centers around the world‚Ä¶ so we built a global Pi-Hole using it.\n\nSeasoned Fly users will know that DNS uses UDP, a sister protocol of TCP and a protocol which has not been supported on Fly to date. Well, don‚Äôt tell anyone, but we have UDP support in beta so building a global Pi-Hole was a great way to give this a workout.\n\n\n## From Pi-Hole to Fli-Hole\n\nGetting Pi-Hole ready for global filtering doesn‚Äôt require any changes to Pi-Hole itself. We even use the official Pi-Hole Docker image for Fly deployment. Here‚Äôs our Dockerfile:\n\nIt simply takes that image and sets environment variables for which interface to listen on, and to allow DNSMasq (a component of Pi-Hole) to listen on all subnets for DNS traffic.\n\nThe other part of the configuration is the fly.toml file which defines how our application should be deployed to the Fly platform. You generate the core of this file with the command fly init and when asked, let it generate a name, use the Dockerfile builder and set the internal port to 80.\n\nThere‚Äôs just one final addition needed, an extra [[services]] section that looks like this:\n\nThis routes external UDP traffic on port 53 to the application‚Äôs internal port 53.\n\nThat‚Äôs nearly all the configuration done. The last step is to set the password on Pi-Hole‚Äôs web interface. This is picked up from an environment variable WEBPASSWORD. Fly‚Äôs secrets are passed to running applications as environment variables, so the secure way to set this is to run:\n\nObviously, substituting in your own password as needed.\n\nThe Fli-Hole is now ready to deploy. Run:\n\nAnd Fly will place an instance of Fli-Hole in a region close to where you are running the commands. If you want to check out the App dashboard, run:\n\nWhich will open the dashboard. From there you can log in, using the password you set in secrets, to look at the other settings and logs gathered by Pi-Hole.\n\n\n## Going Regional\n\nNow, you could deploy Fli-Hole to every Fly region on the planet, but that‚Äôs not the smart way of doing it. Say you were traveling, in an entirely hypothetical sense of course, to Europe. All you need to do is run:\n\nAnd that would add the Frankfurt data center and associated region to the pool and start routing nearby traffic through there. Now you are ready to go on a European vacation with your Pi-Hole. If you were in the UK, for example, then the edge network would send your requests to Fra, via the UK‚Äôs edge nodes, for the fastest response.\n\nIt‚Äôs worth pointing out these instances of Pi-Hole have no persistence or sharing of settings. Each one is an independent Pi-Hole. It‚Äôs a strength and an easily mitigated weakness; just update the Dockerfile to change settings and configuration files and redeploy the application to get permanent global updates.\n\n\n## Through the Fli-Hole\n\nWhat we‚Äôve shown here is how quick it can be to configure an application to run on the Fly platform, from taking a Docker image off the shelf, connecting it to the web and UDP, setting configuration and secrets, and then sending it out into the world. You may not need a Pi-Hole everywhere, but your next big app might need to be everywhere at the drop of a hat."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/maps-apps-and-tracks/",
    "content": "How do you make an online map that tracks a Marathon runner from a mobile device? And how do you deploy that to the world using Fly? Well, we just did that and here‚Äôs the whole story.\n\nMarathons, long and hard. Personally, I wouldn‚Äôt be caught doing one, but I‚Äôm always up to help someone who is, especially when I can use a bit of Fly to do it. Steve, our global infrastructure whiz, asked if I could think of a good way to track Mark, a friend of his, as he did his own personal London Marathon, the Markathon\n\nYou see, we‚Äôre not always hammering away at the keyboards at Fly fulfilling our mission to deploy apps globally with the lowest latency. Sometimes we‚Äôre hammering away on keyboards at Fly on side projects. And this was a fun one.\n\n\n## Map Making\n\nLet‚Äôs start with the route. This is likely to be the fiddliest bit of getting an app like this set up as you‚Äôll need to draw out your exact route. There are apps like OnTheGoMap.com which will let you create the route. From there Steve exported Mark‚Äôs route to GPX format. Then it was over to Google Maps, specifically the Your Places/My Maps view, which could load in the GPX format data. It was there that Steve added mile markers in and exported the file as KML. We now had data to work with, stored as Markathon.kml - if you want to follow along, you‚Äôll find all the code and assets in the Markathon Github Repository.\n\nFor displaying the map, we went with Leaflet.js, the rightly popular interactive JavaScript map viewer. While Leaflet has a range of plugins, we found it easiest to work with leaflet-kml, a branch of the official KML plugin, nicely isolated for better maintenance. All we needed to do was copy the L.KML.js file into our mappages directory.\n\nNext up, we had to pull this all into a web page, index.html. For this, we took our cues from the leaflet-kml package‚Äôs README example. It‚Äôs a simple minimal map displayer:\n\nThe only change is the name of the asset we‚Äôre using. With that in place, we‚Äôre ready to show some maps. Head back to the markathon directory:\n\nNow, we need to serve that up locally. I recommend serve for that. It‚Äôs a neat little node server you can install with npm install -g serve. Once installed, run serve mappages and head to localhost:5000 in your browser and you should see a map. It‚Äôll appear to be covered with broken image icons though because we have no images for those mile-markers. Looking in the KML file, we find lots of \u003chref\u003eimages/icon-1.webp\u003c/href\u003e defining those mile-marker images, so we found a set and after renaming them appropriately, popped them in the images directory.\n\n\n## Tracking Down Mark\n\nThere‚Äôs an elephant in the room at this point. We haven‚Äôt talked about how we track Mark as he does his run. We really don‚Äôt want to get into writing our own mobile app; ideally we want nothing to do with that. Well, the good news is that OwnTracks exists.\n\nOwnTracks is an open source app you can install on iOS or Android and it can be configured to send your location to an MQTT server, or as in our case, an HTTP endpoint. You can read about configuring OwnTracks HTTP settings in its booklet. We‚Äôll come back to that though because right now, we need a server.\n\n\n## Services Noted\n\nWhile the map view is static, we need to be able to collect Mark‚Äôs location data and display it to people viewing the map. We‚Äôre using Node and Express here, so first, we have to set that up. Run npm init and hit return for the default for most answers except for the entry point, which you should set to server.js. Then run:\n\nWe can now create server.js . There‚Äôs a bit of preamble in the server.js to bring those packages in:\n\nOn to the business of the app. We need to set up an endpoint for the OwnTracks app to POST location data to. This is the /log/ endpoint:\n\nIn this version, for simplicity, we‚Äôre going to keep Mark‚Äôs latitude and longitude in memory; in sharedlat and sharedlon. Note that when we need it, we can use a Redis cache on Fly which can not only save the data but also let us share it between instances of our app in case thousands start to view.\n\nWe‚Äôre only tracking one runner in this app so the matchtopic is hard coded to Mark. When Mark sets up his OwnTracks, that matchtopic is his id to post his location. When someone (we assume Mark, but note we haven‚Äôt turned on any authentication) posts to the /log endpoint, the posted JSON is parsed. If the type is ‚Äúlocation‚Äù and the topic is the same as matchtopic, then the lat and lon are extracted from the POST body and stored in our sharedlat and sharedlon variables. Oh yes, and do, do respond to that POST with something.\n\nHow do other users find out where Mark is? Simply by asking where:\n\nThere are two other tasks for this server, serving our mappages directory up, and listening on our selected port:\n\nOur directory now looks like this:\n\n\n## Services Rendered\n\nNow we need to go back to the mappages directory and enable the map to get, and update with, the stored coordinates in the server. Most of this work is done in a mark.js file:\n\nThis is a simple function that queries the /where endpoint, parses the JSON response and, first time through, puts a marker on the map. Subsequent queries will move that marker on the map. We now need to hook in that function so it gets run regularly. First, add it to the \u003cscript\u003e includes at the start of index.html:\n\nThen, after the fetch...then chain further down add:\n\nNow the browser will query every 30 seconds for a location.\n\nLet‚Äôs test this. First run the app locally with npm start and it should come up on port 3000. Navigate to http://localhost:3000/ to view the map.\n\nNow pop this into a test-local.sh file:\n\nThis shell script makes a JSON object which will emulate the OwnTracks app calling into the server. Run sh ./test-local.sh and if it all works, you should see a marker pop up around the start marker within a minute or so.\n\n\n## Fly To The Finish Line\n\nTo make this into a Fly app, first download and install the Fly CLI tool flyctl and run fly auth signup to get a Fly account.\n\nNow we can initialize the app to run on Fly by running fly init in the markathon directory.\n\nHit return to get a generated app name, hit return to use your personal organization and then select node to use Fly‚Äôs node builtin builder. Finally hit return to use the default port. Fly will now write out a configuration file for the app.\n\nOh, yes, and then run fly deploy to put it online into a region near you. Then run fly open to view the app in your browser. You‚Äôll have to set up OwnTracks to send its location to this server‚Äôs /log endpoint, at which point you should be able to see yourself on the map.\n\nIf you can‚Äôt set up OwnTracks to test it, copy your test-local.sh to test-remote.sh and change the last line to:\n\nThat automatically gets the hostname of your Fly app and posts the mock location data up.\n\nWe‚Äôve set this up for London, but if you create a route for anywhere in the world, the map viewer should automatically focus on the new route.\n\n\n## Next‚Ä¶\n\nThere‚Äôs plenty that could be added to this app: basic authentication for the HTTP post from OwnTracks, Redis backing support, or more ambitiously, support for multiple runners.\n\nWe‚Äôve been able to concentrate on building our app instead of working out how to get it into the cloud. That meant that Mark could run his Marathon and raise money for his chosen charity. And we all got to know how to make a tracking map that runs on Fly."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/the-september-fly-changelog-new-names-and-easier-updates/",
    "content": "September‚Äôs changes have been all about making the Fly CLI easier to install, update, and use on every platform including Windows 10. And we‚Äôve added some new ways to build and deploy websites. Read on for details.\n\nSome common requests are addressed this month. One is ‚Äòcan you let us type ‚Äúfly‚Äù instead of ‚Äúflyctl‚Äù?‚Äô, and it is a shorter command, that is true. Another is can you ‚Äòmake updating flyctl easier?‚Äô, something we are more than happy to do. And in September, we did both.\n\n\n## Fly and Flyctl\n\nWe‚Äôve modified the installer for the Fly CLI so that while it installs itself as flyctl it also sets up a symbolic link to the fly command name. Once you are up to date with Flyctl, you‚Äôll be able to use fly as your preferred command. We‚Äôre taking this carefully as this is the start of a migration which will end with fly being the actual name of the CLI app.\n\n\n## Easier Updating\n\nThe Fly CLI regularly checks in to see if there‚Äôs a new version of itself available. We‚Äôve previously enhanced this feature so that it could show people the command they needed to run to update. Well, in our September updates, we‚Äôve gone one better. Now, when there is a new version available, all you need to do is run fly version update and the Fly CLI will take care of the rest.\n\n\n## Extra Builtin Builders\n\nTwo new static builders for creating static web sites quickly ‚Äî hugo-static and staticplus ‚Äî have been added. The former runs a Hugo build and then deploys the results as a static website. The latter added HTTPS redirection for sites with a custom domain, but we‚Äôll be retiring that soon, thanks to a combination of a change being accepted upstream and a big new feature coming for Fly builtins in a future release.\n\n\n## Windows 10 Improvements\n\nBetter Windows installation and updating using PowerShell have been implemented. We‚Äôd love to hear what you think if you use Fly and Windows 10.\n\n\n## Init Extended\n\nThere are also two new ways to initialize an app with fly init. When you really want to always overwrite the existing configuration, --overwrite will do that job for you. And when you want to do the exact opposite ‚Äî create a new app but not rewrite the configuration ‚Äî then --nowrite is the option for you.\n\n\n## Other Changes\n\nUpdated and new commands for the upcoming DNS+domains feature have been incorporated in this release too; read the feature preview on the Fly Community site for more about that.\n\nThis is the Fly Changelog where we list all significant changes to the Fly platform, tooling, and websites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\n\n## 30th September\n\nflyctl: Version 0.0.144 released\n\n\n## 17th September\n\nflyctl: Version 0.0.143 released\n\n\n## 15th September\n\nflyctl: Version 0.0.142 released\n\n\n## 14th September\n\nflyctl: Version 0.0.141 released\n\n\n## 27th August\n\nflyctl: Version 0.0.140 released"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/using-heroku-postgres-from-a-fly-app/",
    "content": "In a previous article, we showed how simple migrating a database-using application. But what we didn‚Äôt mention was that you don‚Äôt have to migrate an application from Heroku to Fly to use Heroku‚Äôs databases. You can use Heroku databases with a native Fly application.\n\nHeroku supports applications with no apps in them and only add-ons, like Postgres, as a way of providing those services to other applications. As we showed, you can access Heroku Postgres from Fly so it makes it a useful way to get yourself a database for some storage.\n\nSo let‚Äôs step through the process of what you need to do, and what you need to look out for.\n\n\n## Getting Heroku Prepared\n\nFirst, sign up for Heroku and get your free account there. For this example we‚Äôll be using their Hobby plan so there‚Äôs no expense involved.\n\nNow, you should arrive, once logged in to Heroku, at the Heroku dashboard. Select New on the right of the dashboard and New App in the dropdown menu. On the next screen, give your application a name and select whether you want the database to work out of the US or Europe. Then click Create App and arrive at the initial application setup screen. The details shown won‚Äôt be of much relevance to you for this task. Select the Overview tab to get a higher-level view of the app.\n\nYou should see entries about the configuration of the app, including at the top, Installed add-ons. Select Configure add-ons in that section and you‚Äôll be taken to a page which doesn‚Äôt obviously say it‚Äôs for add-ons. That‚Äôs underneath the top section for Dynos. The add-ons section has a text field where you can type add-on names. Type postgres into there and select Heroku Postgres. You should see a dialog inviting you to choose a plan - stay on Hobby Dev for a 1GB database capable of holding up to 10,000 rows to keep things free - then click Provision.\n\n\n## Getting Your Credentials\n\nAnd now, there‚Äôs a Heroku Postgres database listed in your add-ons. Click on the database name in the add-ons list to drill down into the database‚Äôs configuration. Select Settings and then View Credentials. You‚Äôll want to take a note of what is now displayed, but the most important line in the credentials is the URI line. It‚Äôs a long one, with a long hostname, database, username, and password, but it contains all you need to connect to the new database. This is the value that turns up in Heroku apps as DATABASE_URL but for our purposes, we‚Äôre going to have to manually transfer it to our app.\n\nAlso, pay attention to the warning: Please note that these credentials are not permanent. Heroku rotates credentials periodically and updates applications where this database is attached.. You won‚Äôt get any warning that credentials have changed, but if you do have an issue in the future, make checking the credentials your first port of call.\n\n\n## Preparing To Fly\n\nFor this example, we‚Äôre going to use Node and Sequelize to create a simple Postgres-backed REST API server for storing truefacts. The part we‚Äôre most interested in is setting up the connection. Here‚Äôs the opening of the main code:\n\nWe have to make some modifications to the DATABASE_URL which we got from Heroku. To do those changes as reliably as possible, we use pg-connection-string to parse the URL into its component parts (in terms of Postgres).\n\nOnce parsed, we can create the Sequelize connection, and add in the changes needed to connect to Heroku:\n\nAs you can see, we pass over the database, user, password, and host untouched to the Sequelize constructor. What we do add is in the options object. That dialectOptions value requires SSL (because external access to Heroku demands SSL be turned on) and then turns off the rejection of self-signed certificates on the server‚Äôs certificate chain, an issue which Heroku‚Äôs Postgres has.\n\nThe rest of the example application is a simple REST API, where you add true facts about something by POSTing a JSON object to one endpoint and retrieving those facts as JSON objects. You‚Äôll find the code in the truefacts repository in fly-examples.\n\nThat‚Äôs pretty much all you need to know to get a connection to Fly, apart from how you pass your database credentials to the application. For this, you need to set a secret in the form:\n\nNow you are ready to fly deploy your application. It‚Äôll connect up to the backend Postgres on Heroku. Remember that‚Äôs limited to 20 connections unless you upgrade to a paid plan.\n\n\n## Wrapping up\n\nWe‚Äôve shown how to connect to Heroku‚Äôs Postgres without a Heroku application being migrated. Using Heroku‚Äôs services is an option for users who need a database option right now, and with Heroku‚Äôs data centers in the US and Europe, the database won‚Äôt be too far away, while your Fly application can be as close as possible to the user.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/migrating-heroku-database-apps-to-fly/",
    "content": "How do you handle databases when you migrate an application from Heroku to Fly? Migrating Heroku apps to Fly gives your app a real boost in performance using Turboku, but how do you move the database connection of an app over?\n\nI was recently asked how the database migration in the original Turboku demo worked. For that original demo, the migration was practically automatic. We already knew that you could access a Heroku database from outside Heroku. Since then, Changes in how Heroku and the Postgres driver handle things have meant you need to be a little more proactive with your database connecting code. So, if we‚Äôre setting out to make an app that migrates without changes to Fly, what do we need to do?\n\n\n## Start With The DATABASE_URL\n\nWhen you attach a Postgres addon to your Heroku app, it also creates a URL which contains everything you need to connect to the database. You can get the value of DATABASE_URL from the environment and that has all the credentials you need to connect to the Postgres database. It also contains the host, port and database name to connect to. For most situations, it can be used without modification. The demo uses Massivejs here as a layer on top of the node-postures (aka pg) package and connecting looks something like this:\n\nNow, if we wanted to connect to Heroku Postgres from outside Heroku, the number one requirement is that you switch to SSL. Now, you could do that by appending a string with the required change to the end of the connection string. But experience tells me that connection strings can change and change in sometimes unpredictable ways. So‚Ä¶\n\n\n## Turning on SSL\n\nThe first thing we need to do is to parse the connection string. To handle that, we‚Äôll use the pg-connection-string package and we‚Äôll extract the parse function:\n\nThis is the same package the pg library uses to parse connection strings so you shouldn‚Äôt have any compatibility issues with it. We can now take the connection string and parse it:\n\nAnd now we can set the ssl field:\n\nAh! You may say, all you need to do is set ssl to true. And in a perfect world, that would be correct. Firstly, any value in the ssl setting equates to ssl being set to true. Secondly, we need to tweak the SSL configuration because Heroku Postgres has servers with a self-signed certificate in their chain of server certificates. If we just had SSL turned on, the client driver would try and verify the server‚Äôs identity and throw an error when it saw that self-signed certificate. You can read more about this in this github issue.\n\nThe fix, now, is to set the sslmode to require which requires SSL to be enabled and then rejectUnauthorized to false to stop that certificate verification check. The best part is we don‚Äôt need to do anything else to connect; we can use our parsed connector rather than the connection string on most connection calls to Postgres:\n\n\n## Putting It Together\n\nPutting all this together, we get this code:\n\nThere‚Äôs a little extra code to help out when testing locally against a local Postgres - don‚Äôt define DATABASE_URL and it‚Äôll fall back to a non-SSL localhost connection. In our example code, flydictionary, the rest of the code a simple dictionary with the ability to search or add words and save them in the database.\n\n\n## Pushing to Heroku\n\nIf you want to install this app, grab the flydictionary code, log in to Heroku and deploy the app with heroku create then push the app up to Heroku with git push heroku master.\n\nThe app will start running but stop immediately as there‚Äôs no database for it to take to - Use:\n\nThis will create a Postgres database and attach it to your app. Now you can run heroku open to open a browser onto your newly created app. Add some words to the dictionary.\n\n\n## Coming to Fly\n\nBringing Heroku web apps to Fly is an uncomplicated process thanks to the specialized Heroku migration support.\n\nAll you need to do is go to the https://fly.io/heroku. Log in to Heroku there. That will show the available apps on the page and you can select the app you just created on Heroku.\n\nThen you press the Turboku! button and the app will be automatically migrated over to Fly. That includes migrating the Heroku DATABASE_URL environment variable which will be turned into a Fly secret.\n\nThings to remember with a Heroku database connection: Heroku only has two regions: the US and Europe. Database performance will be a function of how close your selected Fly regions are to your Heroku database region. Consider caching within Fly regions if performance is mission-critical.\n\n\n## Fly Closer\n\nThanks to the connection changes we made to the application, it‚Äôll just start running on Fly and we can treat it like other Fly applications, adding and removing regions, scaling up, scaling down and more, while getting the benefit of being closer to the user. And remember that with Turboku, apps migrated this way are automatically updated when changes are pushed to Heroku.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/always-be-connecting-with-https/",
    "content": "Making sure your users always connect to the HTTPS secured version of your site is a big concern these days. With search engines marking down non-HTTPS sites and users adopting tools to push their web browsing to the more secure option, it is something that developers and site owners need to take seriously.\n\nAt Fly, you‚Äôll have already discovered that when you create an app, it appears as http://appname.fly.dev and if you connect there, your browser switches automatically, by redirection, to https://appname.fly.dev. That‚Äôs how we do it for the fly.dev domain.\n\nThings are different when you bring your own certificate along and attach it to your app. There‚Äôs no automatically activated HTTPS upgrading because we understand people like to have the freedom to choose how their connections behave.\n\nSo how do you make a connection upgrade to HTTPS with your own custom domain‚Ä¶\n\n\n## A Fly Connection‚Äôs Life\n\nWhen an application connects to a Fly app, it looks up the domain, gets an IP address back and connects to the IP address. With Fly, those IP addresses are AnyCast IP addresses and the connection will be routed to the nearest edge of the Fly network.\n\nThese Fly network edges are made up of proxies which handle the incoming connection. When you define handlers for an app (in fly.toml), it‚Äôs at this edge where the handlers step in to manage the ‚Äúhttp‚Äù or ‚Äútls‚Äù components of the connect.\n\nOnce the edge proxy has located the nearest server and datacenter where your app has an instance running, the proxied connection is routed to that server. This stage of the connections journey take place over encrypted networking so the connection is made unencrypted.\n\nBoth https and http connections are sent to the internal port of the app. The app then responds and the proxy handles the response appropriately. If you wanted to upgrade all your HTTP connections to HTTPS, the appropriate response would be to redirect the client to the HTTPS version of the URL.\n\n\n## Detecting HTTP Connections\n\nYou may wonder ‚ÄúBut if all connections are flattened out to unencrypted HTTP, how do we spot actual HTTP connections?‚Äù. That‚Äôs where being routed through a proxy comes into it. Proxies can add information to the connection about where the original connection came from.\n\nThese usually appear in the HTTP header and make up part of what we call the Fly runtime environment. The one header we are interested in is X-Forwarded-Proto. It contains the protocol the incoming connection used - ‚Äòhttp‚Äô or ‚Äòhttps‚Äô.\n\nSo, at its simplest, if our incoming connection to a has X-Forwarded-Proto: http then we Redirect (HTTP Status 301) to the same URL but with https in its header. All connections are then automatically upgraded.\n\n\n## Practical Upgrading - Go\n\nHere‚Äôs a recent example of doing this in practice. We‚Äôve been using an excellent lightweight Go server (PierreZ‚Äôs goStatic). It deals entirely in http connections which works great with the Fly architecture but we wanted to get it to upgrade non-‚Äúfly.dev‚Äù connections. This is a shorter version of the Go code we added\n\n\n## Practical Upgrading - Node (and Express)\n\nFor Node (and Express), the same effect can be achieved with a little bit of Express middleware added to the stack:\n\n\n## Practical Upgrading - With Other Apps\n\nIn-App Upgrading like this is a great way to always ensure that where-ever your app is deployed. You can allow other applications to do the redirection work for you, like Nginx. Mix in something like:\n\ninto your nginx.conf and you‚Äôll be upgrading your connections in the same way. This would, for example, but useful in a custom domain proxy, ensuring that all the customer‚Äôs users were getting a secure connection.\n\n\n## Beyond manual Upgrading\n\nEven if you are not on Fly, you can make use of this technique, though rather than checking the request‚Äôs headers, you‚Äôll need to check the request itself and the URL‚Äôs schema to identify if it‚Äôs an HTTP connection.\n\nBack on Fly, we are looking at ways that we can also automate this process, without forcing a default behavior on all our users. We‚Äôll get back to you on that when it‚Äôs ready; until then we think these redirection tips will help keep your apps securely connected.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/more-fly-answers-to-questions/",
    "content": "We get asked questions about Fly in a lot of places on the web which we answer. But, not everyone is everywhere on the web, so with Fly Answers Questions, we bring those answers to you. If you have questions about Fly, why not ask @flydotio on Twitter, or drop a query in the Fly Community.\n\nQ: Is it ok to run non-HTTP apps on Fly? From Pier via community.fly.io\n\nA: It is, with one caveat: your applications need to have a network service of some kind. Why is that? Well, when your app deploys, we run health checks to ensure you can connect to it. They are, by default, TCP connection checks but we do have an option for full HTTP checks. Anyway, unless there is a network service that can be connected to, the health checks will fail and Fly will kill off the app for being apparently faulty.\n\nWe do this ourselves for some of our internal services. When we run our Buildkite agents, we use the fact that they export Prometheus metrics on port 8080 and use that port for the health checks to look at.\n\nAnd no, this isn‚Äôt how things are going to stay. We have support for Worker processes on the way.\n\nQ: What IP Ranges is Fly on? From David via community.fly.io\n\nA: This is quite a regular question and the question itself comes from the strategy of protecting your database or other servers from unauthorized access by clear-listing the IP addresses of servers that will be legitimately connecting. This also blocklists the rest of the internet.\n\nThe first answer is ‚Äúit‚Äôs complicated‚Äù. We do have a set of registered ranges in AS40509 but they may not be the addresses that are seen by servers being connected to. Instead, it‚Äôs likely they see the server‚Äôs IP address which isn‚Äôt assigned within that AS.\n\nWe have considered publishing our IP addresses, but we are literally adding new servers every day. With that rate of change, it is likely there would be a gap between an updated list being published and various other systems syncing with the up-to-date list.\n\nAs that‚Äôs the case, our second answer is we‚Äôd suggest that you regard Fly IP addresses as subject to change in the context of security-related features. That said, you can always use improved authentication between clients and servers with the exchange of client certificates during a TLS connection to help protect the connection.\n\nThere‚Äôs a third answer for the future though. Everything above is related to IPV4. With IPV6, we are able to generate addresses per application through our API, so in the future, it would be possible to use those addresses to control an IPV6-capable firewall‚Äôs clear-list. This, and other solutions, are all being worked on at Fly.\n\nQ: How can I rename an App on Fly? From Dan via community.fly.io\n\nA: We prefer people to use the generated app names on Fly. Apart from anything else, we make it easy to attach a custom domain to your application, making the issue of having a particular name mostly moot. We do understand, though, that people may want a specific name. We don‚Äôt have a way to do that yet but there are plans in the works to provide a solution.\n\nUntil then, we do have a workaround, but you will lose your app‚Äôs history, secrets, certificates, hostnames, and deployment configuration, so it‚Äôs entirely at your own risk:\n\nDon‚Äôt forget to run flyctl destroy original-app-name to tidy up. Then configure the new app‚Äôs secrets, certificates, hostnames, and deployment configuration including the scaling and regions it was mapped to.\n\n\n## Community.fly.io\n\nIf you follow the links to community.fly.io, you will see that our new community site is a great place to ask questions, look for hints and tips and get your Fly apps running at their best. If you have a Fly account, sign in today as we‚Äôve enabled single sign-on between Fly and the community site making it easier than ever to participate."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/flyctl-builtins-the-fly-changelog-for-august/",
    "content": "Super-simple builtin builders and smart certificates creation - it‚Äôs all in the latest flyctl (v0.0.139) available now for your command line. Find out more about it in the Changelog.\n\nFor the latest version of flyctl, we‚Äôve focussed on making your life fast and simple. From getting your first deployment up and running to setting up a host‚Äôs certificate.\n\n\n## Builtins\n\nFirst up, we‚Äôve got the new ‚Äúbuiltin‚Äù builders which you can select when you init your App. We have builtin builders for Node, Ruby, Deno and Go and there‚Äôs also a static web server available. No need for a Dockerfile - just init and deploy.\n\nYou can find out more at the command line with flyctl builtins list which will give you information on all of the builtins, and flyctl builtins show \u003cbuiltin-name\u003e which will give you all the details of the named builtin, including the virtual Dockerfile that the builtins use.\n\nWith builtins all you need to do is:\n\nYou can see full examples for Go, Node, Ruby, Deno and a static web server in the documentation.\n\n\n## Images\n\nIn previous versions of flyctl, we have been able to deploy an image from the command line. In this version, you can now use the image builder (or --image flag on flyctl init) to select a public Docker image to be published to Fly. This persists the selected image in a fly.toml file.\n\nBy using the fly.toml file, it also lets you configure the ports and health checks for the image. Best of all, it lets you save that information into your git repository so you can perform repeatable builds with images.\n\n\n## Backup Regions\n\nWhen, for whatever reason, an App instance is unable to deploy in a particular region, Fly will look to deploying it in a backup region. This is a longstanding behavior of Fly, but flyctl has been not been good in communicating that that is what is happening, leading to confusion over why an App is deploying in a region but not in the region list.\n\nNot anymore! flyctl regions list will now show the regions and the associated backup regions that an app instance may appear in. Also, flyctl status will display (b) next to any region which is not in the region pool and is therefore in a backup region. This should give everyone a better view of where their app is running.\n\n\n## Guided Certs\n\nIn the past, when creating a certificate for a hostname, you had to refer to the documentation or UI for the various steps you needed to take. In this version of flyctl, we‚Äôve embedded some smart-documentation into the process. When you add a certificate now, flyctl will give you detailed instructions on what you need to set with your DNS provider to direct traffic to your App through your domain name and verify your ownership (allowing a certificate to be generated). It‚Äôs all part of making flyctl your preferred way to work with Fly.\n\n\n## Orgs and DNS\n\nAlso implemented in 0.0.138, are the orgs and dns commands. Orgs allows you to add and remove organizations - which you can also do from the Web UI. What you currently can‚Äôt do is invite and remove users from the organization; you‚Äôll have to use the Web UI for that. This, and the dns command, are works in progress and could change in a later version. We thought it was better to let you see what was in the longer pipeline.\n\n\n## Other changes\n\nThis is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\n\n## 25th August\n\nflyctl: Version 0.0.139 released.\n\nFly Platform/Web\n\n\n## 24th August\n\nflyctl: Version 0.0.138 released"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/sandboxing-and-workload-isolation/",
    "content": "Workload isolation makes it harder for a vulnerability in one service to compromise every other part of the platform. It has a long history going back to 1990s qmail, and we generally agree that it‚Äôs a good, useful thing.\n\nDespite a plethora of isolation options, in the time I spent consulting for technology companies I learned that the most common isolation mechanism is ‚Äúnothing‚Äù. And that makes some sense! Most services are the single tenant of their deployment environment, or at least so central to the logical architecture that there‚Äôs nothing to meaningfully isolate them from. Since isolation can be expensive, and security is under-resourced generally, elaborate containment schemes are often not high up on the list of priorities.\n\nThat logic goes out the window when you‚Äôre hosting other people‚Äôs stuff. Fly.io is a content delivery network for Docker containers. We make applications fast by parking them close to their users; we do that by running bare metal servers in a bunch of data centers around the world, and knitting them together with a global WireGuard mesh. Fly.io is extremely easy to play with ‚Äî single-digit minutes to get your head around, and rather than talk about it, I‚Äôll just suggest you grab a free account and try it.\n\nMeanwhile, I‚Äôm going to rattle off a bunch of different isolation techniques. I‚Äôll spoil the list for you now: we use Firecracker, the virtualization engine behind Amazon‚Äôs Lambda and Fargate services. But the solution space we chose Firecracker from is interesting, and so you‚Äôre going to hear about it.\n\n\n## chroot\n\nPeople like to say ‚Äúchroot isn‚Äôt a security boundary‚Äù, but of course that isn‚Äôt really true, it‚Äôs just not very strong by itself. Chroot is the original sandboxing technique.\n\nThe funniest problem with chroot is how it‚Äôs implemented: in the kernel process table, every struct proc (I was raised on BSD) has a pointer to its current working directory and to its root directory. The root directory is ‚Äúenforced‚Äù when you try to cd to ‚Äú..‚Äù; if your current working directory is already the root, the kernel won‚Äôt let ‚Äú..‚Äù go below it. But when you call chroot(2), you don‚Äôt necessarily change directories; if you‚Äôre ‚Äúabove‚Äù your new root, the kernel will never see that new root in a path traversal.\n\nThe real problem, of course, is the kernel attack surface. We don‚Äôt need to get cute yet; by itself, considering no other countermeasures, chroot gives you ptrace, procfs, device nodes, and, of course, the network.\n\nYou shake a lot of these problems off by not running anything as ‚Äúroot‚Äù, but not everything. A quick-but-important aside: in real-world attacks, the most important capability you can concede to an attacker is access to your internal network. It‚Äôs for the same reason that SSRF vulnerabilities (‚Äúunexpected HTTP proxies‚Äù) are almost always game-over, even though at first blush they might not seem much scarier than an unchecked redirect: there will be something you can aim an internal HTTP request to that will give an attacker code execution. Network access in a chroot jail is like that, but far more flexible.\n\nThis problem will loom over almost everything I write about here; just keep it in mind.\n\nchroot is a popular component in modern sandboxes, but none of them really rely on it exclusively.\n\n\n## Privilege Separation\n\nIt‚Äôs 1998 and the only serious language you have available to build in is C. You want to receive mail for a group of users, or authenticate and kick off a new SSH session. But those are complicated, multi-step operations, and nobody knows how to write secure C code; it‚Äôll be 30 years before anyone figures that out. You assume you‚Äôre going to screw up a parse somewhere and cough up RCE. But you need privileges to get your job done.\n\nOne solution: break the service up into smaller services. Give the services different user IDs. Connect services with group IDs. Mush the code around so that the gnarliest stuff winds up in the low-privileged services with the fewest connections to other services. Keep the stuff that needs to be privileged, like mailbox delivery or setting the login user, as tiny as you can.\n\nCall this approach ‚Äúprivsep‚Äù.\n\nDespite what its author said about his design, this approach works well. It‚Äôs not foolproof, but it has in fact a pretty good track record. The major downside is that it takes a lot of effort to implement; your application needs to be aware that you‚Äôre doing it.\n\nIf you can change your applications to fit the sandbox, you can take privsep pretty far. OpenBSD got this right with ‚Äúpledge‚Äù and ‚Äúunveil‚Äù, which allow programs to gradually ratchet down the access they get the kernel. It‚Äôs a better, more flexible idiom than seccomp, about which more later. But you‚Äôre not running OpenBSD, so, moving on.\n\n\n## Prelapsarian Containers\n\nPeople like to say ‚ÄúDocker isn‚Äôt a security boundary‚Äù, but that‚Äôs not so true anymore, though it once was.\n\nThe core idea behind containers is kernel namespacing, which is chroot extended to other kernel identifiers ‚Äî process IDs, user IDs, network interfaces. Configured carefully, these features give the appearance of a program running on its own machine, even as it shares a running kernel with other programs outside its container.\n\nBut even with its own PID space, its own users and groups, and its own network interfaces, we still can‚Äôt have processes writing handler paths to /sys, rebooting the system, loading kernel modules, and making new device nodes, and while many of these concerns can be avoided simply by not running as root, not all of them can.\n\nSystems security people spent almost a decade dunking on Docker because of all the gaps in this simplified container model. But nobody really runs containers like this anymore.\n\n\n## Incarceration\n\nEnter mandatory access control, system call filtering, and capabilities.\n\nMandatory access control frameworks (AppArmor is the one you‚Äôll see) offer system- (or container-) wide access control lists. You can read a version of Docker‚Äôs default AppArmor template to see what problems this fixes; it‚Äôs a nice concise description of the weaknesses of namespaces on their own.\n\nSystem call filters let us turn off kernel features; in 2020, if you‚Äôre filtering system calls, you‚Äôre probably doing it with seccomp-bpf.\n\nCapabilities split ‚Äúroot‚Äù into a whole mess of sub-privileges, ensuring that there‚Äôs rarely a need to give any program superuser access.\n\nThere are lots of implementations of this idea.\n\nModern Docker, for instance, takes advantage of all these features. Though imperfect, the solution Docker security people arrived at is, I think, a success story. Developers don‚Äôt harden their application environments consciously, and yet, for the most part, they also don‚Äôt run containers privileged, or give them extra capabilities, or disable the MAC policies and system call filters Docker enforces by default.\n\nIt may be even easier to jail a process outside of Docker; Googlers built minijail and nsjail, Cloudflare has ‚Äúsandbox‚Äù, there‚Äôs ‚Äúfirejail‚Äù, which is somewhat tuned for things like browsers, and systemd will do some of this work for you. Which tool is a matter of taste; nsjail has nice BPF UX; firejail interoperates with AppArmor. Some of them can be preloaded into uncooperative processes.\n\nWith namespaced jails, we‚Äôve arrived at the most popular current endpoint for workload isolation. You can do better, but the attacks you‚Äôll be dealing with start to get subtle.\n\n\n## Language Runtimes\n\nA limitation of jailed application environments is that they tend to be applied container- or at least process-wide. At high-volumes, allocating a process for every job might be expensive.\n\nIf you relax the requirement to run ordinary Unix programs, you can get some of the benefits of jails without fine-grained per-process security models. Just compile everything to Javascript and run them in v8 isolates. The v8 language runtime makes promises, which you might or might not trust, about what cotenant jobs can access. Or you could use Fastly‚Äôs Lucet serverside WASM framework.\n\nFrom a security perspective, assuming you trust the language runtimes (I guess I do) these approaches are attractive when you can expose a limited system interface, which is what everyone does with them, and less attractive as a general design if you need all of POSIX.\n\n\n## Emulation\n\nHere‚Äôs a problem we haven‚Äôt addressed yet: you can design an intricate, minimal whitelist of system calls, drop all privileges, and cut most of the filesystem off. But then a Linux kernel developer restructures the memory access checks the kernel uses when deref‚Äôing pointers passed to system calls, and someone forgets to tell the person who maintains waitid(2), and now userland programs can pass kernel addresses to waitid and whack random kernel memory. waitid(2) is innocuous, you weren‚Äôt going to filter it out, and yet there you were, boned.\n\nOr, how about this: every time a process faults an address, the kernel has to look up the backing storage to resolve the address. Since this is relatively slow, the kernel caches. But it has to keep those caches synchronized between all the threads in a process, so the per-thread caches get counters tied to the containing process. Except: the counters are 32 bits wide, and the invalidation logic is screwed up, so that if you roll the counter, then immediately spawn a thread, then have that thread roll the counter again, you can desynchronize a thread‚Äôs cache and get the kernel to follow stale pointers.\n\nBugs like this happen. They‚Äôre called kernel LPEs. A lot of them, you can mitigate by tightening system call and device filters, and compiling a minimal kernel (you weren‚Äôt really using IPv6 DCCP anways). But some of them, like Jann Horn‚Äôs cache invalidation bug, you can‚Äôt fix that way. How concerned you are about them depends on your workloads. If you‚Äôre just running your own applications, you might not care much: the attacker exploiting this flaw already has RCE on your systems and thus some access to your internal network. If you‚Äôre running someone else‚Äôs applications, you should probably care a lot, because this is your primary security barrier.\n\nIf namespaces and filters constitute a ‚Äújail‚Äù, gVisor is The Village from The Prisoner. Instead of just filtering system calls, what if we just reimplement most of Linux? We run ordinary Unix programs, but intercept all the system calls, and, for the most part, instead of passing them to the kernel, we satisfy them ourselves. The Linux kernel has almost 400 system calls. How many of them do we need to efficiently emulate the rest? gVisor needs less than 20.\n\nWith those, gVisor implements basically all of Linux in userland. Processes. Devices. Tasks. Address spaces and page tables. Filesystems. TCP/IP; the entire IP network stack, all reimplemented, in Go, backended by native Linux userland.\n\nThe pitch here is straightforward: you‚Äôre unlikely to have routine exploitable memory corruption flaws in Go code. You are sort of likely to have them in the C-language Linux kernel. Go is fast enough to credibly emulate Linux in userland. Why expose C code if you don‚Äôt have to?\n\nAs batshit as this plan is, it works surprisingly well; you can build gVisor and runsc, its container runtime, relatively easily. Once you have runsc installed, it will run Docker containers for you. After reading the code, I sort of couldn‚Äôt believe it was working as well as it did, or, if it was, that it was actually using the code I had read. But I scattered a bunch of panic calls across the codebase and, yup, that all that stuff is actually happening. It‚Äôs pretty amazing.\n\nYou are probably strictly better off with gVisor than you are with a tuned Docker configuration, and I like it a lot. The big downside is performance; you‚Äôll be looking at a low-double-digits percentage hit, degrading with I/O load. Google runs this stuff at scale in GCE; you can probably get away with it too. If you‚Äôre running gVisor, you should brag about it, because, again, gVisor is pretty bananas.\n\n\n## Lightweight Virtualization\n\nIf you‚Äôre worried about kernel attack surface but don‚Äôt want to reimplement the entire kernel in userland, there‚Äôs an easier approach: just virtualize. Let Linux be Linux, and boot it in a virtual machine.\n\nYou almost certainly already trust virtualization; if hypervisors are comprehensively broken, so is all of AWS, GCE, and Azure. And Linux makes hypervising pretty simple!\n\nThe challenge here is primarily about performance. A big part of the point of containers is that they‚Äôre lightweight. In a sense, the grail of serverside isolation is virtualization that‚Äôs light enough to run container workloads.\n\nIt turns out, this is a reasonable ask. A major part of what makes virtual machines so expensive is hardware emulation, with enough fidelity to run multiple operating systems. But we don‚Äôt care about diverse operating systems; it‚Äôs usually fine to constrain our workloads to Linux. How lightweight can we a virtual machine if it‚Äôs only going to boot a simple Linux kernel, with simple devices?\n\nTurns out: pretty lightweight! So we‚Äôve got Kata Containers, which is the big-company supported serverside lightweight virtualization project that came out of Intel‚Äôs Clear Containers (mission statement: ‚Äúcome up with a container scheme that is locked in to VT-x‚Äù). Using QEMU-Lite, Kata gets rid of BIOS boot overhead, replaces real devices with their virtio equivalents, and aggressively caches, and manages to get boot time down by like 75%. kvmtool, an alternative KVM runtime, gets even lighter.\n\nThere‚Äôs two catches.\n\nThe first, and really the big problem for the whole virtualization approach, is that you need bare metal servers to efficiently do lightweight virtualization; you want KVM but without nested virtualization. You‚Äôre probably not going to shell out for EC2 metal instances just to get some extra isolation.\n\nThe second, more philosophical problem is that QEMU and kvmtool are relatively complicated C codebases, and we‚Äôd like to minimize our dependence on these. You could reasonably take the argument either way between gVisor, which emulates Linux in a memory-safe language, or Kata/kvmtool, which runs virtualized Linux with a small memory-unsafe hypervisor. They‚Äôre both probably better than locked-down runc Docker, though.\n\n\n## Firecracker\n\nLightweight virtualization is how AWS runs Lambda, its function-as-a-service platform, and Fargate, its serverless container platform. But rather than trusting (and painstaking tuning) QEMU, AWS reimplemented it, in Rust. The result is Firecracker.\n\nFirecracker is a VMM optimized for security. It‚Äôs really kind of difficult to oversell how clean Firecracker is; the Firecracker paper boasts that they‚Äôve implemented their block device in around 1400 lines of Rust, but it looks to me like they‚Äôre counting a lot of test code; you only need to get your head around a couple hundred lines of Rust code to grok it. The network driver, which adapts a Linux tap device to a virtio device a guest Linux kernel can talk to, is about 700 lines before you hit tests ‚Äî and that‚Äôs rust, so something like 1/3 of those lines are use-statements! It‚Äôs really great.\n\nThe reason Firecracker (and, if you overlook the C code, kvmtool) can be this simple is that they‚Äôre pushing the system complexity down a layer. It‚Äôs still there; you‚Äôre booting an actual, make-menuconfig‚Äôd kernel, in all of it‚Äôs memory-unsafe glory. But you‚Äôre doing it inside a hypervisor where, in the Firecracker case, really you‚Äôre only worried about the integrity of the kvm subsystem itself.\n\nWe aren‚Äôt yet significant contributors to Firecracker, but it still feels weird talking the project up because it‚Äôs such a core part of our offering. That said: the team at AWS really did this thing the Western District Way:\n\n\n## General Thoughts\n\nKeep in mind, I think, that no matter how intricate your Linux system isolation is, the most important attack surface you need to reduce is exposure to your network. If you can spend time segmenting an unsegmented single-VPC network or further tightening the default Docker seccomp-bpf policy, your time is probably better spent on the network.\n\nRemember also that when security tools designers think about isolation and attack surface reduction, they‚Äôre generally assuming that you need ordinary tools to run, and ordinary tools want Internet access; your isolation tools aren‚Äôt going to do the network isolation out of the box, the way they might, for instance, shield you from Video4Linux bugs.\n\nIt seems to me like, for new designs, the basic menu of mainstream options today is:\n\nThese are all valid options! I‚Äôll say this: for ROI purposes, if time and effort is a factor, and if I wasn‚Äôt hosting hostile code, I would probably tune an nsjail configuration before I bought into a containerization strategy."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/serve-small-with-fly-io-and-gostatic/",
    "content": "Static websites are great for carrying unchanging content, be it assets, images, fonts or even, as in this case, an entire site. Well, I say entire site, but if you saw my last article, you‚Äôll know I recently rebranded a Maker organization and needed to deploy a ‚Äúsignpost‚Äù page pointing people to the new site. I want this signpost to have a tiny footprint so it will never cost anything to deploy.\n\nNow, the thing with Docker images is that you don‚Äôt really notice the layers of OS and applications that pile up in the background. Something as notionally simple as say running Apache HTTPD will still need an OS layer under it, no matter how minimal, you put the two parts together and the image size soon builds up. And you still have to add the content.\n\nThis is where something like GoStatic comes in. It‚Äôs a small, self-contained web page server which can run in a bare Docker image - no OS, just the binary. As the author points out, the official Golang images can weigh in with as much as half a gigabyte of image. For GoStatic, the image is an unchunky 6MB.\n\n\n## Go GoStatic\n\nSo, how do you make use of GoStatic on Fly? Let‚Äôs step though it now.\n\nOne thing you need to know is that by default GoStatic uses port 8043. So add -p 8043 to your fly init command when you create your project. That‚Äôll route traffic to port 80 and 443 to port 8043 on the application.\n\nWe already have an index.html we want to serve, so our next stop is the Dockerfile. Delete the example contents and replace it with just two lines.\n\nAnd we are ready to deploy! Just run flyctl deploy:\n\nOnce deployed, all you need to do then is flyctl open and a browser will open and navigate to the site.\n\nYou‚Äôll also notice that this has been upgraded to an https connection. All that is left is to attach a custom domain to it and we‚Äôre done.\n\n\n## Behind the scenes\n\nSo, what magic is going on here? Well, the Dockerfile in GoStatic explains a lot of it.\n\nIt uses a Docker multistage build to build our server binary in the first stage. Then it starts a new stage from scratch, literally using the command FROM SCRATCH. This says that there is no base image, just start building on top of nothing, an empty image. The rest of the GoStatic Dockerfile creates a passwd file so there are some usernames to work with and copies over the GoStatic binary.\n\nAnd then it all hands over to our own Dockerfile. GoStatic serves files out of /srv/http so we copy over our index.html to that directory. And that‚Äôs it. Everything else is managed by Fly, the build, the deployment and the upgrading to an https connection. There used to be a version of GoStatic which would handle HTTPS connections and certificates, but that functionality has been retired now servers like Caddy exist. On Fly, the lack of HTTPS support means it‚Äôs simple to just let Fly take on the HTTPS work for you.\n\n\n## A Small Squeeze\n\nOne last tip. Fly deploys new applications with 512MB of RAM and about a quarter of a virtual CPU. It‚Äôs called a micro-2x firecracker VM. But, we‚Äôre doing so little here, we could scale the VM size down. Let‚Äôs look at the scale settings:\n\nAnd now we can set the vm size:\n\nIf you want to find out about the other VM sizes, run flyctl platform vm-sizes.\n\n\n## Wrapping up\n\nWe‚Äôve got ourselves a tiny static web server and deployed it to Fly.io, and as a bonus, shrunk the VM‚Äôs footprint to match it and save running costs. Enjoy!"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/hugo-s-there-flying-with-hugo-and-caddy/",
    "content": "There I was wondering what to do about a website for a new community venture I was running where I thought, yes, let‚Äôs generate the site with Hugo, serve it with Caddy and run it all on Fly. Why Hugo and Caddy? Well, they both have good reputations as Go-based tooling thats compact and powerful, so let‚Äôs go make and host a site‚Ä¶\n\n\n## Fly First\n\nI‚Äôll need to be able to reference what the app name, and consequently site name, will be. Because of that, I‚Äôm going to start by initializing my Fly deployment:\n\nSelect your own app name or let the system generate one for you; it won‚Äôt matter as we‚Äôre going to front this set-up to a custom domain. We‚Äôre going to be using a Dockerfile (hence --dockerfile) and our server will operate on port 80 (-p 80).\n\n\n## The Hugo Configuration\n\nThe site doesn‚Äôt have much content and a stroll through the Hugo Quickstart will get us a front page and a blog article built in no time at all. It boils down to installing hugo, then installing a hugo theme and finally creating a config.toml file. Here‚Äôs ours:\n\nThat‚Äôs enough for our basic site to build running hugo which generates all the static files. Running hugo server -D lets us browse it locally on localhost:1313. The one thing to note? We‚Äôll be serving this site up on the makeronicc.fly.dev domain for now.\n\n\n## Docker And Hugo\n\nNext we want to get Docker to do the build work. Time to make a Dockerfile.\n\nThere are various Hugo docker images out there but the one I like is klakegg/hugo on Docker Hub. As well as having Docker images that can be used for running Hugo as a Docker container, there‚Äôs an ONBUILD image. This is designed to be a stage in a multi-stage Docker build. It‚Äôs run as part of the pipeline, but then results can be copied from its image to a new, cleaner image, less all the build tools.\n\nSo our Dockerfile starts like this:\n\nThat, when built with Docker, will load up the image and the current working directory contents, run hugo over it and deposit the results in /target in the hugo image. Hugo build, done. Now, let‚Äôs talk about serving it up.\n\n\n## Caddy Hack\n\nNow we come to Caddy, which is a great web server ‚Äúbuilt for now‚Äù - It has integrated handling of obtaining and managing Let‚Äôs Encrypt certificates so running an HTTPS site becomes super-simple. There‚Äôs only one issue - Fly already does all that certificate management for us, so although we want Caddy because it‚Äôs compact and easy to work with, we‚Äôre going to want to turn off Caddy‚Äôs own certificate system.\n\nCaddy is configured in a number of ways, JSON, API or the Caddyfile. I use the Caddyfile for this as its more human-readable. But now a public service announcement:\n\nWhen you search for Caddy and, well, anything at all, when you get to a result, scroll to the top of the page to make sure you aren‚Äôt on the Caddy 1 documentation. Caddy 2 is the current version but the google-juice for Caddy 1 documentation is still super high and the two are so similar yet different, it can be terribly frustrating to keep landing on the wrong docs.\n\nRight, back to creating our Caddyfile. Most of what I just talked about can be summed up in one opening block.\n\nThat turns off all the certificate management. Now we can tell Caddy to serve files for our makeroni.cc domain.\n\nNotice that this is just for the http protocol connections. That‚Äôs because, once the TLS connection has passed through the Fly edge, it travels on the encrypted Fly network as a normal HTTP request.\n\nThat‚Äôs the Caddyfile created. Now to pull the two parts together in the Dockerfile.\n\n\n## Adding Caddy\n\nAt the moment, our Dockerfile simply brings in and runs the Hugo static generator at build time. We need to take the results of that and put it into a Caddy docker image. There are official Caddy images, so I‚Äôll use one of them:\n\nDocker will now start with this Caddy image. Our Caddyfile says it will serve files out of /usr/share/caddy so we‚Äôll want to copy the files from our Hugo build over to there by adding:\n\nThe --from points to the named image we created at the start with AS hugo. Now all we need is to put the Caddyfile in place.\n\n\n## Ready To Fly\n\nWe‚Äôre ready to publish the site. Run flyctl deploy and watch as the Hugo site is built, copied into a Caddy image, that image is then flattened and despatched to a Fly firecracker node. You don‚Äôt have to worry about that though, just run flyctl open and your browser will open on your application.\n\nOne thing worth noticing is that, although we only configured http://makeroni.fly.dev, it‚Äôs being automatically upgraded to https: to secure the connection.\n\nBut we aren‚Äôt done yet. Remember we wanted the site to be provisioned on makeroni.cc.\n\n\n## Fly Domain\n\nThe first step is to get the DNS system to point makeroni.cc to the IP address of makeroni.fly.dev. I can get the IP Address by running flyctl ips list.\n\nWe want the V4 address. Now, I need to go to the registrar of the DNS entry, in my case NameCheap, and get to the DNS management pages, specifically the Advanced Management page of that.\n\nIt‚Äôs there I can add an A record. A @ 77.83.141.28 (The @ goes in the host column on Namecheap).\n\nSave that and let it propagate and then go to http://makeroni.cc and you should see your site. If you follow any links though, you‚Äôll notice you are back on https://makeronicc.fly.dev. That‚Äôs because Hugo generated all the links with that address.\n\nThat‚Äôs easy enough to fix (we‚Äôll get back to it in a moment), but there‚Äôs another more important thing to look at.\n\nIf I try to go to https://makeroni.cc and I get an error saying the connection is not secure. I haven‚Äôt created a TLS certificate for the domain.\n\nThe quickest way to do this is to add an AAAA record in the same way I added an A record. The AAAA record in a DNS record should point to the IP V6 address for the host; it‚Äôs up in the flyctl ips list output too. So I add AAAA @ 2a09:8280:1:9f04:7aa6:a706:a3d7:ccba to the DNS. With that in place and propagated I can now go and request a certificate.\n\nAnd the traffic could flow securely‚Ä¶. Except there‚Äôs one last change we need to make to the Caddyfile.\n\n\n## Caddy Changes\n\nRemember I set up the Caddyfile with\n\nWell, I‚Äôm not serving the files on that URL now so I‚Äôll need to change that to match our custom domain:\n\nNow it‚Äôll respond to requests for files from the makeroni.cc domain‚Ä¶ but what if I want to make sure that people who accidentally access the old makeronicc.fly.dev site end up in the right place. For that, another rule in the Caddyfile is needed:\n\nNow those old requests will head to our new server. Redeploy the image to Fly and everything is ready to roll.\n\n\n## Wrapping Up\n\nWe‚Äôve gone through configuring a multistage image build which generated a static Hugo site, then loaded it into an image with Caddy. We‚Äôve configured Caddy for development deployments on Fly and then we‚Äôve got ourselves a custom domain set up, and made that work with TLS certificates from Let‚Äôs Encrypt and a small modification to Caddy‚Äôs setup."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/flyctl-evolved-fly-changelog/",
    "content": "There‚Äôs a new flyctl (v0.0.137) available for your command line, with cleaner commands and extra helpers. Find out more about it in the Changelog.\n\nThis flyctl release brings in some big changes in the command structure as we move to an app-centric command style. What does that mean? Well, the apps subcommand is being deprecated; we‚Äôve kept it in place for this release but now all its commands have top level commands of their own:\n\nThe move, restart, resume, suspend commands also now take an appname as their last argument. The status command has also followed suit in this change, so you can now type flyctl status appname rather than flyctl status -a appname - The -a option will remain supported.\n\nThe list command at the top level has been around for a while and has advantages over the older list command: you can match appnames with fragments of text and filter on status or organization. Talking about organizations, flyctl list orgs will list the organizations your account has access to.\n\nWe‚Äôve also made some small usability changes in how you initialize an application. The init command offers you a selection of builders or the chance to use a Dockerfile. If you don‚Äôt have one, init will create one for you with a simple hello world deployment to get you going. If you don‚Äôt want the example generated, use --dockerfile when running init. And, yes, you can still specify a builder with --builder, that‚Äôs not going away.\n\n\n## Other changes\n\n\n## Platform changes\n\nIt‚Äôs not all been flyctl changes. There‚Äôs a fix for a problem with cookie headers and HTTP/2 which is now in place. Also, if your application parses headers, you‚Äôll find that a Via header has been added to enable applications to trace their route through the Fly edge.\n\nThis is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\n\n## 9th July\n\nflyctl: Version 0.0.137 released\n\nFly Platform/Web"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/run-apollo-graphql-close-to-your-users/",
    "content": "Fly.io can run API servers close to users. It‚Äôs kind of like a CDN for your GraphQL server. Here‚Äôs a guide to building an edge GraphQL Server with Apollo and Redis.\n\nI‚Äôm a newly minted GraphQL convert. We built Fly on top of GraphQL and the experience turned me into a shameless cheerleader. An API format with static typing? That‚Äôs my jam.\n\n(If you don‚Äôt care for JAMStack puns you can just go read our guide on building an Edge GraphQL service with Apollo)\n\nSpeaking of jam, you‚Äôve probably used application stacks that push content close to users. Hosting JavaScript and markup on a CDN can help make an app snappy.\n\nYou can also apply CDN like infrastructure to a GraphQL API to get a nice speed boost. All you need is a way to run API servers and an application cache close to users.\n\nWhich is why we built a platform to run API servers close to users and paired it with a global Redis cache service. It‚Äôs a great place to run Apollo Server, for example, with its cache capabilities and first class Redis support.\n\nWe built a demo GraphQL API based on the Open Library REST API. Read the guide or check out the source code."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-cdns-generate-certificates/",
    "content": "It‚Äôs been a hectic first couple of weeks at Fly, and I‚Äôm writing things up as I go along, because if I have to learn, so do you. This is going to be a bit of a meander; you‚Äôll have to deal.\n\nLet‚Äôs start with ‚Äúwhat‚Äôs Fly?‚Äù Briefly: Fly is a content delivery network for Docker containers. Applications hosted on Fly are fast because they‚Äôre running on machines close to users. To do that, we run bare metal servers in a bunch of cities and host containers on them in Firecracker VMs. We proxy traffic from edge servers to containers through a global WireGuard mesh. It‚Äôs much easier to play with than ECS or K8s is, so signing up for a free account is probably the best way to get a feel for it, and a pleasant way to burn 5-10 minutes.\n\nObviously, to do stuff like this, you need to generate certificates. The reasonable way to do that in 2020 is with LetsEncrypt. We do that for our users automatically, but ‚Äúit just works‚Äù makes for a pretty boring writeup, so let‚Äôs see how complicated and meandering I can make this.\n\nIt‚Äôs time to talk about certificate infrastructure.\n\n\n## ACME\n\nRather than verifying information from ‚ÄúQualified Independent Information Sources‚Äù, LetsEncrypt does domain-validated certificates, based simply on proof of ownership of a domain, and is driven by a protocol called ACME. ACME is really simple. It‚Äôs been implemented in almost pure Bourne shell. The most complicated thing about it is JWS signatures, which are awful, but at least standardized. The ACME protocol is itself done over normal HTTP requests; the flow is roughly:\n\nACME challenges are intended to verify your ownership of a domain. There are three of them (four, if you count preauthorization, which LetsEncrypt doesn‚Äôt do); originally, they were:\n\ntls-http-01, in which you‚Äôre given a token to put on your server, under /.well-known/acme-challenge, and serve to LetsEncrypt‚Äôs client on 80/tcp. This is simple to describe and implement, but requires you to respond to HTTP requests on 80/tcp, which lots of people (sensibly) don‚Äôt want to do.\n\ntls-dns-01, in which you‚Äôre given a token to put in a TXT record in your DNS zone. This directly proves control over a domain, but it can be hard for operators to do. In particular, especially in larger organizations, the people who need certificates are not necessarily given access to DNS configuration.\n\ntns-sni-01, in which you‚Äôre given a token to embed in the SAN of a certificate you serve to TLS clients who request it through TLS SNI, which is TLS‚Äôs equivalent of the HTTP ‚ÄúHost‚Äù header. This is more complicated to implement, but is the most seamless of the challenges: all you need to do it is to run the TLS server you were going to run anyways.\n\n\n## The Story Of tls-sni-01\n\nBut tls-sni-01 no longer exists, because it‚Äôs insecure. The problem with SNI challenges is shared hosting.\n\nBecause IP addresses are scarce, many hosting providers arrange for customers to share IP addresses. As requests arrive for customers, they‚Äôre routed based on SNI.\n\nIn the same way that you can configure a local nginx to respond to any Host header without breaking the Internet, hosting providers routinely allow people to ‚Äúclaim‚Äù arbitrary hostnames on their platforms. This ostensibly doesn‚Äôt matter, because without control of the DNS, you can‚Äôt get people to talk to your claimed hostname.\n\nSimilarly, hosting providers will often let you provide your own TLS certificates.\n\nYou may see where this is going already. Here‚Äôs what LetsEncrypt did to verify domain ownership using SNI:\n\nIf a hosting provider let you claim names in the ‚Äú.invalid‚Äù TLD, and upload your own certificate for them, you could get a certificate issued for all the customers hosted on your IP. Heroku let you do this, as did AWS Cloudfront, and who knows who else.\n\nLetsEncrypt quickly took the SNI challenges down while hosting providers deployed fixes. Ultimately, SNI was so widely used this way that CAs concluded SNI was fundamentally unsafe to use as a challenge, and the ACME SNI challenge was deprecated, and finally removed last year.\n\n\n## A Note About A Related Problem\n\nThis attack is an instance of a broader attack class called ‚Äúsubdomain takeover‚Äù, which is a mainstay among bug bounty hunters. HackerOne will tell you all about it, if you want to make $50 or so in an evening.\n\nSo, any time you‚Äôre hosting content for customer domains, you have the problem of what happens when the customer stops using your service. As you might expect, lots of times you‚Äôll forget to stop forwarding DNS to old expired services. But your account on those services has lapsed, and that usually means that other people can claim the same names you were using. Since you‚Äôre still directing traffic to the service, the new claimant has now hijacked one of your subdomains.\n\nWhich is bad for all kinds of reasons; it allows you to steal cookies, violate CORS, bypass CSP; it even impacts OAuth2.\n\nFly mitigates this problem for ALPN challenges by not reusing IP addresses. Every application gets a unique, routable IPv6 address, and we won‚Äôt attempt Lets Encrypt validation unless the target hostname resolves via CNAME to that IPv6 address. (We do something similar for DNS challenges).\n\n\n## ALPN\n\nRecall the virtue of the tls-sni-01 challenge: it doesn‚Äôt require you to have access to your DNS configuration, nor do you need to open 80/tcp. You want a challenge that works this way. And there is one: the new third ACME challenge, tls-alpn-01.\n\nTo grok tls-alpn-01, you‚Äôll of course need to know what ALPN is. It‚Äôs an easy concept: Imagine TLS was a transport protocol in its own right, alongside TCP and UDP; ALPN would be its port number. I mean, they‚Äôre strings, not numbers, but same idea.\n\nWhy does TLS need such a thing? Most things that use TLS have their own TCP ports already. The answer is, of course, HTTP/2. HTTP/2 isn‚Äôt wire-compatible with HTTP/1 (it‚Äôs a binary protocol optimized for pipelining). But it can‚Äôt have its own TCP port, because if it did, nobody would be able to speak it: huge chunks of the Internet are locked down to ports 80 and 443.\n\n(We‚Äôre not, at Fly, by the way; you can run any TCP service you want here. But I digress from my digressions).\n\nTo solve this problem, when Google was designing SPDY (HTTP/2‚Äôs predecessor), they came up with NPN, ‚ÄúNext Protocol Negotiation‚Äù. The way NPN worked was:\n\nBy doing this, Chrome could opt into SPDY when talking to Google servers without burning a round trip for the negotiation.\n\nWhen SPDY turned into HTTP/2, something like NPN needed to get standardized, too. But the IETF tls-wg wasn‚Äôt a fan of NPN; in particular, it reversed the normal order of TLS negotiation, where the client proposes and the server chooses. So the IETF came up with ALPN, Application Layer Protocol Negotiation. ALPN works like this:\n\nThere‚Äôs a clear privacy implication here, right? Because the ALPN protocol you might be asking for is ‚Äútor‚Äù. The IETF ruins everything. And that‚Äôs true, but it‚Äôs complicated.\n\nFirst, the security offered by the encrypted NextProtocol frame was a little sketchy. Here‚Äôs an outline of an attack:\n\nIn practice, with Firefox, you could at one point do this simply by sending a bogus certificate; Firefox would complete the handshake, NPN included, even if the certificate didn‚Äôt validate.\n\n(For what it‚Äôs worth, some of the privacy issues here got mooted in TLS 1.3).\n\n\n## The JPEG Cat Extension\n\nAdditionally, while privacy was doubtlessly on Adam Langley‚Äôs mind when he wrote the NPN spec, the more important problem was probably middlebox compatibility.\n\nThe way middleboxes work is, enterprises buy them. They‚Äôre quite expensive, and enterprises buy big ghastly bunches of them in one go, so vendors work really hard to win those deals. And one straightforward way to win a bakeoff is to come to it with more features than your competitors. Here‚Äôs a feature: ‚Äúfilter connections based on what application protocol the client selects‚Äù. The Chrome team, presumably seeing that dumb feature a mile away, took it off the table by encrypting NPN selections.\n\n(This sounds paranoid, but only if you‚Äôve never worked on real-world TLS. In the NPN vs. ALPN tls-wg thread, AGL cited an ISP they found in the UK that took it upon themselves to block all the ECDHE ciphersuites. Why? Who knows? People do stuff like this.)\n\nUltimately, ALPN beat out NPN in the tls-wg. But, just as they were wrapping up the standard, Brian Smith at Mozilla (and author of Rust‚Äôs ring crypto library) threw a wrench in the works.\n\nIt had been Mozilla‚Äôs experience that, in some cases, middleboxes would hang when they got a ClientHello that was more than 255 bytes long. Hanging is very bad, because Mozilla needed timeout logic to detect it and try a simpler handshake, but that logic would also fire for people on crappy Internet connections, and had the effect of preventing those people from using modern TLS at all.\n\nMiraculously, a day later, Xiaoyong Wu at F5 jumped onto the thread to explain that older F5 software confused 256 byte ClientHello frames with TLSv2. TLS frame lengths are 2 bytes wide; once the ClientHello ticks past 255 bytes, the high length byte becomes 01h. That byte occupies the same point in the frame as the message type in SSLv2. To the F5, the frame could be a long-ish ClientHello‚Ä¶ or a very long SSLV2MTCLIENTHELLO, which was also 01h. The F5 chose SSLv2.\n\nThe fix? Send /more/ bytes! At 512 bytes, the high length byte is no longer 01h. And thus was born the ‚Äújpeg-of-a-cat‚Äù extension, which AGL took the fun out of by renaming it ‚Äúthe TLS ClientHello Padding Extension‚Äù.\n\n\n## Back To ACME\n\nThis is a little anti-climactic, but we‚Äôve come all this way, so you might as well understand how Fly (and other CDNs, and things like Caddy) generates certificates with ACME:\n\nThe ALPN challenge is more explicit than the SNI challenge; we had to specifically set up a subservice to complete ALPN challenges for customers, rather than doing it sort of implicitly based on our native SNI handling. (We wouldn‚Äôt have had the problem anyways based on how our certificate handling works, but this is the logic behind why ALPN is OK and SNI isn‚Äôt).\n\nThis process is pretty much seamless; all you have to do is say ‚Äúyeah, I want a TLS certificate for my app‚Äôs custom domain‚Äù. It only works with individual hostnames, though, which may be fine, but if it isn‚Äôt, you can do a DNS challenge with us to generate a wildcard certificate."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/flyctl-meets-json/",
    "content": "We‚Äôve just shipped a new version of Flyctl and although there‚Äôs only one new flag added, it‚Äôs a flag that can change the way you use flyctl forever.\n\nSay hello to --json or -j for short. This new flag attempts to present all output from flyctl as JSON. If a command queries the Fly GraphQL API, you‚Äôll get the JSON data from that call in your output. If a command shows you logs, you‚Äôll get the logs as structured JSON. When you are deploying, nearly all (we‚Äôll talk about that nearly later) the output of the command comes in the form of JSON formatted messages.\n\nYou may wonder what this can do for you. Well, we hope it‚Äôll let you create your own automation solutions for your workflow with Fly.\n\n\n## Getting the JSON out\n\nLet‚Äôs start with an example:\n\nWith JSON output enabled, the list apps command returns a JSON array of objects with the id, name, status, deployment, hostname and organization of each Fly application you have access to.\n\nAny application which accepts JSON can work with this data. If you‚Äôre scripting, you‚Äôll most likely want to process, filter and reorganize the JSON data. For that we recommend jq. With jq you get a tool that can slice and dice JSON files into the data you want.\n\nFor example, say you just wanted a list of hostnames and status to feed into your management platform, and your management platform only accepted CSV files.\n\n\n## Get in the jq\n\nWe aren‚Äôt going to go into jq in depth today. It‚Äôs a tool that is rich with features. Working through the jq tutorial and browsing the jq manual will get you started using jq to filter JSON content. Most useful is the jq playground where you can experiment with jq queries.\n\nAs an aside, the question with web tools like jq playground is how do you get command line data into it. Well, on the mac, you can pipe output to pbcopy which takes output and puts it on the clipboard. On Linux, you can use xsel --clipboard --input to do the same. Then it‚Äôs just a matter of pasting your data into the appropriate web form. Therefore:\n\nWill load up your clipboard with a list of all your apps that you can paste into the JSON text box in jq playground. Enter something like .[] | select(.Status==\"running\") and watch only the apps with running status appear in the Result box. You are now in a great place to start experimenting.\n\n\n## Making it happen\n\nFinally, let‚Äôs have an example which runs commands. In this example we want to find all the apps which may not have deployed for whatever reason. For each one, we want all the application‚Äôs info, as per the flyctl info command. In fact thats what we actually run - flyctl info and then we‚Äôll turn all that data into a JSON array for some unspecified app to process:\n\nLet‚Äôs work this through, command by command:\n\nThis lists out, as we already know, all the apps available in JSON format. That JSON gets fed to:\n\nTake the array and work through it element by element (.[]) copying it all through the pipe. Then for everything that matches the select criteria (.Deployed==false) copy it through the pipe to the final stage. .Name simply says to jq, write out the Name property. So this makes a list of names we can run flyctl info on. For that we turn to the buzz saw of Unix, xargs:\n\nThis command takes each line from the input (in this case, app names) and for each one runs flyctl info --json -a with the input argument appended. This will generate a stream of JSON info objects but, for this example, we want them gathered up in an array. We can turn to jq again:\n\nThis uses the jq ‚Äúslurp‚Äù option - yes, it really is called that - to slurp up all the input which is passed through untouched with the ‚Äú.‚Äù query, but wrapped in an array. That‚Äôs ready to feed into another application (in this case, a database, but thats another story).\n\n\n## Deploy watching\n\nDeployment, unlike other commands, doesn‚Äôt produce a single JSON object. As an ongoing process, it‚Äôll produce a stream of JSON ‚ÄúSource/Status/Message‚Äù objects reflecting the progress of the deployment. Currently, there‚Äôs also a stream on non-JSON messages mixed with the JSON stream, so although you can, we don‚Äôt recommend following the JSON output to monitor deployments. Instead, run the flyctl deploy command with --detach and then poll flyctl status for the application‚Äôs deployment status or run flyctl monitor to track deployments on an application.\n\n\n## Wrapping Up\n\nThe new Flyctl --json support should make automating Fly a breeze and we can‚Äôt wait to hear what you are building with it.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-changelog-9th-june/",
    "content": "This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. This week, new commands - restart, list and monitor - enhanced commands, an improved Deno buildpack and a new way to install flyctl.\n\nIn this edition: a new way to install flyctl (and native Windows installers), new commands in flyctl that let you restart, list and monitor applications, an improved open command and an important update to the Deno Buildpack.\n\n\n## Flyctl\n\n\n## Other changes:\n\n\n## Deno Buildpack:\n\nYou can get the Changelog in the blog or through an RSS feed of just changelog updates available on fly.io/changelog.xml. There‚Äôs also a dedicated ChangeLog page with all the recent updates.\n\n\n## 8th June 2020\n\nflyctl: Version 0.0.129 released\n\n\n## 2nd June 2020\n\nflyctl: Version 0.0.128 released\n\nFly Platform/Web"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-changelog-may-29th-2020/",
    "content": "This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. This week, pause/resume arrives for applications and a quicker way to view your Fly apps.\n\nYou can get the Changelog in the blog or through an RSS feed of just changelog updates available on fly.io/changelog.xml. There‚Äôs also a dedicated ChangeLog page with all the recent updates.\n\nMost of this week‚Äôs changelog items are covered in an recent article Fly - Now with Power Pause. There‚Äôs also updates to the Deno Buildpack.\n\n\n## 28th May 2020\n\nflyctl: Version 0.0.124 released\n\nFly Platform/Web\n\n\n## 18th May 2020\n\nflyctl: Version 0.0.123 released"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-now-with-power-pause/",
    "content": "The latest feature for Fly is now available and it‚Äôs‚Ä¶. a pause for your applications. And a matching resume, of course.\n\n\n## What‚Äôs a pause for?\n\nWhen you start building your applications, there may come a point where you don‚Äôt want to keep your application running - mothballing while another project happens, schedule delays, or just plain wanting to manage your costs.\n\nThat‚Äôs where flyctl suspend comes in. It‚Äôll take your application, save its state, and then reduce its running instances to zero. What doesn‚Äôt go away are the networking configuration, IP addresses and certificates, which are maintained in the background. Think of it as a deep freeze for your application. That means everything is ready to resume with the least amount of fuss.\n\n\n## Ready to resume\n\nWhen you are ready to come back online, flyctl resume will bring your application back to life. It does it carefully though, bringing just one instance back, even if the application was originally deployed across multiple regions.\n\nIt does this by starting with a scale minimum count of 1. You can use flyctl scale set min=n to scale it back up, where n is, typically the number of regions in your region pool. And you can see that by running flyctl regions list.\n\n\n## Regional Setting\n\nThe regions command has also had an enhancement with the addition of a set command. If you found adding and deleting regions to get to your desired region pool was a chore, this new command will be a timesaver. Just enter your desired region pool and set will take care of it.\n\n\n## Status of Play\n\nflyctl has long been able to list the applications under your account. Now that you can pause applications, you also need to be able to quickly see what state all of your applications are in. So we‚Äôve fixed that by adding the application‚Äôs current status to the results.\n\n\n## Open for Applications\n\nFinally, we noticed that when users deploy a Fly application, the first thing they do is look up the application‚Äôs host name, open up their browser and browse to that hostname. Well, that‚Äôs something we could add a shortcut for.\n\nSay hello to the new flyctl open command which takes the current application and, assuming it‚Äôs been deployed, opens your web browser and navigates to the application‚Äôs hostname.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today.\n\nUpdated Oct 29 2020 to use suspend/resume commands"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/websockets-and-fly/",
    "content": "One of the regular questions we get at Fly is ‚ÄúDo you support WebSocket connections?‚Äù. The answer is ‚ÄúYes‚Äù, but before you head off let us tell you a bit more and show you an example.\n\nWebSockets are powerful things for creating interactive applications. Example Zero for WebSocket examples is the chat application. This leverages WebSockets‚Äô ability to keep a connection alive over a long period of time while bidirectionally passing messages over it that ideally should be something conversational.\n\nIf you haven‚Äôt got an example app like that, we‚Äôve got one here for you - flychat-ws in fly-examples. It‚Äôs been put together to use only raw WebSockets (and express for serving up pages) and no other libraries. (A shoutout to other libraries that build on WebSockets like socket.io).\n\nLet‚Äôs get this application up on Fly first. Clone the repository and run:\n\nto fill out the node_modules directory.\n\nAssuming you have installed flyctl and signed up with Fly (head to the hands-on if you haven‚Äôt), the next step is to create an application:\n\nHit return to autogenerate a name and accept all the defaults. Now run:\n\nAnd watch your new chat app deploy onto the Fly platform. When it‚Äôs done run:\n\nAnd your browser will open with your new chat window. You‚Äôll notice we did no special configuration or changes to the application to make it deploy. So let‚Äôs dive in and see what‚Äôs in there.\n\n\n## Down the WebSocket\n\nThe source for this application isn‚Äôt extraordinary. The only thing that should stand out is the fly.toml file which was created when we ran fly apps create:\n\nThe fly.toml file contains all the configuration information about how this app should be deployed; it looks like this:\n\nAnd to paraphrase the contents, it says\n\nThis is all out-of-the-box Fly configuration.\n\n\n## Into the Server\n\nThe server.js file is a whole 17 lines long but it does plenty in 17 lines:\n\nFirst it pulls in the packages needed, express and ws the WebSockets library.\n\nThen it configures express to serve static files from the public directory:\n\nThe public directory contains the web page and JavaScript for the chat application. Now we move on to starting up the servers. There are two to start up: the WebSocket Server and the Express server. But they need to know where to listen, so we‚Äôll grab the port from the environment - or default to port 3000:\n\nNow we can start the servers:\n\nReading from the inside out, it starts the Express server with it listening on our selected port and then hands that server over to create a new WebSocket.Server.\n\nNow all we have to do is tell the code what to do with incoming connections:\n\nWhen a client connects, it‚Äôll generate a connection event on the server. We grab the socket that connection came in and add an event handler for incoming messages to it. This handler takes any incoming message and sends it out to any connected client. We don‚Äôt even have to track which clients are connected in our simple chat. The WebSocket server maintains a list of connected clients so we can walk through that list.\n\nAnd that‚Äôs the end of the server. Yes, there isn‚Äôt a lot there but it all works. It would be remiss of us at this point not to mention that we use a Dockerfile to assemble the image that‚Äôs run on Fly; here it is:\n\nI say remiss because this is where we set the port number in the environment to match up with the port in the fly.toml file from earlier. Oh, and we use npm start as the command to start the server up because in package.json we‚Äôve made sure we remembered to set a script up:\n\nSo, you can run npm start to run the server locally (by default on port 3000), or you can build and run it locally using Docker:\n\nOf course, in this case you‚Äôve already deployed it to Fly with a single command. Let‚Äôs move on to the user-facing side of things.\n\n\n## Now for the Client\n\nThe client code is all in the public directory. One HTML file lays out a simple form and calls some JavaScript on loading. That JavaScript is all in client.js and that‚Äôs what we are going to look at now:\n\nThere‚Äôs a global socket because we only need one to connect to the server. This gets initialised in our connect call, and it‚Äôs here that the code touches on the fact it‚Äôll be running on Fly. It looks up the URL it has been served from, and the port, and if served from an https: URL uses secure WebSockets (wss:). If not, it‚Äôll use ordinary WebSockets (ws:).\n\nFly‚Äôs default configuration is to serve up internal port 8080 on external port 80 unsecured and 443 with TLS. That TLS traffic is terminated at the network edge so from the application‚Äôs point of view, it‚Äôs all traffic on one port and no need to do anything special to handle TLS. Pow, less code to write and manage. All you have to do is make sure you don‚Äôt try to do anything special for these connections.\n\nOnce we have the URL, we open the WebSocket saying we want to work with a ‚Äújson‚Äù protocol. With the socket opened, let‚Äôs wire it up to receive messages:\n\nThis simply decodes the JSON into a message and pops it into our chat display. Most of the code is about doing the page manipulation. The last part of the connect process wires up the submit on a form where you type messages:\n\nThe last part of this is that sendMessage function:\n\nWhich is mostly CSS manipulation and reading the name and message fields, and who wants to spend time on that? The important part for the sockets side of things is these two lines:\n\nWhere a message is composed as a JSON object and then that JSON object is turned into a string and sent. The message will return soon enough as our server broadcasts to every client, including the one that originated the message. That means there‚Äôs really no need to update our messages view when we send. Score one for lazy coding.\n\n\n## Ready to Fly\n\nSo what‚Äôs this walk through the code shown us? Obviously that it‚Äôs incredibly easy to deploy an app to Fly, but also that Fly takes care of TLS connections so there‚Äôs less code for you to make. That it‚Äôs simple to make an Express app that also services sockets. That you can quickly test locally both as a native app and as a docker image. And to go remote it takes just one command to move it all onto Fly‚Äôs global infrastructure. That WebSockets simply work on Fly is just part of what Fly brings to the developers‚Äô table.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/mux-fly-wocket-and-rtmp/",
    "content": "In this Guest Post by Dylan Jhaveri at Mux, he talkes about Wocket, a proof of concept application that streams live video from your brower to an RTMP server. And yes, they use Fly to do it.\n\n\n## Wocket (WebSocket to RTMP)\n\nThis project is a proof-of-concept to demonstrate how you can stream live from your browser to an RTMP server. Streaming via RTMP is how you stream to Twitch, Youtube Live, Facebook Live, and other live streaming platforms. Typically, this requires running a local encoder software (for example: OBS or Ecamm Live). Those are great products and if you are streaming seriously you probably still want to use them. But we threw this project together to show how you might be able to pull off the same thing from a browser. In this example, instead of streaming to something like Twitch, Youtube Live, etc, we will be using the live streaming API provided by Mux, which gives you an on-demand RTMP server that you can stream to.\n\nThis project uses Next.js and a custom server with WebSockets. It should be noted that this project is a fun proof-of-concept. If you want to learn more about the challenges of going live from the browser take a look at this Mux blog post The state of going live from a browser.\n\nThis is what this project looks like. This will access the browser‚Äôs webcam and render it onto a canvas element. When you enter a stream key and click ‚ÄúStart Streaming‚Äù it will stream your webcam to a Mux live stream.\n\n\n## Clone the repo\n\n\n## Setup\n\n\n## Prerequisites to run locally\n\nFor development you‚Äôll probably want to use dev, which will do little things like hot reloading automatically.\n\nThe last line you should see is something along the lines of:\n\nVisit that page in the browser and you should see Wocket!\n\n\n## Getting a Mux stream key\n\nTo get a stream key and actually start streaming to an RTMP ingest URL you will need a free Mux account. After you sign up create a live stream either with the API or by navigating to ‚ÄòLive Streams‚Äô in the dashboard and clicking ‚ÄòCreate New Live Stream‚Äô see below:\n\nWithout entering a credit card your live streams are in ‚Äòtest‚Äô mode which means they are limited to 5 minutes, watermarked with the Mux logo and deleted after 24 hours. If you enter a credit card you get $20 of free credit which unlocks the full feature set and removes all limits. The $20 of credit should be plenty to cover the costs of experimenting with the API and if you need some more for experimentation please drop us a line and let us know!\n\n\n## Running the application in production\n\nAgain, this should just be considered a proof of concept. I didn‚Äôt write this to go to production. I beg you, don‚Äôt rely on this as is for something important.\n\n\n## Deploying to fly.io\n\nWe will deploy the server with flyctl. Fly.io will use the Dockerfile to host the server.\n\n\n## Putting it all together\n\nThe intended way of using this would be to use the MediaRecorder API and send video whenever the MediaRecorder instance fires the dataavailable event. The demo front-end is an example of how you could wire everything together using the getMediaRecorder and the MediaRecorder API.\n\n\n## Other projects\n\nSome other projects I found when trying to figure out this whole canvas -\u003e RTMP thing that were hugely helpful:\n\nOther ways of solving this problem:"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-changelog-may-15th-2020/",
    "content": "This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\nSince the last ChangeLog, we implemented a whole new scaling system with updates to the platform and flyctl. You can read all the details in the Updating Scale article. We‚Äôve also been improving the performance of our backhaul, which moves traffice between the Fly edge and datacenters.\n\n\n## 13th May 2020\n\nflyctl: Version 0.0.122 released\n\nFly Platform/Web\n\n\n## 8th May 2020\n\nflyctl: Version 0.0.121 released\n\nFly Platform/Web\n\n\n## 7th May 2020\n\nflyctl: Version 0.0.120 released\n\n\n## 4th May 2020\n\nflyctl: Version 0.0.118 released"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/deno-on-fly/",
    "content": "Deno has reached version 1.0, and congratulations to all involved. Deno is a better Nodejs, with TypeScript baked in and intergrated package management. We‚Äôre going to show you how to deploy Deno applications onto Fly and let your applications run closer to your users.\n\nWe‚Äôve been working with Deno for Fly for a while and its great blend of TypeScript, V8, Rust and simplicity makes for a great app experience. And great apps deserve a great deployment. We enjoy Deno so much that when we brought out our first cloud native buildpack we made it specifically for Deno.\n\nThat‚Äôs not the only way to deploy Deno on Fly. Fly is flexible and can run with buildpacks or you can just use a Dockerfile. In this article, we‚Äôll take you through building and deploying Deno with a Dockerfile.\n\n\n## Why use a Dockerfile?\n\nUsing a Dockerfile gives you complete control of the packaging of your applications, paring the process down to the bare minimum steps. With a buildpack, to make things as simple as possible, various assumptions are made for you (what underlying OS, how the files are organized, which ports are open by default) and this can lead to larger images than necessary.\n\nSo, you may get going with a buildpack, but if you are comfortable with the Dockerfile syntax and how it builds images, you can get quite the build time performance boost and a smaller image file to boot.\n\n\n## Building with a Dockerfile\n\nWe‚Äôve put a modified version of our tutorial example, hellodeno, up on the fly-examples repo. Clone or download hellodeno-dockerfile and we can begin. Let‚Äôs look at what files there are first:\n\nThe new files here, over the original hellodeno, are the Dockerfile and the deps.ts file. The Dockerfile is based on the readme example in deno-docker, a repository of Docker images for Ubuntu, Centos, Debian and Alpine Linux.\n\nLet‚Äôs step through the Dockerfile and see what it does:\n\nThe first line brings in the Alpine Linux and Deno base image, already loaded with the Deno 1.0.0 toolchain.\n\nOur application opens up port 8080 to do its work, so we expose that in the Dockerfile configuration.\n\nThe application will be set up in the /app directory of the image (avoiding the sometimes problematic mistake of loading code into the root of the filesystem).\n\nAnd best practices say that even running in a container, you shouldn‚Äôt run as root, so the Dockerfile switches over to use a deno user.\n\nAs noted, there is also a deps.ts file in our directory. This is a Deno convention of a single file which exports the dependencies that Deno automatically imports. Using this file, it‚Äôs possible to pin Deno imports to a particular version. Here, the Dockerfile copies that file across and then asks Deno to cache all the packages referenced in it. These will all go into their own layer in the Docker image.\n\nWe now copy all the other files over to the Docker image, and do a similar caching step with our code, server.ts.\n\nFinally, we tell Docker the command to run this image‚Äôs contents. The ENTRYPOINT is set to ‚Äúdeno‚Äù in the alpine-deno image we‚Äôre building with so the CMD settings are combined with the entry point to create a startup command deno run --allow-net server.ts.\n\nNow, there‚Äôs a file in the repo, the .dockerignore file, which lists files not to be copied over to that image. Pro-tip: remember to include your .git directory in there so that your Docker image doesn‚Äôt include the complete history of your application.\n\n\n## Testing locally\n\nYou may want to test your image before you attempt to deploy it. Make sure you have Docker installed and run:\n\nAnd browse to http://localhost:8080 to see a greeting from the example app.\n\n\n## Deploying to Fly\n\nWe‚Äôll assume you are all signed up, logged in and have the essential flyctl installed. If not, catch up by following our quick step-by-step Hands-On.\n\nThe first thing you need to do to deploy an app on Fly is to create a slot for the app on the platform and a fly.toml file for the deployment settings. That‚Äôs all done with one command flyctl init:\n\nThe flyctl init command will prompt you for an application name. We recommend you go for an auto-generated name, you can enter one but it may be rejected because it matches an already existing app. Then you‚Äôll be asked what organization you want the app created under. Organizations are a way of sharing apps between Fly users, so for now, as we aren‚Äôt sharing this, select your personal organization (the one with your name). Once you‚Äôve done that the fly.toml file will be created.\n\nNow it‚Äôs time to deploy. Run flyctl deploy and the image will be created and pushed onto the Fly platform. Once it‚Äôs there it will be deployed to a datacenter and a given a host name.\n\nTo find that hostname, run flyctl info:\n\nAnd there‚Äôs our hostname, hellodeno-dockerfile.fly.dev. Now connect to http://hellodeno-dockerfile.fly.dev (substituting in your app‚Äôs hostname as appropriate) and you should see a greeting from the application. You‚Äôll also notice that you‚Äôve been redirected to the https version of the site and had your application automatically secured.\n\n\n## Next with Deno\n\nThat is far from the end of what you can do with a Deno application on Fly. You can scale a Deno application around the world and add multiple domain names with automatically generated certificates to it, just like every other Fly application. So spread your Deno wings on Fly and make your great app today - don‚Äôt forget to tell us about it too. As for us? We‚Äôll be building more examples, as well as production applications, with Deno and you‚Äôll read about any insights we gain from the Deno and Fly combination here."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/updating-scale/",
    "content": "One thing you will find with Fly is we never miss an opportunity to refine the user experience for you. As part of that process, today, we‚Äôre unveiling a new scaling model and commands.\n\nWe have been recently looking at how scaling commands work on Fly and we‚Äôve come up with a new system with a simpler model and more compact command set to work with. We‚Äôd like to introduce it to you today.\n\n\n## The Scaling System\n\nPreviously, at least in the user interface, we offered regions as a fixed list with a number of instances per region set as part of a scaling plan. This worked but introduced some operations that were not intuitive, such as having to reset that list or elements in the list to remove regions. It was also fairly rigid and we wanted to automate the process more and let scaling manage more of the global load.\n\nThe new system is based on a pool of regions where the application can be run. Using a selected model, the system will then create at least the minimum number of application instances across those regions. The model will then be able create instances up to the maximum count. The min and max are global parameters for the scaling. There are two scaling models, Standard and Balanced.\n\nStandard: Instances of the application, up to the minimum count, are evenly distributed among the regions in the pool. They are not relocated in response to traffic. New instances are added where there is demand, up to the maximum count.\n\nBalanced: Instances of the application are, at first, evenly distributed among the regions in the pool up to the minimum count. Where traffic is high in a particular region, new instances will be created there and then, when the maximum count of instances has been used, instances will be moved from other regions to that region. This movement of instances is designed to balance supply of compute power with demand for it.\n\nIt‚Äôs worth noting that the scaling model, in conjunction with the platform, may not deploy in a predictable way. For example, if an allowed region is unable to allocate new instances, the platform will fallback to creating one in a nearby region. As this fallback operation is a feature of the system, it‚Äôs not something to worry about when an region that is not in the pool shows up in the currently running instances (as displayed with flyctl status).\n\n\n## flyctl regions\n\nThis is a new flyctl command, taking over from the previous flyctl scale regions command. It has its own subcommands:\n\nThis list shows that a pool with five regions in it.\n\nThis pool reflects the possible locations where the application may be run. A region being in the pool doesn‚Äôt mean there will be an instance in that location. That all depends on the scaling model and min/max count settings which are selectable with flyctl scale.\n\n\n## flyctl scale\n\nThis command has been expanded to give you more control over your scaling.\n\nThe show subcommand will display the current configuration of an application:\n\nHere, the standard model is being used, with a min of 5 instances and a max of 10. All the instances are created with a micro-2x VM.\n\nThe balanced and standard subcommands select the model and can optionally set the min and max values as key=value settings. If we wanted to just switch to a balanced model, we would enter:\n\nOr, if we wanted to switch to a standard model with a max of 20 we could enter:\n\nThere is also a set subcommand which lets you vary the min and max without changing the model.\n\nYou can, in just in one line, set the model and all the parameters too.\n\nThe vm command remains as it was, showing and setting the size of the virtual machine - in terms of cpu and memory - that each instance of the application will be run with. flyctl scale vm will display detailed information about the currently selected vm size. Specifying a vm size on the command line flyctl scale vm cpu1mem1 will switch the application to using that size of vm globally.\n\n\n## Going Forward\n\nScaling will always be a constantly evolving feature on Fly as we match more models with use cases and find new ways to fine tune models and commands.\n\nDo let us know what you think of the changes by dropping a line to support or leaving a message on our Spectrum Community.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-changelog-1st-may-2020/",
    "content": "This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. You can also use the RSS feed of just changelog posts available on fly.io/changelog.xml or consult our dedicated ChangeLog page with all the recent updates.\n\nSince the last ChangeLog, we‚Äôve introduced some enhancements to flyctl which make it easier to deploy tagged local and remote images directly to Fly. There is also a new load-balancing algorithm in operation which should be more effective with widely deployed global applications.\n\n\n## 30th April 2020\n\nflyctl: Version 0.0.117 released\n\nFly Platform/Web"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/deno-on-fly-using-buildpacks/",
    "content": "We really like Deno at Fly. It really is a better Node and we‚Äôd love people to build more with it. Our plan? ‚ÄúLet‚Äôs make building and deploying Deno apps on Fly as simple as other languages‚Äù. Now, you can configure, build and deploy Deno code in just two commands, and we‚Äôd like to show you how we did it."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/imaginary-on-fly-just-one-click/",
    "content": "One of our most popular uses for Fly is providing global image services using Imaginary; we even have a guide for it. But at Fly we know we can make it simpler, which is why we‚Äôve now got a one-click launcher for Imaginary available today.\n\nWarning: This document is old! It is likely wrong in some important way.\n\nThe folks behind Imaginary have worked with Fly and they also added the one-click launcher to their README on GitHub so that potential Imaginary users can be up, converting, resizing and more faster than ever. For Imaginary‚Äôs creators, it‚Äôs also a chance to turn that interest into project funding through revenue sharing. This is something that‚Äôs going to be big.\n\nOne-click launchers make it simple for any project to create running, production-ready versions of their applications. There‚Äôs no infrastructure needed, just the ability to paste in the button code where needed. And with scalable SVG buttons, even the button just works on your page.\n\n\n## Is your project next?\n\nImaginary are the first, we hope of many, open source project to work with us to make their application launchable with One-click onto Fly. Our plan is to make it a valuable way for these projects to fund themselves.\n\nWe‚Äôre looking for open source projects to work with us to create a process that works for everyone. If your project is interested in engaging with Fly around One-click launching, drop a mail to Christina, Fly‚Äôs Community Manager to find out more."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-changelog-17th-april-2020/",
    "content": "This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. As well as the RSS feed of just changelog posts available on fly.io/changelog.xml, there‚Äôs now a dedicated ChangeLog page with all the recent updates.\n\nThis past week has been one focused on the Fly infrastructure with earlier certificate renewals now in place and a dedicated Changlog page.\n\nWe also made time to bring in some user contributed changes to the flyctl which make debugging fly.toml files easier and clearer upgrade instructions. In other updates, there‚Äôs also better help for scaling your VM sizes and flyctl is packaged for Arch Linux now.\n\n\n## 16th April 2020\n\nflyctl: Version 0.0.116 released\n\nFly Platform/Web\n\n\n## 13th April 2020\n\nFly Platform/Web"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-changelog-9th-april-2020/",
    "content": "This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. There is also an RSS feed of just changelog posts available on fly.io/changelog.xml.\n\n\n## 7th April 2020\n\nFlyCtl ‚Äì v0.0.115 released\n\nFly Platform/Web\n\n\n## 3rd April 2020\n\nFlyCtl ‚Äì v0.0.114 released\n\n\n## 2nd April 2020\n\nFly Platform/Web"
  },
  {
    "title": "",
    "url": "https://fly.io/blog/powerbuilding-with-fly/",
    "content": "When you deploy to Fly, you are deploying an image to the Fly infrastructure. Fly currently uses Docker images for this purpose, for widest compatibility. We‚Äôre going to take a look at some features available which can power-up your image building.\n\n\n## Identifying your build\n\nWhen you deploy, the flyctl application looks for instructions to build the image in a Dockerfile or creates a builder VM to do the image build. Let‚Äôs start with the flags that control where flyctl looks for things.\n\n\n## Controlling the build\n\nSo, now we can tell flyctl what config and docker file we want to use. The next part of taking control of the build applies to any invocation of flyctl where a docker file is involved and that‚Äôs --build-args.\n\nBut let‚Äôs first rewind back into some docker commands. ARG and ENV both deal with variables that can be set in the build process. The ARG command allows variables to be specified that are taken from the build command and used during the build process. The ENV command allows variables to be created that will become environment variables set within the image when it starts running.\n\nUsing a combination of these, it‚Äôs possible to take a command-line argument and turn it into a runtime environment variable. Consider, for example, that we want to set the port that an NGINX server runs on. We may have had something like:\n\nin the docker file. To make that controllable from the command line we can replace that with:\n\nThe ARG command makes a variable called NGINX_PORT and sets it to a default of 8080. The ENV command creates an environment variable also called NGINX_PORT. This variable will live on into the running version of the image and can be used by scripts within it to control applications running in the image. It takes its value from the ARG setting of NGINX_PORT through the expansion of ${NGINX_PORT} which refers specifically to the ARG variable and thus will expand to either the default or the passed-in value.\n\nWhich leaves the question of how to set that ARG value with flyctl. That‚Äôs where --build-args comes in. You can use this option to pass a number of name/value pairs over to the build process. So for our example above we could do:\n\nThis will set the build argument which will override the default of the environment variable.\n\n\n## When to use ‚Äìbuild-args\n\nIt may look like this is a good way to pass credentials and other sensitive data to your Fly applications, but it isn‚Äôt. This is built for non-sensitive data as the information is baked into the image and could be retrievable. If it‚Äôs sensitive information you want to pass to the application, check out Fly Secrets which are securely stored and injected, as environment variables, into the application when the image starts running on the Fly platform."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-answers-more-questions/",
    "content": "We get asked questions about Fly in a lot of places on the web which we answer. But, not everyone is everywhere on the web, so with Fly Answers Questions, we bring those answers to you. If you have questions about Fly, why not ask @flydotio on Twitter, or drop a query in our Fly Spectrum.chat forum.\n\nQ: What if I get a large number of malicious TCP connections from around the world that are malicious and cause a large number of apps to be launched in the various datacenters. Are we on the hook for the bill? And is there anything in place to stop that happening? - (via Spectrum.chat/flyio)\n\nA: Let‚Äôs start with what we have to stop that sort of problem. Fly apps have a number of configurable constraints to prevent runaway scaling. There‚Äôs a per-region max count and a global max count to prevent too many instances from being started.\n\nGoing beyond those maximums means that incoming requests will automatically be dropped. It‚Äôs also worth mentioning that we have a number of protections against malicious connections already baked into the Fly networking infrastructure.\n\nComing soon, and currently in preview, is a configurable monthly maximum spend which will cap your spending. Also, where there was obviously a DDoS attack or similar, we‚Äôd look to waive usage for any crazy spikes that happened.\n\nQ: I‚Äôm working on a small team project using Fly, and we‚Äôd like to have a dedicated deploy key for our GitHub Actions CD workflow. Is there a way to do this yet? - (via Spectrum.chat/flyio)\n\nA: The easiest way to get a dedicated deploy key is to create a dedicated user for your CD workflow. Once you have a dedicated user you can get a personal access token for that user by going to fly.io/user/personal_access_tokens and selecting Create Access Token.\n\nThat user doesn‚Äôt have access to your Fly projects yet though. For that, you‚Äôll need to create a Fly organization. Projects created in an Organization can be worked on by all the members of the Organization. Create an Organization by going to fly.io/organizations and invite your new dedicated CD user to the organization. To complete the process, you‚Äôll need to log in as the CD user and accept that invite. You can now configure your CD processes using that user to deploy to Fly.\n\nQ: We run a monorepo which means we have a lot of Fly configuration files and Dockerfiles associated with projects. We‚Äôve found the --config flag which lets us select different fly configuration files but flyctl always seems to build with the file called Dockerfile. What can we do? - (via various support requests)\n\nA: You can update your flyctl and make use of one of a range of new options we‚Äôve been building for people with more sophisticated build configurations. We developed flyctl for the path of least resistance and that meant assuming that Dockerfile would be the build file for most users. You folks are great at pushing us to make things better, so we now have a new option --dockerfile for the deploy command. That lets you say which Dockerfile should be used in the build process. We‚Äôll have a new article on flyctl deploy options soon which will give you the power to bend Fly deployment to your will."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/making-datasets-fly-with-datasette-and-fly/",
    "content": "The creator of Datasette, the tool for Dataset sharing and exploration, has added Fly to the platforms you can use to publish and share data. We take a look at Datasette and show how well it works with Fly.\n\nI‚Äôve always liked finding a good dataset. With a background in databases and writing, I know a good dataset can bring a demo to life, be it a census of Squirrels in Central Park or a survey of grocery purchases in London. Datasets can also provide valuable foundations for citizen journalism and, under analysis, provide insights.\n\nThe key to making these datasets work for people is making them accessible and available, which is where Datasette comes in. It‚Äôs a project by Simon Willison designed to make sharing datasets easy. It all hinges on SQLite - essentially, you load up an SQLite database with data and then hand it to Datasette which presents it through a web site, to the world.\n\n\n## First, the data!\n\nI‚Äôm going to use the New York Central Park Squirrel Census Data for my data source because squirrels rock. Go there and click the View Data button to see the data presented in the very fine NYC OpenData viewer.\n\nI want the raw data though so I‚Äôll click on Export and select CSV which will kick off an immediate download. We now have a 2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv file to work with.\n\n\n## Making an Sqlite database\n\nEnsure you have SQLite3 installed on your system; it‚Äôll already be there on a macOS system as it is heavily used throughout the OS. If you run sqlite3 filename.db the data will be persisted to that file rather than just held in memory, so let‚Äôs begin.\n\nAnd the first thing we need to do is set the mode to CSV. If you look at the documentation, you‚Äôll see the .mode command listed as setting the output mode. That‚Äôs not quite completely true, it‚Äôs also used as a hint to the import command.\n\nNow we are ready to import with .import. This command takes the CSV filename and a table name to import into. If the table isn‚Äôt there, it‚Äôll use the first row in the CSV file to create the table columns and then import. (ProTip: If the table already exists it just imports everything, including the header row so always drop the table first).\n\nAnd we can do a quick check on what actually got imported with the .schema command.\n\nThere are a lot of columns about squirrels. Now I can exit sqlite3 and get ready to apply Datasette to the database.\n\n\n## Installing Datasette\n\nThere are a couple of ways to install and run Datasette. I‚Äôm going to go with the one that is simplest for most developers:\n\nAnd we‚Äôre ready to test running datasette squirrels.db:\n\nNavigating to http://127.0.0.1:8001 and clicking on the Squirrel table, we should see the squirrels data available:\n\nAnd clicking through you can start using Datasette‚Äôs UI to compose views of particular facets of the data, write your own SQL queries, export data as JSON or CSV or access the data through a REST/JSON API.\n\nThe next stop is making it available to the world.\n\n\n## Publishing to Fly\n\nWhile other platforms are already built into Datasette, the Fly publishing element of Datasette is a new plugin, created by Datasette‚Äôs developer. That means it has to be installed separately with pip3 install datasette-publish-fly\n\nWith that installed, you can run\n\nThe --app flag lets you set the app name and it will be rejected if it clashes with an existing app. You may, if you are in multiple organizations, be asked to pick one of those too. Once you‚Äôve done that, the publish command takes over, builds an image and deploys it onto the Fly platform. If you want to know what IP address and hostname the app is on, run flyctl info -a \u003cappname\u003e like so:\n\nAnd that also tells us where we need to browse: squirrels.fly.dev. We‚Äôre online and we can dig down into a table view where we can compose queries.\n\n\n## Deploying with Plugins\n\nDatasette Plugins aren‚Äôt just for publishing; there are a whole range of additional capabilities waiting to be slotted in. Take datasette-cluster-map, for example. It looks for latitude and longitude columns in the data and turns the data into an interactive map using them. Let‚Äôs see how we use this with Fly.\n\n\n## Tuning the tables\n\nThe Squirrel data has X and Y coordinates which match Longitude and Latitude; we‚Äôll need to rename those columns first. Now, for a long time, sqlite lacked the ability to rename columns, so you‚Äôll find many workarounds on the web if you search. The good news is, though, that since 2018 you have been able to rename columns with the alter table rename column command. So I‚Äôll just load up the sqlite3 database and alter those columns:\n\n\n## Run Locally Again\n\nI now need to install that plugin with pip3:\n\nAnd run datasette locally:\n\nAnd if we browse to that http://localhost:8001/ we‚Äôll see the index page as we did before. Now to deploy to Fly.\n\n\n## Deploying and Plugins\n\nThe difference here from when we previously published to Fly is that we have to list the plugins we need installed with our Datasette. The --install flag takes care of that, so now I can publish to Fly with:\n\nAnd that will include the cluster-map plugin. If I now browse to ‚Äúhttps://squirrels-mapped.fly.dev‚Äù, and click in on the squirrels table:\n\nOur view of the Squirrels data now includes a cluster map over Central Park that we can click in on and get a closer view. When sightings resolve to a single squirrel, you can hover over it to get all the details.\n\n\n## Datasette and Fly\n\nSo what does Fly add to Datasette? Well, as well as being super simple to deploy, you may have noticed that all the connections we‚Äôre making are HTTPS secured, with Let‚Äôs Encrypt certificates being generated automatically. If you want more, it‚Äôs simple to use your own custom domain or hostname with your Fly/Datasette deployment. You can also deploy all around the world so your dataset is available where people need it to be. And there‚Äôs also the edge networking/SSL termination which makes interaction that bit snappier. There are a whole lot more to explore in Datasette - check out the documentation - and it‚Äôs a great way to discover how you can make your applications Fly.\n\nThanks to Simon Willison, not only for Datasette and the Fly plugin, but also for his feedback on this article. And to the Squirrels of New York‚Äôs Central Park for taking part in the census.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/continuous-deployment-with-gitlab/",
    "content": "We recently added a guide to Continuous Deployment with Fly and GitHub Actions to the Fly documentation and almost immediately, I was asked if we could do the same for GitLab. Of course! Better still, let‚Äôs make this an example for other platforms.\n\n\n## Starting with Continuous Deployment to Fly\n\nTo deploy to Fly you essentially need four things\n\nFor this example, we‚Äôll deploy hellofly, our small Go application which says hello - GitLab can import the repository for you.\n\nOn GitLab, the CI/CD system takes care of the first requirement for you. The CI/CD process is all driven by a .gitlab-ci.yml file. Lets walk through the .gitlab-ci.yml I used to deploy from GitLab to Fly:\n\nThe default section does the essential preparation. It selects an image to use for the virtual machine - or ‚ÄúRunner‚Äù to use GitLab terminology - that will be used run the CI/CD process. A simple option is to use the same image as you use in the Fly app - our example is a Go app so we use the golang:1.13 image, but you could use any image which has sufficient toolchain components to build it.\n\nWith Fly, the deployment process runs in its own container, locally or remotely. That means only a minimal set of the components from a typical toolchain is needed to build the application.\n\nOnce you have selected your image and the deploy process begins, GitLab‚Äôs CI/CD engine will automatically copy the contents of the repository over to the Runner image. And that‚Äôs requirement one met.\n\nNext up is getting a runnable copy of flyctl into our Runner. For GitLab, we want to do this before it runs our deployment scripts, so we‚Äôll add it to the default section as the before_script.\n\nThis does two things. It installs curl from the package repository (after making sure its indexes are up to date). Then it uses curl to download the flyctl install script and runs that. Using the script should ensure that the right version of flyctl is installed. This covers requirement two.\n\nNow, we are going to quickly skip to the end, as the rest of the .gitlab-ci.yml is just this:\n\nThat will run the flyctl deploy command. That will then build and deploy the fly application. If we committed .gitlab-ci.yml at this point though, it would fail as it lacks two things.\n\n\n## Configuring for Fly\n\nThe things needed are a fly.toml file and an auth token. The fly.toml file will need to be created using fly init so you‚Äôll likely do this locally and then add the file to the repository so it will be there for flyctl to find when it‚Äôs run in the deployment container. Add that to the repository (do remove the filename from the .gitignore file first) and that‚Äôs requirement three handled.\n\nFor the API token, you‚Äôll want to use flyctl again, this time to reuse the token you were issued when you logged in. Run flyctl auth token and it will display the API token your session is currently using. Now you‚Äôll need to turn this token value into the value of the FLY_API_TOKEN environment variable inside the deployment container. For GitLab, you‚Äôll need to go to the repositories settings, select CI/CD, then expand the Variables section. Create a new variable called FLY_API_TOKEN and copy the auth token value into the Value field. Then turn on the Protected and Masked switches so that it is not leaked through the logs. Save the new variable. It should look like this:\n\n\n## Ready to Deploy\n\nNow we are ready to commit the .gitlab-ci.yml to the repository and install our CD pipeline. Here‚Äôs the whole file:\n\nCommit and push that up to GitLab and the deployment process will begin almost immediately. On the GitLab web UI, head to CI/CD and then Jobs and you should see the deployment job running. Click on the running badge to watch its progress.\n\n\n## Job‚Äôs Done\n\nAnd that‚Äôs pretty much it for deploying with GitLab‚Äôs CI/CD system. There‚Äôs a lot more functionality in there allowing you to structure the pipeline as you want and trigger different jobs at different times. We‚Äôve just touched on the simple case of wanting to deploy when an update is pushed. The principles here are applicable to most CI/CD platforms and should let you incorporate Fly into your workflows today.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-answers-questions/",
    "content": "We get asked questions about Fly in a lot of places on the web which we answer. But, not everyone is everywhere on the web, so with Fly Answers Questions, we bring those answers to you. If you have questions about Fly, why not ask @flydotio on Twitter, or drop a query in our Fly Spectrum.chat forum.\n\n\n## Q: Can I set up a custom hostname or domain with Fly?\n\nA: You can. In fact you can set up as many of them on an application as you like. When you create a custom hostname on Fly and validate it, you also get Let‚Äôs Encrypt certificates automatically. You can read more about this in Custom Domains with Fly which takes you through the process and how to automate it.\n\n\n## Q: If I delete the fly.toml file, can I regenerate it easily?\n\nThe fly.toml file is the file which contains a Fly application‚Äôs configuration. This question was via HackerNews.\n\nA: Sure. flyctl config save -a \u003cappname\u003e will retrieve the last used configuration and save it locally as fly.toml. If you want to save it with a different name, use -c for the config file name, flyctl config save -a \u003cappname\u003e -c \u003cconfig.toml\u003e.\n\n\n## Q: When using the Turboku adapter do you actually run my Heroku slug as a container on each edge?\n\nThis question (and the two following) came via Twitter.\n\nA: Yes, we do run your Heroku slug as a container in one of our datacenters. We spin up instances as near as possible to where a request enters our edge network, which may be in the same region or geographically very close. We don‚Äôt put the actual slug at the edge though, to give us more flexibility in responding to load.\n\n\n## Q: Isn‚Äôt the DB connection latency insane, though, for pages with lots of queries?\n\nA: You can choose the regions your app runs in to minimize database latency. We do this by default for Heroku/Turboku apps. You can do this for other apps too - for example if your database runs in datacenters in Virginia then - flyctl scale regions iad=1 will bring up the applications in the Virginia region or in regions close to it. You‚Äôll still get benefits of SSL termination at the edge with restricted regions for the app.\n\nNote: 13/05/20: The scaling commands have been updated since this article was published.\n\n\n## Q: Can I do only SSL termination?\n\nA: Yes you can. Setting handlers=['tls'] in the fly.toml configuration file will let you TLS terminate at the edge; currently it supports up to HTTP 1.1 (so no HTTP2 support).\n\n\n## Q: What is Fly doing about global databases in general?\n\nA: We‚Äôve got Redis for local caching already and have a replication system to distribute changes for that in place. We have tested CockroachDB as a distributed drop-in replacement for PostgreSQL, but have found it not as easy to migrate from PostgreSQL as our users would have liked. That said, we‚Äôve heard that CockroachDB is filling that compatibility gap and we‚Äôll be looping round to it as soon as possible.\n\n\n## Q: Our application has a repository on GitHub. Is there a recommended way to continuously deploy our application to Fly?\n\nA: You can use GitHub Actions to deploy straight to Fly. We‚Äôve got a guide to it - Continuous Deployment with Fly and GitHub Actions - which takes you through the steps with an example application.\n\n\n## Q: Could you clarify if Fly automatically scales down to zero for completely idle apps?\n\nThis question via Twitter.\n\nA: Sure can. We don‚Äôt scale to zero for completely idle apps, we keep one instance ticking over in a region on our default region model (close to the edge region where the app was created) so that its always there to respond to traffic quickly.\n\nScaling to zero on idle means an app can be very slow, cold-starting regularly, so we‚Äôve gone for a single instance in a region model as our lowest scale point. The good thing is that the Anycast IP address and edge TLS termination still make this a faster option than other alternatives.\n\nIf you want to know more about Fly scaling, check out this recent Scaling blog post or delve into Fly‚Äôs scaling documentation for even more detail."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/scaling-fly-for-all/",
    "content": "Scaling controls are now available to all Fly users. You have direct control of how many instances of your Fly applications are running in the Fly regions around the world. And you can take control of the size of your application‚Äôs virtual machines.\n\nUpdate 15/May/2020: The contents of this article have been superceded by a new scaling system on Fly. We‚Äôve refined the scaling models and commands to make things simpler to use. To read about these changes see the article on the updated scale system or read the revised scale documentation.\n\nWant to know a Fly secret? We‚Äôve got a lot of things in our platform that we‚Äôre steadily rolling out to users. One of those things is global scaling - we‚Äôve been using it internally and with selected customers, but now, it‚Äôs available for all.\n\nWhen you create an application on Fly, it uses our default region plan. This places the application in the best available region for access from the Fly edge network. When an application is created, it is put in the default region, which for most applications will be the closest region to where the application was created.\n\nWhen an instance hits its connections limit, by default between 20-25, a new instance added and Fly‚Äôs autopilot scaling will decide the optimal location for that new instance. It‚Äôs a very effective default plan.\n\n\n## Scaling over regions\n\nWe are moving on though and the first thing to do is to empower you. Specifically with power to control where your data center instances are and the minimum nuber of instances that are available there. Say hello to the flyctl scale commands.\n\nWith scale regions you can see what your current scaling is set to or you can set which regions your app is running in and minimum number of app instances you want running in each one. Let‚Äôs start by seeing what the default region looks like:\n\nAnd that‚Äôs it. There are no regions set and no instance counts displayed so this application runs in the default region.\n\nLet‚Äôs use the example of scaling from our front page:\n\nThe application will now run at least one instance in ams (Amsterdam), hkg (Hong Kong) and sjc (Sunnyvale, California). We‚Äôll discuss weight at another time - it controls the Fly‚Äôs preference for where new instances are created. When you do use a scale regions command, the Fly platform generates a new version of the deployed app and redeploys to match the new scaling.\n\n\n## Regions and Fly\n\nIf you aren‚Äôt sure about what regions there are, you can look regions up in the documentation or use the flyctl platform regions command:\n\n\n## Scaling VMs\n\nScaling out over regions is one of the scaling options. Another option is to scale the instances themselves. By default, applications are allocated a micro-VM (micro-2x) with 512MB of memory and a quarter share of a vCPU cores. It‚Äôs small but pretty mighty.\n\nRunning flyctl platform vm-sizes will display this table with the current per second and per month pricing for each VM size. That information is also available on the pricing page.\n\n\n## Setting a VM Size\n\nThe VM size for an application applies to all instances currently deployed and deployed in the future. Just run flyctl scale vm \u003csizename\u003e to set the size and, like the region scaling, a new version of the app will be created and all the instances will be redeployed.\n\nSo, you now know enough commands to get scaling today and configure your Fly app to the size that suits you.\n\nFor more on Scaling, check out our Scaling documentation where you can learn about the other scale commands, what the ‚ÄúAutoscaling‚Äù in the output is about and more.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/how-to-custom-domains-with-fly/",
    "content": "We‚Äôve just published Custom Domains with Fly, the latest of our application guides which shows how you can use Fly to proxy traffic or serve applications for multiple custom domains.\n\nIn Custom Domains with Fly, you‚Äôll learn how to configure an NGINX server to proxy traffic to external sites and how to attach host and domain names to your Fly applications using Fly‚Äôs command line flyctl.\n\nFor those of you who practice devops and automate everything, the article also has details on the Fly GraphQL API for adding, removing, checking and deleting hosts and associated certificates. There‚Äôs also a repository of ready-rolled Node applications which clearly demonstrate how the API works and offer a jumping off point for any developer looking to customize their Fly experience.\n\n\n## Speedrun!\n\nFor a taste of how straightforward the process can be, here‚Äôs a speed run for adding a host/domain to an application. Let‚Äôs say we have example.com and a Fly app called custom-quartz.\n\n\n## For Proxies, For All\n\nThere‚Äôs no limit to the number of certificates you can attach to an Fly application. That means that if you‚Äôre in the business of producing branded applications with custom domains, Fly is a great place to unify all your proxying needs into one globally fast application which scales on demand.\n\nFly your own flag on your Fly applications today.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-friday-feb28-news/",
    "content": "## This week with Fly:\n\nGet yourself to fly.io/heroku to try out Turboku, the art of faster Heroku apps.\n\n\n## Fly Notes - We Complete You\n\nBeing able to hit the [TAB] key and getting the possible next part of your command makes discovering command lines so much easier. The good news is that, from version 0.0.96, Fly‚Äôs command line generates the files you need so you can activate tab completion.\n\nFlyctl‚Äôs version command has a -c/--completion option which takes a shell name as a parameter, bash or zsh. Run flyctl version -c zsh and it will output the file you need to activate completions for flyctl. Typically running\n\nshould place the generated file in the right directory to be picked up when you log in next.\n\nBash users will have to decide where to store their generated file and source it in their .bashrc - there‚Äôs no default directory for completions on bash.\n\nDon‚Äôt forget to update your flyctl for this feature!\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/graphql-hasura-and-fly/",
    "content": "If you‚Äôre wondering how to access your data with GraphQL, then it‚Äôs worth looking at the Hasura GraphQL engine. It‚Äôs gives you a GraphQL backend service. Oh, and we‚Äôve just published a guide on how to deploy Hasura on Fly.\n\nHasura gives you an open source GraphQL backend which can make it simple to query PostgreSQL by helping you map all your schemas and roles. It also rolls in the ability to query other GraphQL services so you can mix in services. With an interactive console for analyzing and exploring, it‚Äôs remarkably useful.\n\nAmong the various ways you can deploy Hasura, there‚Äôs the option to deploy it with a Docker image. That‚Äôs where Fly can come in and turn Hasura into a global GraphQL backend. There‚Äôs some configuration to be done - such as where to find your database, setting secrets and turning the interactive console on and off - and we take you through each of the steps.\n\nDive in and get yourself an Hasura deployment on Fly and do GraphQL everywhere. It‚Äôs far simpler on Fly.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-friday-customizing-with-dockerfiles/",
    "content": "Last week‚Äôs Fly Friday video showed how you can simply deploy a Docker image to Fly. Today, we‚Äôll show how straightforward it is to customize that deployed image with Fly.\n\nWe deployed Docker‚Äôs official httpd image in our first Fly Friday video. That image, when run, serves up files from its /usr/local/apache2/html. By default the image contains the words ‚ÄúIt works‚Äù. If we want it to say something else, we need to copy our content into that directory.\n\nTo do that we use a Dockerfile that is just two lines long.\n\nThe Dockerfile contains all the instructions needed to create our new image. The FROM says take the Docker httpd image and use it as a base for our new image. The COPY says copy the contents of a local directory into /usr/local/apache2/html/ within the new image. Fly will then deploy that new image.\n\nWe‚Äôll also have to create the public_html directory and put some HTML content in there.\n\nWant to see all this in action? Watch this Fly Friday video:\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/fly-friday-flyctl-and-ports/",
    "content": "We‚Äôre always improving the tools at Fly. There are the big features like Fly buildpack support of course and then there are the smaller features which just make life that little bit simpler, like the new -p option for Fly app creation.\n\nTL;DR: The -p flag is used with flyctl apps create when you create a Fly app for the first time. Setting -p with a numeric value automatically sets the internal port of the generated fly.toml configuration file.\n\nThe internal port is the port your application uses to communicate with the world. We defaulted it to 8080.\n\nWith more and more people making use of prebuilt Docker images on Fly though, they don‚Äôt have control of which port their image uses. So they have to change the internal port setting.\n\nWe noticed that their workflow had become ‚Äúcreate app, edit fly.toml file to change the internal port‚Äù. That‚Äôs one step too many for us.\n\nAnd so that‚Äôs why we added the -p flag which sets the internal port for you, no edits required. It‚Äôs available from release 0.94 of Flyctl.\n\nYou can see it in action in this, the first of our Fly Friday videos:\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/simpler-fly-deployments-nodejs-rails-golang-java/",
    "content": "We‚Äôve just added a new feature to flyctl which lets you build and deploy NodeJS, Rails, Go, and Java applications to the Fly platform with just two commands. That new feature is support for Cloud Native Buildpacks, but you don‚Äôt need to know what Buildpacks are to use them with Fly.\n\n\n## A Speedrun for everyone\n\nLet‚Äôs get straight down to it. Make sure you have the latest version of flyctl and then get a Node application like this version of our hello world example - ‚Äúhellonode-builder‚Äù.\n\nNow let‚Äôs create a fly application for it with one line:\n\nHit return for an autogenerated name and flyctl will set up a fly app for you. Make a note of that name. Then all you need to do is:\n\nAnd the application will be turned into a docker image and deployed onto the Fly Global Application Platform. Connect to your newly deployed app by running flyctl open and you‚Äôre already Flying.\n\n\n## Flyctl and Buildpacks\n\nWhat we have added is the ability for flyctl to use Cloud Native Buildpacks from the buildpacks.io project. This new Buildpack system is an evolution of existing Buildpack systems from Heroku and Cloud Foundry, designed to be used by different platforms.\n\nThe new Cloud Native Buildpack system isn‚Äôt compatible with the older Heroku and Cloud Foundry buildpacks, but new Cloud Native Buildpacks are appearing every day to supercede them. As the availability expands, even more languages and frameworks will be able to be automatically built and deployed by flyctl.\n\nThe new Buildpack support complements our existing support of Dockerfiles for building your Fly deployment images. You can use that when there‚Äôs no appropriate buildpack available or you want to do something different from existing buildpacks. We‚Äôre making sure you have all the flexibility and power you need to hand.\n\n\n## Available Cloud Native Builders\n\nThere are already Cloud Native Buildpacks that support building Rails, NodeJS, Go and Java applications with Ubuntu, Alpine or distroless stacks. Here are some of the stacks we‚Äôve already tested:\n\nIf you are unsure which to use, we recommend the ‚Äúheroku/buildpacks:18‚Äù option for its wide coverage.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/turboku/",
    "content": "When creating a new platform for globally faster apps, one question often comes up: ‚ÄúBut I already have an application on another platform, why should I use yours?‚Äù Easy enough. Fly runs your application around the globe‚Äîin datacenters that are close to your users‚Äîthrough an edge network which is optimized for faster TLS handling.\n\nThe next question is how can they move their application the Fly. For that we‚Äôve created ‚ÄúTurboku‚Äù, a simple way to bring your Heroku apps to Fly. What we do is take your Heroku web dynos and turn them into Fly applications automatically.\n\nThat way they can be deployed to our global edge network of fast TLS nodes. Your users‚Äô requests will automatically land at the closest datacenter in the Fly network. You‚Äôll only have to use one IP address which works globally and handles all the routing for you - well, two if you are doing IPv6.\n\nThe TLS negotiation times will be less too in part because the DNS lookup will be quicker (with only one IP address to resolve), and in part because the TLS terminates closer to the user.\n\nFly even makes getting a modern ECDSA TLS certificate easy - it‚Äôs automatic for your Fly domain and just needs a DNS record change to be enabled for any custom domain.\n\n\n## The Tale of the Tape\n\nTo see how this works in practice, we benchmarked Heroku and Fly performance around the world, averaging results and comparing the time taken for each stage of the connection using an existing application. We also recorded the total time for the network connection, the time for the full request to be processed and, as Fly supports Redis caching, the time taken when a cache is in use.\n\n\n## Try Turboku\n\nEvery component of the Fly platform can make your Heroku applications faster for the most important people of all. Your users. So learn to Fly today, it‚Äôs free.\n\nIf you have an Heroku app you want to make fast, you can start it up on Fly by going to fly.io/heroku or you can read our new application guide Speed up a Heroku App for a complete guide to the process - including how to connect all the good stuff up to your own custom domains.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On where you can get a free account and deploy your first app today.\n\nArticle updated 28/Feb/2020 with benchmark data and video."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/command-lines-flyctl-and-fly/",
    "content": "We‚Äôve just launched the Fly Global Application Platform, designed to make all your applications fast and global. You can dive into Fly and sign up right now for free simply by installing our new command-line tool, flyctl. With the new Fly Global Application Platform, we‚Äôre doing everything command line first.\n\n\n## Why Command Lines?\n\nAs we‚Äôve developed Fly, we‚Äôve seen numerous benefits that come from driving the service through the command line. Our newest addition to our command armory is the flyctl command which now, quite literally, covers the entire lifecycle of a Fly application, and beyond. The web interface will continue to play its part, but the command line is king.\n\n\n## What Makes a Command-Line-Controlled Service Compelling?\n\nIntegration. A command-line-driven service can be integrated into all modern workflows and toolchains. That‚Äôs something that cannot be said of web-driven applications which will always need an API layer to even start down the integration path. Command lines are simple, expressive and direct ways to perform a task.\n\n\n## Delivering an Effective Command Line Tool\n\nPrevious command-line technology at Fly was built on Node. Node is great for many things but there‚Äôs a lot of runtime to carry around to get it doing the job you want it to do. So we moved to Go for flyctl and it‚Äôs all rather splendid because now we can make a self-contained binary that you just need to download and run. One binary for macOS, 32- and 64-bit binaries for Linux and 32- and 64-bit executables for Windows.\n\nCreating flyctl in Go also allowed us to adopt the same command-line engine, Cobra, that popular cloud tooling like Kubernetes, Moby/Docker and Hugo use. That helps make it an already familiar environment for developers to work in with the command/subcommand style of requests and contextual help at all levels of that hierarchy.\n\n\n## Maintaining Focus\n\nCommand-line tools let developers work where they are, in the terminal and in their current directory. There‚Äôs nothing more distracting than having to change from terminal to browser and back. It breaks the train of thought between different working modes, reaching for the mouse to click, then back to typing.\n\nWeb interfaces do have an essential role to play in the platform. Our thinking is that web-browser-based elements of the user experience should be about displaying rich, informative dashboards and other metric displays.\n\nWeb-based dashboards and metrics are well suited to run alongside a richly interactive command-line experience.\n\n\n## Command Line Reproducability\n\nIf you‚Äôve ever talked someone through repeating an interaction with a GUI, you‚Äôll know how hard it is‚Äîno matter how precise the direction or explicit the instruction. On the other hand, instructions for the command line are self-contained and self-explanatory. And those instructions can be scripted and repeated as many times as needed. For support and operations, these attributes are invaluable.\n\n\n## Developers First and Always Open\n\nWe‚Äôve put flyctl and the Fly experience together with a developer-first ethos. That means picking the right tools for each part of the task, making things reproducible and documentable, and most of all, making it all enjoyable.\n\nAnd if there‚Äôs something you want to improve or have feedback about, the Flyctl Github repository is open for your input.\n\nWant to learn more about Fly? Head over to our Fly Docs for lots more, including a Hands On so you can get a free account and deploy your first app today."
  },
  {
    "title": "",
    "url": "https://fly.io/blog/welcome-to-fly/",
    "content": "Welcome to Fly, the home of the Fly Global Application Platform. And welcome to the Fly Blog, where you can learn about the power of Fly to bring your applications closer to your users, wherever they are in the world.\n\nCheck out our all new documentation for the Fly platform which covers everything from creating Apps you can deploy to Fly to how to take existing Docker images and setting them free across the globe.\n\nIf you have any questions, drop a line to us on support@fly.io."
  }
]